[{"link": "https://doi.org/10.1145/3593013.3593994", "title": "Certification Labels for Trustworthy AI: Insights From an Empirical Mixed-Method Study", "abstract": "Auditing plays a pivotal role in the development of trustworthy AI. However, current research primarily focuses on creating auditable AI documentation, which is intended for regulators and experts rather than end-users affected by AI decisions. How to communicate to members of the public that an AI has been audited and considered trustworthy remains an open challenge. This study empirically investigated certification labels as a promising solution. Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users\u201a\u00c4\u00f4 attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios. Based on the survey results, we demonstrate that labels can significantly increase end-users\u201a\u00c4\u00f4 trust and willingness to use AI in both low- and high-stakes scenarios. However, end-users\u201a\u00c4\u00f4 preferences for certification labels and their effect on trust and willingness to use AI were more pronounced in high-stake scenarios. Qualitative content analysis of the interviews revealed opportunities and limitations of certification labels, as well as facilitators and inhibitors for the effective use of labels in the context of AI. For example, while certification labels can mitigate data-related concerns expressed by end-users (e.g., privacy and data protection), other concerns (e.g., model performance) are more challenging to address. Our study provides valuable insights and recommendations for designing and implementing certification labels as a promising constituent within the trustworthy AI ecosystem.", "keywords": "'AI', 'Audit', 'Documentation', 'Label', 'Seal', 'Certification', 'Trust', 'Trustworthy', 'User study'", "ccs_concepts": "'Human-centered computing _ Empirical studies in HCI'", "author_names": "'Nicolas Scharowski', 'Michaela Benk', 'Swen J. K\u221a\u00bahne', 'L\u221a\u00a9ane Wettstein', 'Florian Br\u221a\u00bahlmann'", "author_affiliations": "'University of Basel, Center for General Psychology and Methodology', 'ETH Zurich, Mobiliar Lab for Analytics', 'Zurich University of Applied Sciences, School of Applied Psychology', 'University of Basel, Center for General Psychology and Methodology', 'University of Basel, Center for General Psychology and Methodology'"}, {"link": "https://doi.org/10.1145/3593013.3594054", "title": "On the Impact of Explanations on Understanding of Algorithmic Decision-Making", "abstract": "Ethical principles for algorithms are gaining importance as more and more stakeholders are affected by \"high-risk\" algorithmic decision-making (ADM) systems. Understanding how these systems work enables stakeholders to make informed decisions and to assess the systems\u201a\u00c4\u00f4 adherence to ethical values. Explanations are a promising way to create understanding, but current explainable artificial intelligence (XAI) research does not always consider existent theories on how understanding is formed and evaluated. In this work, we aim to contribute to a better understanding of understanding by conducting a qualitative task-based study with 30 participants, including users and affected stakeholders. We use three explanation modalities (textual, dialogue, and interactive) to explain a \"high-risk\" ADM system to participants and analyse their responses both inductively and deductively, using the \"six facets of understanding\" framework by Wiggins & McTighe [63]. Our findings indicate that the \"six facets\" framework is a promising approach to analyse participants\u201a\u00c4\u00f4 thought processes in understanding, providing categories for both rational and emotional understanding. We further introduce the \"dialogue\" modality as a valid explanation approach to increase participant engagement and interaction with the \"explainer\", allowing for more insight into their understanding in the process. Our analysis further suggests that individuality in understanding affects participants\u201a\u00c4\u00f4 perceptions of algorithmic fairness, demonstrating the interdependence between understanding and ADM assessment that previous studies have outlined. We posit that drawing from theories on learning and understanding like the \"six facets\" and leveraging explanation modalities can guide XAI research to better suit explanations to learning processes of individuals and consequently enable their assessment of ethical values of ADM systems.", "keywords": "'XAI', 'learning Sciences', 'algorithmic decision-making', 'algorithmic fairness', 'qualitative methods'", "ccs_concepts": "'Human-centered computing _ Field studies'", "author_names": "'Timoth\u221a\u00a9e Schmude', 'Laura Koesten', 'Torsten M\u221a\u2202ller', 'Sebastian Tschiatschek'", "author_affiliations": "'Faculty of Computer Science, Research Network Data Science, UniVie Doctoral School Computer Science DoCS, University of Vienna', 'Faculty of Computer Science, Research Group Visualization and Data Analysis, University of Vienna', 'Faculty of Computer Science, Research Network Data Science, Research Group Visualization and Data Analysis, University of Vienna', 'Faculty of Computer Science, Research Network Data Science, Research Group Data Mining and Machine Learning, University of Vienna'"}, {"link": "https://doi.org/10.1145/3593013.3594003", "title": "Simplicity Bias Leads to Amplified Performance Disparities", "abstract": "Which parts of a dataset will a given model find difficult? Recent work has shown that SGD-trained models have a bias towards simplicity, leading them to prioritize learning a majority class, or to rely upon harmful spurious correlations. Here, we show that the preference for \u201a\u00c4\u00f2easy\u201a\u00c4\u00f4 runs far deeper: A model may prioritize any class or group of the dataset that it finds simple\u201a\u00c4\u00eeat the expense of what it finds complex\u201a\u00c4\u00eeas measured by performance difference on the test set. When subsets with different levels of complexity align with demographic groups, we term this difficulty disparity, a phenomenon that occurs even with balanced datasets that lack group/label associations. We show how difficulty disparity is a model-dependent quantity, and is further amplified in commonly-used models as selected by typical average performance scores. We quantify an amplification factor across a range of settings in order to compare disparity of different models on a fixed dataset. Finally, we present two real-world examples of difficulty amplification in action, resulting in worse-than-expected performance disparities between groups even when using a balanced dataset. The existence of such disparities in balanced datasets demonstrates that merely balancing sample sizes of groups is not sufficient to ensure unbiased performance. We hope this work presents a step towards measurable understanding of the role of model bias as it interacts with the structure of data, and call for additional model-dependent mitigation methods to be deployed alongside dataset audits.", "keywords": "'neural networks', 'simplicity bias', 'performance disparities', 'fairness'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Computing methodologies _ Neural networks', 'Computing methodologies _ Computer vision', 'Social and professional topics _ Socio-technical systems', 'General and reference _ Empirical studies', 'General and reference _ Evaluation'", "author_names": "'Samuel James Bell', 'Levent Sagun'", "author_affiliations": "'FAIR, Meta AI', 'FAIR, Meta AI'"}, {"link": "https://doi.org/10.1145/3593013.3594084", "title": "A Sociotechnical Audit: Assessing Police Use of Facial Recognition", "abstract": "Algorithmic audits are increasingly used to hold people accountable for the algorithms they implement. However, much work remains to integrate ethical and legal evaluations of how algorithms are used into audits. In this paper, we present a sociotechnical audit to help external stakeholders evaluate the ethics and legality of police use of facial recognition technology. We developed this audit for the specific legal context of England and Wales, and to bring attention to broader concerns such as whether police consult affected communities and comply with human rights law. To design this audit, we compiled ethical and legal standards for governing facial recognition, based on existing literature and feedback from academia, government, civil society, and police organizations. We then applied the resulting audit tool to three facial recognition deployments by police forces in the UK and found that all three failed to meet these standards. Developing this audit helps us provide insights to researchers in designing their own sociotechnical audits, specifically how audits shift power, how to make audits context-specific, how audits reveal what is not transparent, and how audits lead to accountability.", "keywords": "'algorithmic audits', 'accountability', 'ethical and legal considerations', 'facial recognition technology'", "ccs_concepts": "'Social and professional topics _ Surveillance', 'Social and professional topics _ Technology audits', 'Security and privacy _ Human and societal aspects of security and privacy'", "author_names": "'Evani Radiya-Dixit', 'Gina Neff'", "author_affiliations": "'Minderoo Centre for Technology and Democracy, University of Cambridge', 'Minderoo Centre for Technology and Democracy, University of Cambridge'"}, {"link": "https://doi.org/10.1145/3593013.3594090", "title": "Navigating the Audit Landscape: A Framework for Developing Transparent and Auditable XR", "abstract": "\u201a\u00c4\u00faExtended reality\u201a\u00c4\u00f9 (XR) systems work to blend the physical and digital worlds. This means that XR is highly contextual: its functionality, operation and therefore consequences are driven by a tight, run-time coupling of the technology, the user, and their physical environment. It follows that XR brings particular challenges regarding transparency and accountability, given that it can be difficult to foresee and mitigate all potential issues that might arise from using such systems, given their many potential contexts of use. Further, the physicality of XR can directly result in injury, property damage, or worse, in addition to the more traditionally discussed harms arising from algorithmic systems. Therefore the ability to audit the operation of XR systems is paramount \u201a\u00c4\u00ec where information revealing and enabling some reconstruction of an XR system\u201a\u00c4\u00f4s use, run-time behaviour, and surrounding context is important for understanding and scrutinising what happens/happened, and why.  Towards this, we present a framework to support those involved in developing XR systems to make them more auditable. The framework focuses on supporting the building and instrumentation of an XR system for transparency aims, elaborating key considerations regarding the capture and management of audit data during system operation. We demonstrate the framework\u201a\u00c4\u00f4s efficacy with expert XR developers, who indicate the utility and need for such in practice. In all, we provide practical ways forward on, as well as seek to draw attention to, XR transparency and accountability.", "keywords": "'audit', 'transparency', 'accountability', 'responsibility', 'auditability', 'reviewability', 'trust', 'augmented reality', 'virtual reality', 'mixed reality'", "ccs_concepts": "'Human-centered computing _ Ubiquitous and mobile computing design and evaluation methods', 'Human-centered computing _ Mixed / augmented reality', 'Human-centered computing _ Virtual reality', 'Social and professional topics _ Technology audits', 'Human-centered computing _ Ubiquitous and mobile computing systems and tools'", "author_names": "'Chris Norval', 'Richard Cloete', 'Jatinder Singh'", "author_affiliations": "'University of Cambridge', 'University of Harvard', 'University of Cambridge'"}, {"link": "https://doi.org/10.1145/3593013.3594079", "title": "AI Regulation Is (Not) All You Need", "abstract": "The development of processes and tools for ethical, trustworthy, and legal AI is only beginning. At the same time, legal requirements are emerging in various jurisdictions, following a deluge of ethical guidelines. It is therefore key to explore the necessary practices that must be adopted to ensure the quality of AI systems, mitigate their potential risks and enable legal compliance. Ensuring that the potential negative impacts of AI on individuals, society, and the environment are mitigated will depend on many factors, including the capacity to properly regulate its deployment and to mandate necessary internal best practices along lifecycles. Regulatory frameworks must evolve from abstract requirements to providing concrete operational mandates that enable better oversight mechanisms in the way AI systems operate, how they are developed, and how they are deployed. In view of the above, this paper explores the necessary practices that can be adopted throughout a comprehensive lifecycle audit as a key practice to ensure the quality of AI systems and enable the development of compliance mechanisms. It also discusses novel governance tools that enable bridging the current operational gaps. Such gaps were identified by interviewing experts, analysing adaptable tools and methodologies from the software engineering domain, and by exploring the state of the art of auditing. The results present recommendations for novel tools and oversight mechanisms for governing AI systems.", "keywords": "'AI regulation', 'algorithmic auditing', 'machine learning', 'ethical AI'", "ccs_concepts": "'Social and professional topics _ Governmental regulations', 'Human-centered computing'", "author_names": "'Laura Lucaj', 'Patrick van der Smagt', 'Djalel Benbouzid'", "author_affiliations": "'Machine Learning Research Lab, Volkswagen Group', 'Machine Learning Research Lab, Volkswagen Group', 'Machine Learning Research Lab, Volkswagen Group'"}, {"link": "https://doi.org/10.1145/3593013.3594104", "title": "Interrogating the T in FAccT", "abstract": "Fairness, accountability, and transparency are the three conceptual foundations of the FAccT conference. Transparency, however, has yet to be scrutinized to the same degree as accountability and fairness. As a result, we don't know: What does this community mean when it talks about transparency? How are we doing transparency? And to what ends? What commitments does (or should) the T in FAccT signify? This paper interrogates the T in FAccT using perspectives from critical transparency literature. Subsequently, we argue that FAccT might be better off dropping the T from its title for two reasons: (1) transparency can often be counterproductive to FAccT's primary objectives and (2) it is misleading as FAccT is mainly preoccupied with explainability rather than actual transparency. If we want to keep the T, we need to reframe how we think about and do transparency by making transparency contingent, reclaiming it from explainability, and bringing people into transparency processes.", "keywords": "'Transparency', 'Explainability', 'Interpretability', 'Critical Transparency Studies'", "ccs_concepts": "'Human-centered computing', 'Human computer interaction (HCI)', 'HCI theory', 'concepts and models', 'Transparency', 'Explainability', 'Interpretability', 'Critical Transparency Studies'", "author_names": "'Eric Corbett', 'Emily Denton'", "author_affiliations": "'Google Research', 'Google Research'"}, {"link": "https://doi.org/10.1145/3593013.3594019", "title": "Representation in AI Evaluations", "abstract": "Calls for representation in artificial intelligence (AI) and machine learning (ML) are widespread, with \"representation\" or \"representativeness\" generally understood to be both an instrumentally and intrinsically beneficial quality of an AI system, and central to fairness concerns. But what does it mean for an AI system to be \"representative\"? Each element of the AI lifecycle is geared towards its own goals and effect on the system, therefore requiring its own analyses with regard to what kind of representation is best. In this work we untangle the benefits of representation in AI evaluations to develop a framework to guide an AI practitioner or auditor towards the creation of representative ML evaluations. Representation, however, is not a panacea. We further lay out the limitations and tensions of instrumentally representative datasets, such as the necessity of data existence and access, surveillance vs expectations of privacy, implications for foundation models and power. This work sets the stage for a research agenda on representation in AI, which extends beyond instrumentally valuable representation in evaluations towards refocusing on, and empowering, impacted communities.", "keywords": "'datasets', 'responsible AI', 'machine learning evaluation'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Computing methodologies _ Artificial intelligence'", "author_names": "'A. Stevie Bergman', 'Lisa Anne Hendricks', 'Maribeth Rauh', 'Boxi Wu', 'William Agnew', 'Markus Kunesch', 'Isabella Duan', 'Iason Gabriel', 'William Isaac'", "author_affiliations": "'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'University of Washington', 'DeepMind', 'University of Chicago', 'DeepMind', 'DeepMind'"}, {"link": "https://doi.org/10.1145/3593013.3594082", "title": "A Systematic Review of Ethics Disclosures in Predictive Mental Health Research", "abstract": "Applied machine learning (ML) has not yet coalesced on standard practices for research ethics. For ML that predicts mental illness using social media data, ambiguous ethical standards can impact peoples\u201a\u00c4\u00f4 lives because of the area\u201a\u00c4\u00f4s sensitivity and material consequences on health. Transparency of current ethics practices in research is important to document decision-making and improve research practice. We present a systematic literature review of 129 studies that predict mental illness using social media data and ML, and the ethics disclosures they make in research publications. Rates of disclosure are going up over time, but this trend is slow moving \u201a\u00c4\u00ec it will take another eight years for the average paper to have coverage on 75% of studied ethics categories. Certain practices are more readily adopted, or \"stickier\", over time, though we found prioritization of data-driven disclosures rather than human-centered. These inconsistently reported ethical considerations indicate a gap between what ML ethicists believe ought to be and what actually is done. We advocate for closing this gap through increased transparency of practice and formal mechanisms to support disclosure.", "keywords": "'ethics', 'mental health', 'systematic literature review', 'social media'", "ccs_concepts": "'Human-centered computing _ HCI theory', 'concepts and models', 'General and reference _ Surveys and overviews', 'Computing methodologies _ Machine learning'", "author_names": "'Leah Hope Ajmani', 'Stevie Chancellor', 'Bijal Mehta', 'Casey Fiesler', 'Michael Zimmer', 'Munmun De Choudhury'", "author_affiliations": "'GroupLens, University of Minnesota', 'GroupLens, University of Minnesota', 'Georgetown University', 'University of Colorado Boulder', 'Marquette University', 'Georgia Institute of Technology'"}, {"link": "https://doi.org/10.1145/3593013.3593985", "title": "WEIRD FAccTs: How Western, Educated, Industrialized, Rich, and Democratic is FAccT?", "abstract": "Studies conducted on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) samples are considered atypical of the world\u201a\u00c4\u00f4s population and may not accurately represent human behavior. In this study, we aim to quantify the extent to which the ACM FAccT conference, the leading venue in exploring Artificial Intelligence (AI) systems\u201a\u00c4\u00f4 fairness, accountability, and transparency, relies on WEIRD samples. We collected and analyzed 128 papers published between 2018 and 2022, accounting for 30.8% of the overall proceedings published at FAccT in those years (excluding abstracts, tutorials, and papers without human-subject studies or clear country attribution for the participants). We found that 84% of the analyzed papers were exclusively based on participants from Western countries, particularly exclusively from the U.S. (63%). Only researchers who undertook the effort to collect data about local participants through interviews or surveys added diversity to an otherwise U.S.-centric view of science. Therefore, we suggest that researchers collect data from under-represented populations to obtain an inclusive worldview. To achieve this goal, scientific communities should champion data collection from such populations and enforce transparent reporting of data biases.", "keywords": NaN, "ccs_concepts": "'Social and professional topics', 'Computing methodologies', 'Software and its engineering', 'Human-centered computing _ Human computer interaction (HCI)'", "author_names": "'Ali Akbar Septiandri', 'Marios Constantinides', 'Mohammad Tahaei', 'Daniele Quercia'", "author_affiliations": "'Nokia Bell Labs', 'Nokia Bell Labs', 'Nokia Bell Labs', 'Nokia Bell Labs'"}, {"link": "https://doi.org/10.1145/3593013.3593972", "title": "How to Explain and Justify Almost Any Decision: Potential Pitfalls for Accountability in AI Decision-Making", "abstract": "Discussion of the \u201a\u00c4\u00faright to an explanation\u201a\u00c4\u00f9 has been increasingly relevant because of its potential utility for auditing automated decision systems, as well as for making objections to such decisions. However, most existing work on explanations focuses on collaborative environments, where designers are motivated to implement good-faith explanations that reveal potential weaknesses of a decision system. This motivation may not hold in an auditing environment. Thus, we ask: how much could explanations be used maliciously to defend a decision system? In this paper, we demonstrate how a black-box explanation system developed to defend a black-box decision system could manipulate decision recipients or auditors into accepting an intentionally discriminatory decision model. In a case-by-case scenario where decision recipients are unable to share their cases and explanations, we find that most individual decision recipients could receive a verifiable justification, even if the decision system is intentionally discriminatory. In a system-wide scenario where every decision is shared, we find that while justifications frequently contradict each other, there is no intuitive threshold to determine if these contradictions are because of malicious justifications or because of simplicity requirements of these justifications conflicting with model behavior. We end with discussion of how system-wide metrics may be more useful than explanation systems for evaluating overall decision fairness, while explanations could be useful outside of fairness auditing.", "keywords": "'explainable AI', 'right to an explanation', 'adversarial explanations'", "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Human-centered computing _ Interactive systems and tools', 'Information systems _ Decision support systems'", "author_names": "'Joyce Zhou', 'Thorsten Joachims'", "author_affiliations": "'Cornell University', 'Cornell University'"}, {"link": "https://doi.org/10.1145/3593013.3594053", "title": "Questioning the Ability of Feature-Based Explanations to Empower Non-Experts in Robo-Advised Financial Decision-Making", "abstract": "Robo-advisors are democratizing access to life-insurance by enabling fully online underwriting. In Europe, financial legislation requires that the reasons for recommending a life insurance plan be explained according to the characteristics of the client, in order to empower the client to make a \u201a\u00c4\u00fafully informed decision\u201a\u00c4\u00f9. In this study conducted in France, we seek to understand whether legal requirements for feature-based explanations actually help users in their decision-making. We conduct a qualitative study to characterize the explainability needs formulated by non-expert users and by regulators expert in customer protection. We then run a large-scale quantitative study using Robex, a simplified robo-advisor built using ecological interface design that delivers recommendations with explanations in different hybrid textual and visual formats: either \u201a\u00c4\u00fadialogic\u201a\u00c4\u00f9\u201a\u00c4\u00eemore textual\u201a\u00c4\u00eeor \u201a\u00c4\u00fagraphical\u201a\u00c4\u00f9\u201a\u00c4\u00eemore visual. We find that providing feature-based explanations does not improve appropriate reliance or understanding compared to not providing any explanation. In addition, dialogic explanations increase users\u201a\u00c4\u00f4 trust in the recommendations of the robo-advisor, sometimes to the users\u201a\u00c4\u00f4 detriment. This real-world scenario illustrates how XAI can address information asymmetry in complex areas such as finance. This work has implications for other critical, AI-based recommender systems, where the General Data Protection Regulation (GDPR) may require similar provisions for feature-based explanations.", "keywords": "'explainability', 'intelligibility', 'AI regulation', 'financial inclusion'", "ccs_concepts": "'Human-centered computing _ Empirical studies in HCI'", "author_names": "'Astrid Bertrand', 'James R. Eagan', 'Winston Maxwell'", "author_affiliations": "'LTCI, T\u221a\u00a9l\u221a\u00a9com Paris, Institut Polytechnique de Paris', 'LTCI, T\u221a\u00a9l\u221a\u00a9com Paris, Institut Polytechnique de Paris', 'i3, CNRS, T\u221a\u00a9l\u221a\u00a9com Paris, Institut Polytechnique de Paris'"}, {"link": "https://doi.org/10.1145/3593013.3594001", "title": "Explainable AI is Dead, Long Live Explainable AI! Hypothesis-Driven Decision Support Using Evaluative AI", "abstract": "In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their explanations) take control from human decision makers, limiting their agency. Second, giving recommendations and explanations does not align with the cognitive processes employed by people making decisions. This position paper proposes a new conceptual framework called Evaluative AI for explainable decision support. This is a machine-in-the-loop paradigm in which decision support tools provide evidence for and against decisions made by people, rather than provide recommendations to accept or reject. We argue that this mitigates issues of over- and under-reliance on decision support tools, and better leverages human expertise in decision making.", "keywords": NaN, "ccs_concepts": "'Computing methodologies _ Artificial intelligence', 'Human-centered computing _ HCI theory', 'concepts and models'", "author_names": "'Tim Miller'", "author_affiliations": "'The University of Melbourne'"}, {"link": "https://doi.org/10.1145/3593013.3594074", "title": "Explainability in AI Policies: A Critical Review of Communications, Reports, Regulations, and Standards in the EU, US, and UK", "abstract": "Public attention towards explainability of artificial intelligence (AI) systems has been rising in recent years to offer methodologies for human oversight. This has translated into the proliferation of research outputs, such as from Explainable AI, to enhance transparency and control for system debugging and monitoring, and intelligibility of system process and output for user services. Yet, such outputs are difficult to adopt on a practical level due to a lack of a common regulatory baseline, and the contextual nature of explanations. Governmental policies are now attempting to tackle such exigence, however it remains unclear to what extent published communications, regulations, and standards adopt an informed perspective to support research, industry, and civil interests. In this study, we perform the first thematic and gap analysis of this plethora of policies and standards on explainability in the EU, US, and UK. Through a rigorous survey of policy documents, we first contribute an overview of governmental regulatory trajectories within AI explainability and its sociotechnical impacts. We find that policies are often informed by coarse notions and requirements for explanations. This might be due to the willingness to conciliate explanations foremost as a risk management tool for AI oversight, but also due to the lack of a consensus on what constitutes a valid algorithmic explanation, and how feasible the implementation and deployment of such explanations are across stakeholders of an organization. Informed by AI explainability research, we then conduct a gap analysis of existing policies, which leads us to formulate a set of recommendations on how to address explainability in regulations for AI systems, especially discussing the definition, feasibility, and usability of explanations, as well as allocating accountability to explanation providers.", "keywords": "'Explainable AI', 'AI policy', 'social epistemology'", "ccs_concepts": "'Applied computing _ Law', 'Social and professional topics _ Governmental regulations'", "author_names": "'Luca Nannini', 'Agathe Balayn', 'Adam Leon Smith'", "author_affiliations": "'Minsait - Indra Sistemas, Spain and CiTIUS (Centro Singular de Investigaci\u221a\u2265n en Tecnolox\u221a\u2260as Intelixentes), Universidade de Santiago de Compostela', 'Delft University of Technology', 'Dragonfly'"}, {"link": "https://doi.org/10.1145/3593013.3594031", "title": "Care and Coordination in Algorithmic Systems: An Economies of Worth Approach", "abstract": "Algorithmic decision-making has permeated health and care domains (e.g., automated diagnoses, fall detection, caregiver staffing). Researchers have raised concerns about how these algorithms are built and how they shape fair and ethical care practices. To investigate algorithm development and understand its impact on people who provide and coordinate care, we conducted a case study of a U.S.-based senior care network and platform. We interviewed 14 technologists, 9 paid caregivers, and 7 care coordinators to explore their interactions with the platform\u201a\u00c4\u00f4s algorithms. We find that technologists draw on a multitude of moral frameworks to navigate complex and contradictory demands and expectations. Despite technologists\u201a\u00c4\u00f4 espoused commitments to fairness, accountability, and transparency, the platform reassembles problematic aspects of care labor. By analyzing how technologists justify their work, the problems that they claim to solve, the solutions they present, and caregivers\u201a\u00c4\u00f4 and coordinators\u201a\u00c4\u00f4 experiences, we advance fairness research that focuses on agency and power asymmetries in algorithmic platforms. We (1) make an empirical contribution, revealing tensions when developing and implementing algorithms and (2) provide insight into the social processes that reproduce power asymmetries in algorithmic decision-making.", "keywords": "'algorithms', 'care', 'coordination', 'morality', 'qualitative study'", "ccs_concepts": "'Human-centered computing _ Empirical studies in HCI', 'Human-centered computing _ Field studies'", "author_names": "'John Rudnik', 'Robin Brewer'", "author_affiliations": "'School of Information, University of Michigan', 'School of Information, University of Michigan'"}, {"link": "https://doi.org/10.1145/3593013.3593975", "title": "Two Reasons for Subjecting Medical AI Systems to Lower Standards than Humans", "abstract": "This paper concerns the double standard debate in the ethics of AI literature. This debate revolves around the question of whether we should subject AI systems to different normative standards than humans. So far, the debate has centered around transparency. That is, the debate has focused on whether AI systems must be more transparent than humans in their decision-making processes in order for it to be morally permissible to use such systems. Some have argued that the same standards of transparency should be applied to AI systems and humans. Others have argued that we should hold AI systems to higher standards than humans in terms of transparency. In this paper, we first highlight that debates concerning double standards, which have a similar structure to those related to transparency, exist in relation to other values such as predictive accuracy. Second, we argue that when we focus on predictive accuracy, there are at least two reasons for holding AI systems to a lower standard than humans.", "keywords": "'Double Standard', 'Predictive Accuracy', 'Opacity', 'Cost-effectiveness', 'Speed'", "ccs_concepts": NaN, "author_names": "'Jakob Mainz', 'Lauritz Munch', 'Jens Christian Bjerring'", "author_affiliations": "'Novo Nordisk', 'Aarhus University', 'Aarhus University'"}, {"link": "https://doi.org/10.1145/3593013.3594038", "title": "Your Browsing History May Cost You: A Framework for Discovering Differential Pricing in Non-Transparent Markets", "abstract": "In many online markets we \u201a\u00c4\u00fashop alone\u201a\u00c4\u00f9 \u201a\u00c4\u00ee there is no way for us to know the prices other consumers paid for the same goods. Could this lack of price transparency lead to differential pricing? To answer this question, we present a generalized framework to audit online markets for differential pricing using automated agents. Consensus is a key idea in our work: for a successful black-box audit, both the experimenter and seller must agree on the agents\u201a\u00c4\u00f4 attributes. We audit two competitive online travel markets on kayak.com (flight and hotel markets) and construct queries representative of the demand for goods. Crucially, we assume ignorance of the sellers\u201a\u00c4\u00f4 pricing mechanisms while conducting these audits. We conservatively implement consensus with nine distinct profiles based on behavior, not demographics. We use a structural causal model for price differences and estimate model parameters using Bayesian inference. We can unambiguously show that many sellers (but not all) demonstrate behavior-driven differential pricing. In the flight market, some profiles are nearly more likely to see a worse price than the best performing profile, and nearly more likely in the hotel market. While the control profile (with no browsing history) was on average offered the best prices in the flight market, surprisingly, other profiles outperformed the control in the hotel market. The price difference between any pair of profiles occurring by chance is $\u201a\u00c4\u00e20.44 in the flight market and $\u201a\u00c4\u00e20.09 for hotels. However, the expected loss of welfare for any profile when compared to the best profile can be as much as $\u201a\u00c4\u00e26.00 for flights and $\u201a\u00c4\u00e23.00 for hotels (i.e., 15 \u221a\u00f3 and 33 \u221a\u00f3 the price difference by chance respectively). This illustrates the need for new market designs or policies that encourage more transparent market design to overcome differential pricing practices.", "keywords": NaN, "ccs_concepts": "'General and reference _ Empirical studies', 'Information systems _ E-commerce infrastructure', 'Theory of computation _ Bayesian analysis'", "author_names": "'Aditya Karan', 'Naina Balepur', 'Hari Sundaram'", "author_affiliations": "'Department of Computer Science, University of Illinois at Urbana-Champaign', 'Department of Computer Science, University of Illinois at Urbana-Champaign', 'Department of Computer Science, University of Illinois at Urbana-Champaign'"}, {"link": "https://doi.org/10.1145/3593013.3594017", "title": "A Theory of Auditability for Allocation and Social Choice Mechanisms", "abstract": "In centralized market mechanisms individuals may not fully observe other participants' type reports. Hence, the mechanism designer may deviate from the promised mechanism without the individuals being able to detect these deviations. In this paper, we develop a theory of auditability for allocation and social choice problems. Namely, we measure a mechanism's auditabilty by the smallest number of individuals that can jointly detect any deviation. Our theory reveals stark contrasts between prominent mechanisms' auditabilities in various applications. For priority-based allocation problems, we find that the Immediate Acceptance mechanism is maximally auditable, in a sense that any deviation can always be detected by just two individuals, whereas, on the other extreme, the Deferred Acceptance mechanism is minimally auditable, in a sense that some deviations may go undetected unless some individuals possess full information about everyone's reports. For the auctions setup, we find a similar contrast between the first-price and the second-price auction mechanisms. For voting problems, we characterize the majority voting rule as the unique most auditable anonymous voting mechanism. And finally, for the choice with affirmative action setting, we compare the auditability indices of prominent reserves mechanisms.", "keywords": "'auditability', 'mechanisms', 'auctions', 'allocation', 'affirmative action'", "ccs_concepts": "'Accountability', 'Auditing', 'Economics', 'Explainability', 'Mechanism Design', 'Social Choice'", "author_names": "'Aram Grigoryan', 'Markus M\u221a\u2202ller'", "author_affiliations": "'University of California, San Diego', 'University of Bonn'"}, {"link": "https://doi.org/10.1145/3593013.3594060", "title": "Towards Labor Transparency in Situated Computational Systems Impact Research", "abstract": "Researchers seeking to examine and prevent technology-mediated harms have emphasized the importance of directly engaging with community stakeholders through participatory approaches to computational systems research. However, recent transformations in strategies of corporate capture within the tech industry pose significant challenges to established participatory practices. In this paper we extend existing critical participatory design scholarship to highlight the exploitative potential of labor relationships in community collaborations between researchers and participants. Drawing on a reflexive approach to our own experiences conducting agonistic participatory research on emerging technologies at a large technology company, we highlight the limitations of doing participatory work within such contexts by empirically illustrating how and when these relationships threaten to appropriate and alienate participant labor. We argue that a labor-conscious approach to computational systems impact research is critical for countering the commodification of inclusion and invite fellow researchers to more actively investigate such dynamics. To this end, we provide (1) a framework for documenting divisions of labor within participatory research, design, and data practices, and (2) a series of short provocations that help locate and inventory sites of extraction within participatory engagements.", "keywords": "'impact', 'inclusion', 'labor', 'participatory design', 'agonism', 'documentation', 'transparency'", "ccs_concepts": "'Human-centered computing _ Participatory design', 'Human-centered computing _ Collaborative and social computing design and evaluation methods'", "author_names": "'Felicia S. Jing', 'Sara E. Berger', 'Juana Catalina Becerra Sandoval'", "author_affiliations": "'Responsible & Inclusive Technologies, IBM Research, USA and Department of Political Science, Johns Hopkins University', 'Responsible & Inclusive Technologies, IBM Research', 'Responsible & Inclusive Technologies, IBM Research'"}, {"link": "https://doi.org/10.1145/3593013.3594070", "title": "The Dimensions of Data Labor: A Road Map for Researchers, Activists, and Policymakers to Empower Data Producers", "abstract": "Many recent technological advances (e.g. ChatGPT and search engines) are possible only because of massive amounts of user-generated data produced through user interactions with computing systems or scraped from the web (e.g. behavior logs, user-generated content, and artwork). However, data producers have little say in what data is captured, how it is used, or who it benefits. Organizations with the ability to access and process this data, e.g. OpenAI and Google, possess immense power in shaping the technology landscape. By synthesizing related literature that reconceptualizes the production of data for computing as \u201a\u00c4\u00fadata labor\u201a\u00c4\u00f9, we outline opportunities for researchers, policymakers, and activists to empower data producers in their relationship with tech companies, e.g advocating for transparency about data reuse, creating feedback channels between data producers and companies, and potentially developing mechanisms to share data\u201a\u00c4\u00f4s revenue more broadly. In doing so, we characterize data labor with six important dimensions - legibility, end-use awareness, collaboration requirement, openness, replaceability, and livelihood overlap - based on the parallels between data labor and various other types of labor in the computing literature.", "keywords": "'user-generated data', 'empowerment', 'data leverage'", "ccs_concepts": "'Human-centered computing _ HCI theory', 'concepts and models'", "author_names": "'Hanlin Li', 'Nicholas Vincent', 'Stevie Chancellor', 'Brent Hecht'", "author_affiliations": "'University of California, Berkeley', 'University of California, Davis', 'University of Minnesota', 'Northwestern University'"}, {"link": "https://doi.org/10.1145/3593013.3594039", "title": "Add-Remove-or-Relabel: Practitioner-Friendly Bias Mitigation via Influential Fairness", "abstract": "Commensurate with the rise in algorithmic bias research, myriad algorithmic bias mitigation strategies have been proposed in the literature. Nonetheless, many voice concerns about the lack of transparency that accompanies mitigation methods and the paucity of mitigation methods that satisfy protocol and data limitations of practitioners. Influence functions from robust statistics provide a novel opportunity to overcome both issues. Previous work demonstrates the power of influence functions to improve fairness outcomes. This work proposes a novel family of fairness solutions, coined influential fairness (IF), that is human-understandable and also agnostic to the underlying machine learning model and choice of fairness metric. We conduct an investigation of practitioner profiles and design mitigation methods for practitioners whose limitations discourage them from utilizing existing bias mitigation methods.", "keywords": "'machine learning', 'fairness', 'ethics', 'bias mitigation'", "ccs_concepts": NaN, "author_names": "'Brianna Richardson', 'Prasanna Sattigeri', 'Dennis Wei', 'Karthikeyan Natesan Ramamurthy', 'Kush Varshney', 'Amit Dhurandhar', 'Juan E. Gilbert'", "author_affiliations": "'University of Florida', 'IBM', 'IBM', 'IBM', 'IBM', 'IBM', 'University of Florida'"}, {"link": "https://doi.org/10.1145/3593013.3594008", "title": "Domain Adaptive Decision Trees: Implications for Accuracy and Fairness", "abstract": "In uses of pre-trained machine learning models, it is a known issue that the target population in which the model is being deployed may not have been reflected in the source population with which the model was trained. This can result in a biased model when deployed, leading to a reduction in model performance. One risk is that, as the population changes, certain demographic groups will be under-served or otherwise disadvantaged by the model, even as they become more represented in the target population. The field of domain adaptation proposes techniques for a situation where label data for the target population does not exist, but some information about the target distribution does exist. In this paper we contribute to the domain adaptation literature by introducing domain-adaptive decision trees (DADT). We focus on decision trees given their growing popularity due to their interpretability and performance relative to other more complex models. With DADT we aim to improve the accuracy of models trained in a source domain (or training data) that differs from the target domain (or test data). We propose an in-processing step that adjusts the information gain split criterion with outside information corresponding to the distribution of the target population. We demonstrate DADT on real data and find that it improves accuracy over a standard decision tree when testing in a shifted target population. We also study the change in fairness under demographic parity and equal opportunity. Results show an improvement in fairness with the use of DADT.", "keywords": "'Decision Trees', 'Information Gain', 'Domain Adaptation', 'Covariate Shift', 'Fairness', 'folktables'", "ccs_concepts": "'Computing methodologies _ Classification and regression trees', 'Computing methodologies _ Learning under covariate shift', 'Computing methodologies _ Transfer learning'", "author_names": "'Jose M. Alvarez', 'Kristen M. Scott', 'Bettina Berendt', 'Salvatore Ruggieri'", "author_affiliations": "'Scuola Normale Superiore, University of Pisa', 'KU Leuven', 'TU Berlin, Weizenbaum Institute, Germany and KU Leuven', 'University of Pisa'"}, {"link": "https://doi.org/10.1145/3593013.3594088", "title": "(Anti)-Intentional Harms: The Conceptual Pitfalls of Emotion AI in Education", "abstract": "\u201a\u00c4\u00f2Emotion AI\u201a\u00c4\u00f4 is a subset of artificial intelligence (AI) technologies that claim to be able to detect the inner emotional states of individuals by collecting biometric information such as face scans, voice recordings, and traces of physical movement. Despite their growing popularity in education, these systems have the potential to produce serious harm. In this paper, we argue that a major concern with emotion AI technologies has to do with the theories of emotion that undergird them. Most emotion AI technologies are built on the foundations of anti-intentionalist theories of human emotion, which claim that emotions can be understood as discrete, universal states that arise as automatic physiological responses. Anti-intentionalists suggest that emotions are not directed at any object, or subject to cognitive reasons. In our work, we focus on the increasing use of these technologies in education to illustrate the ways in which these anti-intentionalist systems are problematic, as they dissolve the space for pushback against the judgements they make. We argue that their use thereby contributes to harms towards children broadly centered around student disempowerment, surveillance, and classification. We then consider three alternative policy approaches to emotion AI use in schools in light of their role with this political agenda of emotion commodification, assessing each of these options\u201a\u00c4\u00eeinterpretability, technical reform, and non-use\u201a\u00c4\u00eefor their desirability and feasibility. In doing so, we underscore the conceptual harms produced by emotion AI systems in the context of education, and the criteria by which these technologies should be judged by educators and policymakers.", "keywords": "'emotion', 'affect', 'emotion AI', 'education', 'socio-emotional learning (SEL)', 'Basic Emotion Theory (BET)', 'anti-intentionalism', 'affective computing', 'artificial intelligence', 'theories of emotion', 'intentionalism', 'human capital', 'digital classroom'", "ccs_concepts": "'Social and professional topics _ Professional topics _ Computing education _ K-12 education', 'Applied computing _ Education _ Learning management systems', 'Applied computing _ Education _ Computer-managed instruction', 'Applied computing _ Education _ E-learning'", "author_names": "'Nathalie DiBerardino', 'Luke Stark'", "author_affiliations": "'Department of Philosophy, Western University', 'Faculty of Information and Media Studies, Western University'"}, {"link": "https://doi.org/10.1145/3593013.3594011", "title": "On the Praxes and Politics of AI Speech Emotion Recognition", "abstract": "There is no scientific consensus on what is meant by \u201a\u00c4\u00faemotion\u201a\u00c4\u00f9 \u201a\u00c4\u00ec researchers have examined various phenomena spanning brain modes, feelings, sensations, and cognitive structures, among others, in their study of emotional experiences. For the purposes of developing an AI speech emotion recognition (SER) system, however, emotion must be defined, bounded, and instantiated as ground truth in the training data. This means practical choices must be made in which particular emotional ontologies are prioritized over others in the construction of SER datasets. In this paper, I explore these tensions around fairness, accountability, and transparency by analyzing open-source datasets used for SER applications along with their accompanying methodology papers. Specifically, I critique the centrality of discrete emotion theory in SER applications as a contestable emotional framework that is invoked primarily for its practical utility and alignment \u201a\u00c4\u00ec as opposed to scientific rigor \u201a\u00c4\u00ec with machine learning epistemologies. In so doing, I also shed light on the role of the dataset creators as emotional designers in their attempt to produce, elicit, record, and index emotional expressions for the purposes of crafting SER training datasets. Ultimately, by further querying SER through the aperture of Critical Disability Studies, I use this empirical work to examine the sociopolitical stakes of SER as a normative and regulatory technology that siphons emotion into a broader agenda of capitalistic productivity in the context of call center optimization.", "keywords": "'Emotion AI', 'speech emotion recognition', 'affective computing', 'critical study of AI', 'disability and AI', 'social critique of AI'", "ccs_concepts": "'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Human-centered computing _ HCI theory', 'concepts and models', 'Social and professional topics _ Computing profession'", "author_names": "'Edward B. Kang'", "author_affiliations": "'Annenberg School for Communication and Journalism, University of Southern California'"}, {"link": "https://doi.org/10.1145/3593013.3594063", "title": "Ethical Considerations in the Early Detection of Alzheimer's Disease Using Speech and AI", "abstract": "While recent studies indicate that AI could play an important role in detecting early signs of Alzheimer's disease in speech, this use of data from individuals with cognitive decline raises numerous ethical concerns. In this paper, we identify and explain concerns related to autonomy (including consent, depersonalization and disclosure), privacy and data protection (including the handling of personal content and medical information), welfare (including distress, discrimination and reliability), transparency (including the interpretability of language features and AI-based decision-making for developers and clinicians), and fairness (including bias and the distribution of benefits). Our aim is to not only raise awareness of the ethical concerns posed by the use of AI in speech-based Alzheimer's detection, but also identify ways in which these concerns might be addressed. To this end, we conclude with a list of suggestions that could be incorporated into ethical guidelines for researchers and clinicians working in this area.", "keywords": "'ethics', '\"Alzheimers disease\"', 'speech', 'language', 'digital biomarkers', 'autonomy', 'privacy', 'welfare', 'transparency', 'fairness'", "ccs_concepts": "'Applied computing _ Health informatics', 'Computing methodologies _ Natural language processing', 'Social and professional topics _ Medical technologies'", "author_names": "'Ulla Petti', 'Rune Nyrup', 'Jeffrey M. Skopek', 'Anna Korhonen'", "author_affiliations": "'University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge'"}, {"link": "https://doi.org/10.1145/3593013.3594014", "title": "Algorithms as Social-Ecological-Technological Systems: An Environmental Justice Lens on Algorithmic Audits", "abstract": "This paper reframes algorithmic systems as intimately connected to and part of social and ecological systems, and proposes a first-of-its-kind methodology for environmental justice-oriented algorithmic audits. How do we consider environmental and climate justice dimensions of the way algorithmic systems are designed, developed, and deployed? These impacts are inherently emergent and can only be understood and addressed at the level of relations between an algorithmic system and the social (including institutional) and ecological components of the broader ecosystem it operates in. As a result, we claim that in absence of an integral ontology for algorithmic systems, we cannot do justice to the emergent nature of broader environmental impacts of algorithmic systems and their underlying computational infrastructure. Furthermore, an integral lens provides many lessons from the history of environmental justice that are of relevance in current day struggles for algorithmic justice. We propose to define algorithmic systems as ontologically indistinct from Social-Ecological-Technological Systems (SETS), framing emergent implications as couplings between social, ecological, and technical components of the broader fabric in which algorithms are integrated and operate. We draw upon prior work on SETS analysis as well as emerging themes in the literature and practices of Environmental Justice (EJ) to conceptualize and assess algorithmic impact. We then offer three policy recommendations to help establish a SETS-based EJ approach to algorithmic audits: (1) broaden the inputs and open-up the outputs of an audit, (2) enable meaningful access to redress, and (3) guarantee a place-based and relational approach to the process of evaluating impact. We operationalize these as a qualitative framework of questions for a spectrum of stakeholders. Doing so, this article aims to inspire stronger and more frequent interactions across policymakers, researchers, practitioners, civil society, and grassroots communities. https://arxiv.org/abs/2305.05733.", "keywords": NaN, "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Social and professional topics _ Technology audits', 'Software and its engineering _ Software development process management'", "author_names": "'Bogdana Rakova', 'Roel Dobbe'", "author_affiliations": "'Mozilla Foundation', 'Delft University of Technology'"}, {"link": "https://doi.org/10.1145/3593013.3594107", "title": "Cross-Institutional Transfer Learning for Educational Models: Implications for Model Performance, Fairness, and Equity", "abstract": "Modern machine learning increasingly supports paradigms that are multi-institutional (using data from multiple institutions during training) or cross-institutional (using models from multiple institutions for inference), but the empirical effects of these paradigms are not well understood. This study investigates cross-institutional learning via an empirical case study in higher education. We propose a framework and metrics for assessing the utility and fairness of student dropout prediction models that are transferred across institutions. We examine the feasibility of cross-institutional transfer under real-world data- and model-sharing constraints, quantifying model biases for intersectional student identities, characterizing potential disparate impact due to these biases, and investigating the impact of various cross-institutional ensembling approaches on fairness and overall model performance. We perform this analysis on data representing over 200,000 enrolled students annually from four universities without sharing training data between institutions.  We find that a simple zero-shot cross-institutional transfer procedure can achieve similar performance to locally-trained models for all institutions in our study, without sacrificing model fairness. We also find that stacked ensembling provides no additional benefits to overall performance or fairness compared to either a local model or the zero-shot transfer procedure we tested. We find no evidence of a fairness-accuracy tradeoff across dozens of models and transfer schemes evaluated. Our auditing procedure also highlights the importance of intersectional fairness analysis, revealing performance disparities at the intersection of sensitive identity groups that are concealed under one-dimensional analysis.1", "keywords": "'Algorithmic Fairness', 'Education', 'Dropout Prediction', 'Transfer Learning', 'Intersectionality'", "ccs_concepts": "'Applied computing _ Law', 'social and behavioral sciences', 'Applied computing _ Education', 'Computing methodologies _ Machine learning'", "author_names": "'Joshua Gardner', 'Renzhe Yu', 'Quan Nguyen', 'Christopher Brooks', 'Rene Kizilcec'", "author_affiliations": "'University of Washington', 'Teachers College, Columbia University', 'University of British Columbia', 'University of Michigan', 'Cornell University'"}, {"link": "https://doi.org/10.1145/3593013.3594065", "title": "More Data Types More Problems: A Temporal Analysis of Complexity, Stability, and Sensitivity in Privacy Policies", "abstract": "Collecting personally identifiable information (PII) on data subjects has become big business. Data brokers and data processors are part of a multi-billion-dollar industry that profits from collecting, buying, and selling consumer data. Yet there is little transparency in the data collection industry which makes it difficult to understand what types of data are being collected, used, and sold, and thus the risk to individual data subjects. In this study, we examine a large textual dataset of privacy policies from 1997-2019 in order to investigate the data collection activities of data brokers and data processors. We also develop an original lexicon of PII-related terms representing PII data types curated from legislative texts. This mesoscale analysis looks at privacy policies over time on the word, topic, and network levels to understand the stability, complexity, and sensitivity of privacy policies over time. We find that (1) privacy legislation may be correlated with changes in stability and turbulence of PII data types in privacy policies; (2) the complexity of privacy policies decreases over time and becomes more regularized; (3) sensitivity rises over time and shows spikes that appear to be correlated with events when new privacy legislation is introduced.", "keywords": "'Privacy', 'NLP', 'Data Privacy', 'Data Ethics', 'Privacy Policies', 'Data Science', 'Networks'", "ccs_concepts": "'Social and professional topics _ Privacy policies', 'Networks _ Network dynamics', 'Security and privacy _ Privacy protections'", "author_names": "'Juniper Lovato', 'Philip Mueller', 'Parisa Suchdev', 'Peter Dodds'", "author_affiliations": "'Vermont Complex Systems Center, University of Vermont', 'Vermont Complex Systems Center, University of Vermont', 'Computer Science, University of Vermont', 'Vermont Complex Systems Center, Computer Science, University of Vermont'"}, {"link": "https://doi.org/10.1145/3593013.3594086", "title": "Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity", "abstract": "Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from queries when Nk \u201a\u00e2\u2122 n using techniques from compressed sensing (n is the size of the test dataset and Nk is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individuals? To address this supposed violation of regulations and privacy, we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters).", "keywords": "'algorithmic fairness', 'compliance', 'compressed sensing', 'differential privacy', 'machine learning.'", "ccs_concepts": "'Social and professional topics _ Governmental regulations', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Social and professional topics _ User characteristics', 'General and reference _ Evaluation', 'Security and privacy _ Privacy-preserving protocols'", "author_names": "'Faisal Hamman', 'Jiahao Chen', 'Sanghamitra Dutta'", "author_affiliations": "'Department of Electrical and Computer Engineering, University of Maryland, College Park', 'Responsible AI LLC', 'Department of Electrical and Computer Engineering, University of Maryland, College Park'"}, {"link": "https://doi.org/10.1145/3593013.3593974", "title": "Fairness in Machine Learning from the Perspective of Sociology of Statistics: How Machine Learning is Becoming Scientific by Turning Its Back on Metrological Realism", "abstract": "We argue in this article that the integration of fairness into machine learning, or FairML, is a valuable exemplar of the politics of statistics and their ongoing transformations. Classically, statisticians sought to eliminate any trace of politics from their measurement tools. But data scientists who are developing predictive machines for social applications \u201a\u00c4\u00ec are inevitably confronted with the problem of fairness. They thus face two difficult and often distinct types of demands: first, for reliable computational techniques, and second, for transparency, given the constructed, politically situated nature of quantification operations. We begin by socially localizing the formation of FairML as a field of research and describing the associated epistemological framework. We then examine how researchers simultaneously think the mathematical and social construction of approaches to machine learning, following controversies around fairness metrics and their status. Thirdly and finally, we show that FairML approaches tend towards a specific form of objectivity, \u201a\u00c4\u00fatrained judgement,\u201a\u00c4\u00f9 which is based on a reasonably partial justification from the designer of the machine \u201a\u00c4\u00ec which itself comes to be politically situated as a result.", "keywords": "'controversy mapping', 'epistemic virtues', 'objectivity', 'social epistemology', 'sociology of quantification', 'sociology of sciences and technologies', 'situated knowledge', 'fairness in machine learning'", "ccs_concepts": "'Social and professional topics'", "author_names": "'Bilel Benbouzid'", "author_affiliations": "'Universit\u221a\u00a9 Gustave Eiffel'"}, {"link": "https://doi.org/10.1145/3593013.3594098", "title": "Auditing Cross-Cultural Consistency of Human-Annotated Labels for Recommendation Systems", "abstract": "Recommendation systems increasingly depend on massive human-labeled datasets; however, the human annotators hired to generate these labels increasingly come from homogeneous backgrounds. This poses an issue when downstream predictive models\u201a\u00c4\u00eebased on these labels\u201a\u00c4\u00eeare applied globally to a heterogeneous set of users. We study this disconnect with respect to the labels themselves, asking whether they are \u201a\u00c4\u00faconsistently conceptualized\u201a\u00c4\u00f9 across annotators of different demographics. In a case study of video game labels, we conduct a survey on 5,174 gamers, identify a subset of inconsistently conceptualized game labels, perform causal analyses, and suggest both cultural and linguistic reasons for cross-country differences in label annotation. We further demonstrate that predictive models of game annotations perform better on global train sets as opposed to homogeneous (single-country) train sets. Finally, we provide a generalizable framework for practitioners to audit their own data annotation processes for consistent label conceptualization, and encourage practitioners to consider global inclusivity in recommendation systems starting from the early stages of annotator recruitment and data-labeling.", "keywords": "'label bias', 'human annotations', 'cultural bias', 'linguistic bias', 'poststratification', 'causal inference', 'recommendation systems'", "ccs_concepts": "'Social and professional topics _ Cultural characteristics', 'Human-centered computing _ Social recommendation'", "author_names": "'Rock Yuren Pang', 'Jack Cenatempo', 'Franklyn Graham', 'Bridgette Kuehn', 'Maddy Whisenant', 'Portia Botchway', 'Katie Stone Perez', 'Allison Koenecke'", "author_affiliations": "'University of Washington', 'Microsoft Research', 'Microsoft Corporation \u201a\u00c4\u00ec Xbox Division', 'Microsoft Corporation \u201a\u00c4\u00ec Xbox Division', 'Microsoft Corporation \u201a\u00c4\u00ec Xbox Division', 'Microsoft Corporation \u201a\u00c4\u00ec Xbox Division', 'Microsoft Corporation \u201a\u00c4\u00ec Xbox Division', 'Cornell University'"}, {"link": "https://doi.org/10.1145/3593013.3594043", "title": "\u201a\u00c4\u00faI Think You Might Like This\u201a\u00c4\u00f9: Exploring Effects of Confidence Signal Patterns on Trust in and Reliance on Conversational Recommender Systems", "abstract": "With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as \u201a\u00c4\u00fayou might like...\u201a\u00c4\u00f9 or \u201a\u00c4\u00faI think you will like...,\u201a\u00c4\u00f9 but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence, which is accurate, random, always low, or always high. Through semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and perceived anthropomorphism.", "keywords": "'human-AI interaction', 'conversational recommender systems', 'trust', 'reliance', 'confidence'", "ccs_concepts": "'Human-centered computing _ Empirical studies in HCI', 'Computing methodologies _ Artificial intelligence', 'Information systems _ Recommender systems'", "author_names": "'Marissa Radensky', 'Julie Anne S\u221a\u00a9guin', 'Jang Soo Lim', 'Kristen Olson', 'Robert Geiger'", "author_affiliations": "'Paul G. Allen Center for Computer Science & Engineering, University of Washington', 'Google', 'Creative Circle', 'Google', 'Google'"}, {"link": "https://doi.org/10.1145/3593013.3594009", "title": "Algorithmic Transparency and Accountability through Crowdsourcing: A Study of the NYC School Admission Lottery", "abstract": "Algorithms are used to aid decision-making for a wide range of public policy decisions. Yet, the details of the algorithmic processes and how to interact with their systems are often inadequately communicated to stakeholders, leaving them frustrated and distrusting of the outcomes of the decisions. Transparency and accountability are critical prerequisites for building trust in the results of decisions and guaranteeing fair and equitable outcomes. Unfortunately, organizations and agencies do not have strong incentives to explain and clarify their decision processes; however, stakeholders are not powerless and can strategically combine their efforts to push for more transparency.  In this paper, I discuss the results and lessons learned from such an effort: a parent-led crowdsourcing campaign to increase transparency in the New York City school admission process. NYC famously uses a deferred-acceptance matching algorithm to assign students to schools, but families are given very little, and often wrong, information on the mechanisms of the system in which they have to participate. Furthermore, the odds of matching to specific schools depend on a complex set of priority rules and tie-breaking random (lottery) numbers, whose impact on the outcome is not made clear to students and their families, resulting in many \u201a\u00c4\u00fawasted choices\u201a\u00c4\u00f9 on students\u201a\u00c4\u00f4 ranked lists and a high rate of unmatched students. Using the results of a crowdsourced survey of school application results, I was able to explain how random tie-breakers factored in the admission, adding clarity and transparency to the process. The results highlighted several issues and inefficiencies in the match and made the case for the need for more accountability and verification in the system.", "keywords": "'school matching', 'crowdsourcing', 'accountability', 'transparency'", "ccs_concepts": "'Social and professional topics _ Government technology policy', 'Human-centered computing _ User studies'", "author_names": "'Amelie Marian'", "author_affiliations": "'Computer Science, Rutgers University, New Brunswick'"}, {"link": "https://doi.org/10.1145/3593013.3594081", "title": "The Devil is in the Details: Interrogating Values Embedded in the Allegheny Family Screening Tool", "abstract": "The design decisions of developers and researchers in creating algorithmic tools \u201a\u00c4\u00ee like constructing variables, performing feature selection, and binning model outputs \u201a\u00c4\u00ee are sometimes cast as objective technical processes. In reality, these decisions are far from objective, and they are sometimes even made arbitrarily. In this work, we examine how algorithmic design choices can function as policy decisions through an audit of a deployed algorithmic tool, the Allegheny Family Screening Tool (AFST), used to screen calls to a child welfare agency about alleged child neglect in Allegheny County, Pennsylvania. We analyze design decisions in the AFST\u201a\u00c4\u00f4s development process related to feature selection, data collection, and post-processing, highlighting three values implicitly embedded in the tool through these decisions. By aggregating risk scores at the household level, the AFST effectively treats families as \u201a\u00c4\u00farisky\u201a\u00c4\u00f9 by association. In choosing to use training data from the criminal legal system and behavioral health agencies, the AFST prioritizes \u201a\u00c4\u00famaking decisions based on as much information as possible,\u201a\u00c4\u00f9 even when that information is potentially biased across race, disability, and other protected statuses. Finally, by including static features in the model that identify whether a person has ever been affected by the criminal legal system or relied on public benefits, the AFST chooses to mark families in perpetuity, compounding the impacts of systemic discrimination and foreclosing opportunities for recourse for families impacted by the tool. We explore the impacts of these decisions, individually and together, arguing that they function as policy choices that may have discriminatory effects and raise concerns about lack of democratic oversight.", "keywords": "'Algorithm', 'audit', 'policy', 'design', 'values', 'accountability'", "ccs_concepts": "'Applied computing _ Law', 'social and behavioral sciences', 'Social and professional topics _ Computing / technology policy'", "author_names": "'Marissa Gerchick', 'Tobi Jegede', 'Tarak Shah', 'Ana Gutierrez', 'Sophie Beiers', 'Noam Shemtov', 'Kath Xu', 'Anjana Samant', 'Aaron Horowitz'", "author_affiliations": "'American Civil Liberties Union', 'American Civil Liberties Union', 'Human Rights Data Analysis Group', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union'"}, {"link": "https://doi.org/10.1145/3593013.3594050", "title": "To Be High-Risk, or Not To Be\u201a\u00c4\u00eeSemantic Specifications and Implications of the AI Act\u201a\u00c4\u00f4s High-Risk AI Applications and Harmonised Standards", "abstract": "The EU\u201a\u00c4\u00f4s proposed AI Act sets out a risk-based regulatory framework to govern the potential harms emanating from use of AI systems. Within the AI Act\u201a\u00c4\u00f4s hierarchy of risks, the AI systems that are likely to incur \u201a\u00c4\u00fahigh-risk\u201a\u00c4\u00f9 to health, safety, and fundamental rights are subject to the majority of the Act\u201a\u00c4\u00f4s provisions. To include uses of AI where fundamental rights are at stake, Annex III of the Act provides a list of applications wherein the conditions that shape high-risk AI are described. For high-risk AI systems, the AI Act places obligations on providers and users regarding use of AI systems and keeping appropriate documentation through the use of harmonised standards. In this paper, we analyse the clauses defining the criteria for high-risk AI in Annex III to simplify identification of potential high-risk uses of AI by making explicit the \u201a\u00c4\u00facore concepts\u201a\u00c4\u00f9 whose combination makes them high-risk. We use these core concepts to develop an open vocabulary for AI risks (VAIR) to represent and assist with AI risk assessments in a form that supports automation and integration. VAIR is intended to assist with identification and documentation of risks by providing a common vocabulary that facilitates knowledge sharing and interoperability between actors in the AI value chain. Given that the AI Act relies on harmonised standards for much of its compliance and enforcement regarding high-risk AI systems, we explore the implications of current international standardisation activities undertaken by ISO and emphasise the necessity of better risk and impact knowledge bases such as VAIR that can be integrated with audits and investigations to simplify the AI Act\u201a\u00c4\u00f4s application.", "keywords": "'AI Act', 'high-risk AI', 'harmonised standards', 'taxonomy', 'semantic web'", "ccs_concepts": "'Social and professional topics _ Governmental regulations', 'Computing methodologies _ Knowledge representation and reasoning', 'Information systems _ Resource Description Framework (RDF)'", "author_names": "'Delaram Golpayegani', 'Harshvardhan J. Pandit', 'Dave Lewis'", "author_affiliations": "'ADAPT Centre, Trinity College Dublin', 'ADAPT Centre, Dublin City University', 'ADAPT Centre, Trinity College Dublin'"}, {"link": "https://doi.org/10.1145/3593013.3594069", "title": "The Role of Explainable AI in the Context of the AI Act", "abstract": "The proposed EU regulation for Artificial Intelligence (AI), the AI Act, has sparked some debate about the role of explainable AI (XAI) in high-risk AI systems. Some argue that black-box AI models will have to be replaced with transparent ones, others argue that using XAI techniques might help in achieving compliance. This work aims to bring some clarity as regards XAI in the context of the AI Act and focuses in particular on the AI Act requirements for transparency and human oversight. After outlining key points of the debate and describing the current limitations of XAI techniques, this paper carries out an interdisciplinary analysis of how the AI Act addresses the issue of opaque AI systems. In particular, we argue that neither does the AI Act mandate a requirement for XAI, which is the subject of intense scientific research and is not without technical limitations, nor does it ban the use of black-box AI systems. Instead, the AI Act aims to achieve its stated policy objectives with the focus on transparency (including documentation) and human oversight. Finally, in order to concretely illustrate our findings and conclusions, a use case on AI-based proctoring is presented.", "keywords": "'explainable artificial intelligence', 'XAI', 'AI Act', 'EU regulation', 'trustworthy AI', 'transparency', 'human oversight'", "ccs_concepts": "'Computing methodologies _ Artificial intelligence', 'Applied computing _ Law'", "author_names": "'Cecilia Panigutti', 'Ronan Hamon', 'Isabelle Hupont', 'David Fernandez Llorca', 'Delia Fano Yela', 'Henrik Junklewitz', 'Salvatore Scalzo', 'Gabriele Mazzini', 'Ignacio Sanchez', 'Josep Soler Garrido', 'Emilia Gomez'", "author_affiliations": "'European Commission, Joint Research Centre (JRC), Ispra', 'European Commission, Joint Research Centre (JRC), Ispra', 'European Commission, Joint Research Centre (JRC), Sevilla', 'European Commission, Joint Research Centre (JRC), Sevilla', 'European Commission, Joint Research Centre (JRC), Sevilla', 'European Commission, Joint Research Centre (JRC), Ispra', 'European Commission', 'European Commission', 'European Commission, Joint Research Centre (JRC), Ispra', 'European Commission, Joint Research Centre (JRC), Sevilla', 'European Commission, Joint Research Centre (JRC), Sevilla'"}, {"link": "https://doi.org/10.1145/3593013.3594067", "title": "Regulating ChatGPT and Other Large Generative AI Models", "abstract": "Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.", "keywords": NaN, "ccs_concepts": "'Social and professional topics_Computing / technology policy_Government / technology policy_Governmental regulations', 'Additional Keywords and Phrases: LGAIMs', 'LGAIM regulation', 'general-purpose AI systems', 'GPAIS', 'foundation models', 'large language models', 'LLMs', 'AI regulation', 'AI Act', 'direct AI regulation', 'data protection', 'GDPR', 'Digital Services Act', 'content moderation'", "author_names": "'Philipp Hacker', 'Andreas Engel', 'Marco Mauer'", "author_affiliations": "'European New School of Digital Studies, European University Viadrina', 'Faculty of Law, Heidelberg University', 'Faculty of Law, Humboldt-University of Berlin, Germany and European New School of Digital Studies, European University Viadrina'"}, {"link": "https://doi.org/10.1145/3593013.3594118", "title": "Fairness Auditing in Urban Decisions Using LP-Based Data Combination", "abstract": "Auditing for fairness often requires relying on a secondary source, e.g., Census data, to inform about protected attributes. To avoid making assumptions about an overarching model that ties such information to the primary data source, a recent line of work has suggested finding the entire range of possible fairness valuations consistent with both sources. Though attractive, the current form of this methodology relies on rigid analytical expressions and lacks the ability to handle continuous decisions, e.g., metrics of urban services. We show that, in such settings, directly adapting these expressions can lead to loose and even vacuous results, particularly on just how fair the audited decisions may be. If used, the audit would be perceived more optimistically than it ought to be. We propose a linear programming formulation to handle continuous decisions, by finding the empirical fairness range when statistical parity is measured through the Kolmogorov-Smirnov distance. The size of this problem is linear in the number of data points and efficiently solvable. We analyze this approach and give finite-sample guarantees to the resulting fairness valuation. We then apply it to synthetic data and to 311 Chicago City Services data, and demonstrate its ability to reveal small but detectable bounds on fairness.", "keywords": "'data combination', 'fairness auditing', 'urban data', 'disparate impact', 'proxy variables', 'linear programming'", "ccs_concepts": "'Theory of computation _ Linear programming', 'Mathematics of computing _ Distribution functions', 'Applied computing _ Multi-criterion optimization and decision-making', 'Theory of computation _ Sample complexity and generalization bounds', 'Social and professional topics _ Technology audits'", "author_names": "'Jingyi Yang', 'Joel Miller', 'Mesrob Ohannessian'", "author_affiliations": "'Electrical and Computer Engineering, University of Illinois Chicago', 'Computer Science, University of Illinois Chicago', 'Electrical and Computer Engineering, University of Illinois Chicago'"}, {"link": "https://doi.org/10.1145/3593013.3594010", "title": "Rethinking Transparency as a Communicative Constellation", "abstract": "In this paper we make the case for an expanded understanding of transparency. Within the now extensive FAccT literature, transparency has largely been understood in terms of explainability. While this approach has proven helpful in many contexts, it falls short of addressing some of the more fundamental issues in the development and application of machine learning, such as the epistemic limitations of predictions and the political nature of the selection of fairness criteria. In order to render machine learning systems more democratic, we argue, a broader understanding of transparency is needed. We therefore propose to view transparency as a communicative constellation that is a precondition for meaningful democratic deliberation. We discuss four perspective expansions implied by this approach and present a case study illustrating the interplay of heterogeneous actors involved in producing this constellation. Drawing from our conceptualization of transparency, we sketch implications for actor groups in different sectors of society.", "keywords": "'transparency', 'explainability', 'science communication', 'deliberation', 'prediction'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Applied computing _ Law', 'social and behavioral sciences', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence'", "author_names": "'Florian Eyert', 'Paola Lopez'", "author_affiliations": "'WZB Berlin Social Science Center, Germany and Weizenbaum Institute', 'University of Vienna, Austria and Weizenbaum Institute'"}, {"link": "https://doi.org/10.1145/3593013.3593991", "title": "Algorithmic Transparency from the South: Examining the State of Algorithmic Transparency in Chile's Public Administration Algorithms", "abstract": "This paper presents the results and conclusions of the study on algorithmic transparency in public Administration and the use of automated decision systems within the State of Chile, carried out by the Public Innovation Laboratory of the Universidad Adolfo Ib\u221a\u00b0\u221a\u00b1ez in alliance with the Chilean Transparency Council. In the first part we delimit the concept of algorithmic transparency, and the different considerations that can derive from this concept. We detail the information gathering procedure carried out on the use of automated decision systems in the public administration and evaluate its status according to a defined transparency framework. It then examines the state of administrative regulation and access to public information in Chile and how algorithmic transparency could be included within the current legal norms in Chile. The results of this study show that there is a use of automated decision systems in critical operations in the Chilean public Administration and that the current legal framework enables the implementation of an algorithmic transparency standard for the public administration, in a flexible, scaled way and with criteria that allow citizens to evaluate their interaction with these systems. Building on the results of this research, in 2022 the Transparency Council piloted a draft algorithmic transparency standard with seven algorithms from four public agencies. A public consultation and the publication of the final standard is expected in 2023.", "keywords": "'Algorithmic Transparency', 'Public Administration', 'Chile'", "ccs_concepts": "'Computers and Society', 'Public Policy issues', 'decision-support', 'public Administration', 'Computing in government'", "author_names": "'Jos\u221a\u00a9 Pablo Lapostol Piderit', 'Romina Garrido Iglesias', 'Mar\u221a\u2260a Paz Hermosilla Cornejo'", "author_affiliations": "'Universidad Adolfo Iba\u221a\u00b1ez', 'Universidad Adolfo Iba\u221a\u00b1ez', 'Universidad Adolfo Iba\u221a\u00b1ez'"}, {"link": "https://doi.org/10.1145/3593013.3594064", "title": "Co-Design Perspectives on Algorithm Transparency Reporting: Guidelines and Prototypes", "abstract": "Recommendation algorithms by and large determine what people see on social media. Users know little about how these algorithms work or what information they use to make their recommendations. But what exactly should platforms share with users about recommendation algorithms that would be meaningful to them? Research has looked into frameworks for explainability of algorithms as well as design features across social media platforms that can contribute to their transparency and accountability. We build on these prior efforts to explore what a recommendation algorithm transparency report may include and how it should present information to users. Through a human-centered co-design research process we result in: (1) A set of guidelines for recommendation algorithm transparency reports; (2) initial suggestions, in the form of prototypes, for more engaging and interactive forms of transparency; (3) an evaluation of these prototypes\u201a\u00c4\u00f4 strengths and weaknesses, and areas of exploration for future work.", "keywords": NaN, "ccs_concepts": "'Human-centered computing _ HCI design and evaluation methods', 'User studies', 'Human-centered computing _ User centered design', 'Human-centered computing _ Participatory design', 'Human-centered computing _ Interface design prototyping'", "author_names": "'Michal Luria'", "author_affiliations": "'Center for Democracy & Technology'"}, {"link": "https://doi.org/10.1145/3593013.3593977", "title": "Welfarist Moral Grounding for Transparent AI", "abstract": "As popular calls for the transparency of AI systems gain prominence, it is important to think systematically about why transparency matters morally. I'll argue that welfarism provides a theoretical basis for doing so. For welfarists, it is morally desirable to make AI systems transparent insofar as pursuing transparency tends to increase overall welfare, and/or maintaining opacity tends to reduce overall welfare. This might seem like a simple \u201a\u00c4\u00ec even simplistic \u201a\u00c4\u00ec move. However, as I will show, the process of tracing the expected effects of transparency on welfare can bring much-needed clarity to existing debates about when AI systems should and should not be transparent. Welfarism provides us with a basis to evaluate conflicting desiderata, and helps us avoid a problematic tendency to reify trust, accountability, and other such goals as ends in themselves. And, by shifting the focus away from the mere act of making an AI system transparent, towards the harms and benefits that its transparency might bring about, welfarists call attention to often- neglected social, legal, and institutional factors that determine whether relevant stakeholders are able to access and meaningfully act on the information made transparent to produce desirable consequences. In these ways, welfarism helps us understand AI transparency not merely as a demand to look at the innards of some technical system, but rather as a broader moral ideal about how we should relate to powerful technologies that make decisions about us.", "keywords": "'Transparency', 'Welfarism', 'Moral Theory', 'AI Ethics'", "ccs_concepts": "'Human-centered computing _ Collaborative and social computing theory', 'concepts and paradigms', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Social and professional topics _ Computing / technology policy'", "author_names": "'Devesh Narayanan'", "author_affiliations": "'National University of Singapore'"}, {"link": "https://doi.org/10.1145/3593013.3594093", "title": "Co-Designing for Transparency: Lessons from Building a Document Organization Tool in the Criminal Justice Domain", "abstract": "Investigative journalists and public defenders conduct the essential work of examining, reporting, and arguing critical cases around police use-of-force and misconduct. In an ideal world, they would have access to well-organized records they can easily navigate and search. In reality, records can come as large, disorganized data dumps, increasing the burden on the already resource-constrained teams. In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented tool that addresses challenges in working with such data dumps. Our Document Organization Tool (DOT) is a Python library that has data cleaning, extraction, and organization features. Our collaborative design process gave us insights into the needs of under-resourced teams who work with large data dumps, such as how some domain experts became self-taught programmers to automate their tasks. To understand what type of programming paradigm could support our target users, we conducted a user study (n=18) comparing visual, programming-by-example, and traditional text-based programming tools. From our user study, we found that once users passed the initial learning stage, they could comfortably use all three paradigms. Our work offers insights for designers working with under-resourced teams who want to consolidate cutting-edge algorithms and AI techniques into unified, expressive tools. We argue user-centered tool design can contribute to the broader fight for accountability and transparency by supporting existing practitioners in their work in domains like criminal justice.", "keywords": "'Co-Design', 'Document Organization', 'User-Centered Design', 'Collaborative Design'", "ccs_concepts": "'Human-centered computing _ Interaction paradigms', 'Human-centered computing _ User studies', 'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Machine learning'", "author_names": "'Hellina Hailu Nigatu', 'Lisa Pickoff-White', 'John Canny', 'Sarah Chasins'", "author_affiliations": "'EECS, UC Berkeley', 'KQED', 'EECS, UC Berkeley', 'EECS, UC Berkeley'"}, {"link": "https://doi.org/10.1145/3593013.3594103", "title": "Arbitrary Decisions Are a Hidden Cost of Differentially Private Training", "abstract": "Mechanisms used in privacy-preserving machine learning often aim to guarantee differential privacy (DP) during model training. Practical DP-ensuring training methods use randomization when fitting model parameters to privacy-sensitive data (e.g., adding Gaussian noise to clipped gradients). We demonstrate that such randomization incurs predictive multiplicity: for a given input example, the output predicted by equally-private models depends on the randomness used in training. Thus, for a given input, the predicted output can vary drastically if a model is re-trained, even if the same training dataset is used. The predictive-multiplicity cost of DP training has not been studied, and is currently neither audited for nor communicated to model designers and stakeholders. We derive a bound on the number of re-trainings required to estimate predictive multiplicity reliably. We analyze\u201a\u00c4\u00eeboth theoretically and through extensive experiments\u201a\u00c4\u00eethe predictive-multiplicity cost of three DP-ensuring algorithms: output perturbation, objective perturbation, and DP-SGD. We demonstrate that the degree of predictive multiplicity rises as the level of privacy increases, and is unevenly distributed across individuals and demographic groups in the data. Because randomness used to ensure DP during training explains predictions for some examples, our results highlight a fundamental challenge to the justifiability of decisions supported by differentially-private models in high-stakes settings. We conclude that practitioners should audit the predictive multiplicity of their DP-ensuring algorithms before deploying them in applications of individual-level consequence.", "keywords": NaN, "ccs_concepts": "'Computing methodologies _ Machine learning', 'Neural networks', 'Computing methodologies _ Model verification and validation', 'Security and privacy _ Privacy protections', 'Security and privacy _ Social aspects of security and privacy'", "author_names": "'Bogdan Kulynych', 'Hsiang Hsu', 'Carmela Troncoso', 'Flavio P. Calmon'", "author_affiliations": "'SPRING Lab, EPFL', 'Harvard University', 'SPRING Lab, EPFL', 'Harvard University'"}, {"link": "https://doi.org/10.1145/3593013.3593986", "title": "Trustworthy AI and the Logics of Intersectional Resistance", "abstract": "Growing awareness of the capacity of AI to inflict harm has inspired efforts to delineate principles for \u201a\u00c4\u00f2trustworthy AI\u201a\u00c4\u00f4 and, from these, objective indicators of \u201a\u00c4\u00f2trustworthiness\u201a\u00c4\u00f4 for auditors and regulators. Such efforts run the risk of formalizing a distinctly privileged perspective on trustworthiness which is insensitive (or else indifferent) to the legitimate reasons for distrust held by marginalized people. By exploring a neglected conative element of trust, we broaden understandings of trust and trustworthiness to make sense of, and identify principles for responding productively to, distrust of ostensibly \u201a\u00c4\u00f2trustworthy\u201a\u00c4\u00f4 AI. Bringing social science scholarship into dialogue with AI criticism, we show that AI is being used to construct a digital underclass that is rhetorically labelled as \u201a\u00c4\u00f2undeserving\u201a\u00c4\u00f4, and highlight how this process fulfills functions for more privileged people and institutions. We argue that distrust of AI is warranted and healthy when the AI contributes to marginalization and structural violence, and that Trustworthy AI may fuel public resistance to the use of AI unless it addresses this dimension of untrustworthiness. To this end, we offer reformulations of core principles of Trustworthy AI\u201a\u00c4\u00eefairness, accountability, and transparency\u201a\u00c4\u00eethat substantively address the deeper issues animating widespread public distrust of AI, including: stewardship and care, openness and vulnerability, and humility and empowerment. In light of legitimate reasons for distrust, we call on the field to to re-evaluate why the public would embrace the expansion of AI into all corners of society; in short, what makes it worthy of their trust.", "keywords": "'Trust', 'distrust', 'artificial intelligence', 'fairness', 'bias', 'inequality', 'intersectionality', 'accountability', 'transparency'", "ccs_concepts": "'Human-centered computing _ HCI theory', 'concepts and models', 'Social and professional topics _ Computing profession'", "author_names": "'Bran Knowles', 'Jasmine Fledderjohann', 'John T. Richards', 'Kush R. Varshney'", "author_affiliations": "'Lancaster University', 'Lancaster University', 'TJ Watson Research Center, IBM', 'TJ Watson Research Center, IBM'"}, {"link": "https://doi.org/10.1145/3593013.3593997", "title": "Saliency Cards: A Framework to Characterize and Compare Saliency Methods", "abstract": "Saliency methods are a common class of machine learning interpretability techniques that calculate how important each input feature is to a model\u201a\u00c4\u00f4s output. We find that, with the rapid pace of development, users struggle to stay informed of the strengths and limitations of new methods and, thus, choose methods for unprincipled reasons (e.g., popularity). Moreover, despite a corresponding rise in evaluation metrics, existing approaches assume universal desiderata for saliency methods (e.g., faithfulness) that do not account for diverse user needs. In response, we introduce saliency cards: structured documentation of how saliency methods operate and their performance across a battery of evaluative metrics. Through a review of 25 saliency method papers and 33 method evaluations, we identify 10 attributes that users should account for when choosing a method. We group these attributes into three categories that span the process of computing and interpreting saliency: methodology, or how the saliency is calculated; sensitivity, or the relationship between the saliency and the underlying model and data; and, perceptibility, or how an end user ultimately interprets the result. By collating this information, saliency cards allow users to more holistically assess and compare the implications of different methods. Through nine semi-structured interviews with users from various backgrounds, including researchers, radiologists, and computational biologists, we find that saliency cards provide a detailed vocabulary for discussing individual methods and allow for a more systematic selection of task-appropriate methods. Moreover, with saliency cards, we are able to analyze the research landscape in a more structured fashion to identify opportunities for new methods and evaluation metrics for unmet user needs.", "keywords": "'saliency cards', 'transparency', 'interpretability', 'documentation', 'saliency'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Software and its engineering _ Documentation', 'General and reference _ Evaluation', 'Software and its engineering _ Software evolution', 'Human-centered computing _ User studies'", "author_names": "'Angie Boggust', 'Harini Suresh', 'Hendrik Strobelt', 'John Guttag', 'Arvind Satyanarayan'", "author_affiliations": "'CSAIL, Massachusetts Institute of Technology', 'CSAIL, Massachusetts Institute of Technology', 'IBM Research, USA and MIT-IBM Watson AI Lab', 'CSAIL, Massachusetts Institute of Technology', 'CSAIL, Massachusetts Institute of Technology'"}, {"link": "https://doi.org/10.1145/3593013.3594054", "title": "On the Impact of Explanations on Understanding of Algorithmic Decision-Making", "abstract": "Ethical principles for algorithms are gaining importance as more and more stakeholders are affected by \"high-risk\" algorithmic decision-making (ADM) systems. Understanding how these systems work enables stakeholders to make informed decisions and to assess the systems\u201a\u00c4\u00f4 adherence to ethical values. Explanations are a promising way to create understanding, but current explainable artificial intelligence (XAI) research does not always consider existent theories on how understanding is formed and evaluated. In this work, we aim to contribute to a better understanding of understanding by conducting a qualitative task-based study with 30 participants, including users and affected stakeholders. We use three explanation modalities (textual, dialogue, and interactive) to explain a \"high-risk\" ADM system to participants and analyse their responses both inductively and deductively, using the \"six facets of understanding\" framework by Wiggins & McTighe [63]. Our findings indicate that the \"six facets\" framework is a promising approach to analyse participants\u201a\u00c4\u00f4 thought processes in understanding, providing categories for both rational and emotional understanding. We further introduce the \"dialogue\" modality as a valid explanation approach to increase participant engagement and interaction with the \"explainer\", allowing for more insight into their understanding in the process. Our analysis further suggests that individuality in understanding affects participants\u201a\u00c4\u00f4 perceptions of algorithmic fairness, demonstrating the interdependence between understanding and ADM assessment that previous studies have outlined. We posit that drawing from theories on learning and understanding like the \"six facets\" and leveraging explanation modalities can guide XAI research to better suit explanations to learning processes of individuals and consequently enable their assessment of ethical values of ADM systems.", "keywords": "'XAI', 'learning Sciences', 'algorithmic decision-making', 'algorithmic fairness', 'qualitative methods'", "ccs_concepts": "'Human-centered computing _ Field studies'", "author_names": "'Timoth\u221a\u00a9e Schmude', 'Laura Koesten', 'Torsten M\u221a\u2202ller', 'Sebastian Tschiatschek'", "author_affiliations": "'Faculty of Computer Science, Research Network Data Science, UniVie Doctoral School Computer Science DoCS, University of Vienna', 'Faculty of Computer Science, Research Group Visualization and Data Analysis, University of Vienna', 'Faculty of Computer Science, Research Network Data Science, Research Group Visualization and Data Analysis, University of Vienna', 'Faculty of Computer Science, Research Network Data Science, Research Group Data Mining and Machine Learning, University of Vienna'"}, {"link": "https://doi.org/10.1145/3593013.3594087", "title": "Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies", "abstract": "AI systems are adopted in numerous domains due to their increasingly strong predictive performance. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time-consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our work highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other\u201a\u00c4\u00f4s work and produce generalizable scientific knowledge. We also hope this work will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Vivian Lai', 'Chacha Chen', 'Alison Smith-Renner', 'Q. Vera Liao', 'Chenhao Tan'", "author_affiliations": "'University of Colorado Boulder', 'University of Chicago', 'Dataminr Inc.', 'Microsoft Research', 'University of Chicago'"}, {"link": "https://doi.org/10.1145/3593013.3594007", "title": "The Possibility of Fairness: Revisiting the Impossibility Theorem in Practice", "abstract": "The \u201a\u00c4\u00faimpossibility theorem\u201a\u00c4\u00f9 \u201a\u00c4\u00ee which is considered foundational in algorithmic fairness literature \u201a\u00c4\u00ee asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a practitioner\u201a\u00c4\u00f4s perspective of fairness), it becomes possible to identify abundant sets of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when \u201a\u00c4\u00ee and to what degree \u201a\u00c4\u00ee fairness along multiple criteria can be achieved. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed.", "keywords": "'machine learning', 'fairness', 'public policy', 'responsible AI'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Social and professional topics _ Computing / technology policy', 'Social and professional topics _ Socio-technical systems'", "author_names": "'Andrew Bell', 'Lucius Bynum', 'Nazarii Drushchak', 'Tetiana Zakharchenko', 'Lucas Rosenblatt', 'Julia Stoyanovich'", "author_affiliations": "'New York University', 'New York University', 'Ukrainian Catholic University', 'Ukrainian Catholic University', 'New York University', 'New York University'"}, {"link": "https://doi.org/10.1145/3593013.3594003", "title": "Simplicity Bias Leads to Amplified Performance Disparities", "abstract": "Which parts of a dataset will a given model find difficult? Recent work has shown that SGD-trained models have a bias towards simplicity, leading them to prioritize learning a majority class, or to rely upon harmful spurious correlations. Here, we show that the preference for \u201a\u00c4\u00f2easy\u201a\u00c4\u00f4 runs far deeper: A model may prioritize any class or group of the dataset that it finds simple\u201a\u00c4\u00eeat the expense of what it finds complex\u201a\u00c4\u00eeas measured by performance difference on the test set. When subsets with different levels of complexity align with demographic groups, we term this difficulty disparity, a phenomenon that occurs even with balanced datasets that lack group/label associations. We show how difficulty disparity is a model-dependent quantity, and is further amplified in commonly-used models as selected by typical average performance scores. We quantify an amplification factor across a range of settings in order to compare disparity of different models on a fixed dataset. Finally, we present two real-world examples of difficulty amplification in action, resulting in worse-than-expected performance disparities between groups even when using a balanced dataset. The existence of such disparities in balanced datasets demonstrates that merely balancing sample sizes of groups is not sufficient to ensure unbiased performance. We hope this work presents a step towards measurable understanding of the role of model bias as it interacts with the structure of data, and call for additional model-dependent mitigation methods to be deployed alongside dataset audits.", "keywords": "'neural networks', 'simplicity bias', 'performance disparities', 'fairness'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Computing methodologies _ Neural networks', 'Computing methodologies _ Computer vision', 'Social and professional topics _ Socio-technical systems', 'General and reference _ Empirical studies', 'General and reference _ Evaluation'", "author_names": "'Samuel James Bell', 'Levent Sagun'", "author_affiliations": "'FAIR, Meta AI', 'FAIR, Meta AI'"}, {"link": "https://doi.org/10.1145/3593013.3594094", "title": "Examining Risks of Racial Biases in NLP Tools for Child Protective Services", "abstract": "Although much literature has established the presence of demographic bias in natural language processing (NLP) models, most work relies on curated bias metrics that may not be reflective of real-world applications. At the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in NLP. In this work, we focus on one such setting: child protective services (CPS). CPS workers often write copious free-form text notes about families they are working with, and CPS agencies are actively seeking to deploy NLP models to leverage these data. Given well-established racial bias in this setting, we investigate possible ways deployed NLP is liable to increase racial disparities. We specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (NER). We document consistent algorithmic unfairness in NER models, possible algorithmic unfairness in coreference resolution models, and little evidence of exacerbated racial bias in risk prediction. While there is existing pronounced criticism of risk prediction, our results expose previously undocumented risks of racial bias in realistic information extraction systems, highlighting potential concerns in deploying them, even though they may appear more benign. Our work serves as a rare realistic examination of NLP algorithmic fairness in a potential deployed setting and a timely investigation of a specific risk associated with deploying NLP in CPS settings.", "keywords": "'NLP', 'bias', 'race', 'child protection system', 'CPS', 'text processing'", "ccs_concepts": NaN, "author_names": "'Anjalie Field', 'Amanda Coston', 'Nupoor Gandhi', 'Alexandra Chouldechova', 'Emily Putnam-Hornstein', 'David Steier', 'Yulia Tsvetkov'", "author_affiliations": "'Computer Science Department, Johns Hopkins University, USA and Language Technologies Institute, Carnegie Mellon University', 'Heinz College of Information Systems and Public Policy and Machine Learning Department, Carnegie Mellon University', 'Heinz College of Information Systems and Public Policy and Machine Learning Department, Carnegie Mellon University', 'Microsoft Research NYC, USA and Heinz College of Information Systems and Public Policy, Carnegie Mellon University', 'School of Social Work, University of North Carolina at Chapel Hill', 'Heinz College of Information Systems and Public Policy, Carnegie Mellon University', 'Paul G. Allen School of Computer Science & Engineering, University of Washington'"}, {"link": "https://doi.org/10.1145/3593013.3593985", "title": "WEIRD FAccTs: How Western, Educated, Industrialized, Rich, and Democratic is FAccT?", "abstract": "Studies conducted on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) samples are considered atypical of the world\u201a\u00c4\u00f4s population and may not accurately represent human behavior. In this study, we aim to quantify the extent to which the ACM FAccT conference, the leading venue in exploring Artificial Intelligence (AI) systems\u201a\u00c4\u00f4 fairness, accountability, and transparency, relies on WEIRD samples. We collected and analyzed 128 papers published between 2018 and 2022, accounting for 30.8% of the overall proceedings published at FAccT in those years (excluding abstracts, tutorials, and papers without human-subject studies or clear country attribution for the participants). We found that 84% of the analyzed papers were exclusively based on participants from Western countries, particularly exclusively from the U.S. (63%). Only researchers who undertook the effort to collect data about local participants through interviews or surveys added diversity to an otherwise U.S.-centric view of science. Therefore, we suggest that researchers collect data from under-represented populations to obtain an inclusive worldview. To achieve this goal, scientific communities should champion data collection from such populations and enforce transparent reporting of data biases.", "keywords": NaN, "ccs_concepts": "'Social and professional topics', 'Computing methodologies', 'Software and its engineering', 'Human-centered computing _ Human computer interaction (HCI)'", "author_names": "'Ali Akbar Septiandri', 'Marios Constantinides', 'Mohammad Tahaei', 'Daniele Quercia'", "author_affiliations": "'Nokia Bell Labs', 'Nokia Bell Labs', 'Nokia Bell Labs', 'Nokia Bell Labs'"}, {"link": "https://doi.org/10.1145/3593013.3594083", "title": "An Empirical Analysis of Racial Categories in the Algorithmic Fairness Literature", "abstract": "Recent work in algorithmic fairness has highlighted the challenge of defining racial categories for the purposes of anti-discrimination. These challenges are not new but have previously fallen to the state, which enacts race through government statistics, policies, and evidentiary standards in anti-discrimination law. Drawing on the history of state race-making, we examine how longstanding questions about the nature of race and discrimination appear within the algorithmic fairness literature. Through a content analysis of 60 papers published at FAccT between 2018 and 2020, we analyze how race is conceptualized and formalized in algorithmic fairness frameworks. We note that differing notions of race are adopted inconsistently, at times even within a single analysis. We also explore the institutional influences and values associated with these choices. While we find that categories used in algorithmic fairness work often echo legal frameworks, we demonstrate that values from academic computer science play an equally important role in the construction of racial categories. Finally, we examine the reasoning behind different operationalizations of race, finding that few papers explicitly describe their choices and even fewer justify them. We argue that the construction of racial categories is a value-laden process with significant social and political consequences for the project of algorithmic fairness. The widespread lack of justification around the operationalization of race reflects institutional norms that allow these political decisions to remain obscured within the backstage of knowledge production.", "keywords": "'racial categories', 'algorithmic fairness', 'state race-making'", "ccs_concepts": NaN, "author_names": "'Amina A. Abdu', 'Irene V. Pasquetto', 'Abigail Z. Jacobs'", "author_affiliations": "'University of Michigan', 'University of Michigan', 'University of Michigan'"}, {"link": "https://doi.org/10.1145/3593013.3594047", "title": "Datafication Genealogies beyond Algorithmic Fairness: Making Up Racialised Subjects", "abstract": "A growing scholarship has discussed how datafication is grounded on algorithmic discrimination. However, these debates only marginally address how racialised classification or race categories are enforced through quantification and neglect its political and historical conceptualisation. In this work, we argue that literature partially fails to show that datafication reinforces racial profiling beyond the creation of racial categories as features. This article casts a new light on datafication by retracing its genealogy focusing on identification procedures in the colony and at the border. Such a genealogy foregrounds how datafication enforces racialised profiles by showing that it is part of a longer historical trajectory of modes of racialising individuals beyond algorithms and racial categories. Building on archival material, it develops this argument through two case studies. First, it focuses on the study of datafication of colonised bodies through biometrics by Francis Galton during the 19th-century. Second, it takes into account police identification procedures about unauthorised migrants, enforced by the French police at the Italian border in the 20th-century. These two cases show that although race categories as variables have been historically used to translate individuals into data, datafication processes as such also produce racialised profiles. A genealogical approach highlights continuities as well as quantitative and qualitative shifts between analogue and digital datafication. The article concludes arguing that datafication mechanisms have historically enforced legal and political measures by states in the name of science and objectivity and debates around algorithmic fairness should bring this key aspect back to the core of their critiques.", "keywords": "'datafication', 'genealogies', 'racialised subjects', 'classification', 'borders'", "ccs_concepts": "'Social and professional topics _ History of computing', 'Applied computing _ Sociology'", "author_names": "'Ana Valdivia', 'Martina Tazzioli'", "author_affiliations": "'Oxford Internet Institute, University of Oxford', 'Politics and International Relations, Goldsmiths, University of London'"}, {"link": "https://doi.org/10.1145/3593013.3594034", "title": "How Redundant Are Redundant Encodings? Blindness in the Wild and Racial Disparity When Race is Unobserved", "abstract": "We address two emerging concerns in algorithmic fairness: (i) redundant encodings of race \u201a\u00c4\u00ec the notion that machine learning models encode race with probability nearing one as the feature set grows \u201a\u00c4\u00ec which is widely noted in theory, with little empirical evidence; and (ii) the lack of race and ethnicity data in many domains, where state-of-the-art remains (Naive) Bayesian Improved Surname Geocoding (BISG) that relies on name and geographic information. We leverage a novel and highly granular dataset of over 7.7 million patients\u201a\u00c4\u00f4 electronic health records to provide one of the first empirical studies of redundant encodings in a realistic health care setting and examine the ability to assess health care disparities when race may be missing. First, we show that machine learning (random forest) applied to name and geographic information can improve on BISG, driven primarily by better performance in identifying minority groups. Second, contrary to theoretical concerns about redundant encodings as undercutting anti-discrimination law\u201a\u00c4\u00f4s anti-classification principle, additional electronic health information provides little marginal information about race and ethnicity: race still remains measured with substantial noise. Third, we show how machine learning can enable the disaggregation of racial categories, responding to longstanding critiques of the government race reporting standard. Fourth, we show that an increasing feature set can differentially impact performance on majority and minority groups. Our findings address important questions for fairness in machine learning and algorithmic decision-making, enabling the assessment of disparities, tempering concerns about redundant encodings in one important setting, and demonstrating how bigger data can shape the accuracy of race imputations in nuanced ways.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Lingwei Cheng', 'Isabel O Gallegos', 'Derek Ouyang', 'Jacob Goldin', 'Dan Ho'", "author_affiliations": "'Carnegie Mellon University', 'Regulation, Evaluation, and Governance Lab, Stanford University', 'Regulation, Evaluation, and Governance Lab, Stanford University', 'University of Chicago', 'Regulation, Evaluation, and Governance Lab, Stanford University'"}, {"link": "https://doi.org/10.1145/3593013.3594005", "title": "Envisioning Equitable Speech Technologies for Black Older Adults", "abstract": "There is increasing concern that how researchers currently define and measure fairness is inadequate. Recent calls push to move beyond traditional concepts of fairness and consider related constructs through qualitative and community-based approaches, particularly for underrepresented communities most at-risk for AI harm. One in context, previous research has identified that voice technologies are unfair due to racial and age disparities. This paper uses voice technologies as a case study to unpack how Black older adults value and envision fair and equitable AI systems. We conducted design workshops and interviews with 16 Black older adults, exploring how participants envisioned voice technologies that better understand cultural context and mitigate cultural dissonance. Our findings identify tensions between what it means to have fair, inclusive, and representative voice technologies. This research raises questions about how and whether researchers can model cultural representation with large language models.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Robin N. Brewer', 'Christina Harrington', 'Courtney Heldreth'", "author_affiliations": "'University of Michigan', 'Carnegie Mellon University', 'Google'"}, {"link": "https://doi.org/10.1145/3593013.3593971", "title": "Broadening AI Ethics Narratives: An Indic Art View", "abstract": "Incorporating interdisciplinary perspectives is seen as an essential step towards enhancing artificial intelligence (AI) ethics. In this regard, the field of arts is perceived to play a key role in elucidating diverse historical and cultural narratives, serving as a bridge across research communities. Most of the works that examine the interplay between the field of arts and AI ethics concern digital artworks, largely exploring the potential of computational tools in being able to surface biases in AI systems. In this paper, we investigate a complementary direction\u201a\u00c4\u00ecthat of uncovering the unique socio-cultural perspectives embedded in human-made art, which in turn, can be valuable in expanding the horizon of AI ethics. Through semi-structured interviews across sixteen artists, art scholars, and researchers of diverse Indian art forms like music, sculpture, painting, floor drawings, dance, etc., we explore how non-Western ethical abstractions, methods of learning, and participatory practices observed in Indian arts, one of the most ancient yet perpetual and influential art traditions, can shed light on aspects related to ethical AI systems. Through a case study concerning the Indian dance system (i.e. the \u201a\u00c4\u00f2Natyashastra\u201a\u00c4\u00f4), we analyze potential pathways towards enhancing ethics in AI systems. Insights from our study outline the need for (1) incorporating empathy in ethical AI algorithms, (2) integrating multimodal data formats for ethical AI system design and development, (3) viewing AI ethics as a dynamic, diverse, cumulative, and shared process rather than as a static, self-contained framework to facilitate adaptability without annihilation of values (4) consistent life-long learning to enhance AI accountability", "keywords": "'Indian arts', 'AI ethics'", "ccs_concepts": "'Social and professional topics', 'Computing methodologies _ Artificial intelligence'", "author_names": "'Ajay Divakaran', 'Aparna Sridhar', 'Ramya Srinivasan'", "author_affiliations": "'SRI International', 'Independent Researcher', 'Fujitsu Research of America'"}, {"link": "https://doi.org/10.1145/3593013.3594024", "title": "Invigorating Ubuntu Ethics in AI for Healthcare: Enabling Equitable Care", "abstract": "The use of artificial intelligence (AI) in healthcare has the potential to improve patient outcomes and increase efficiency in the delivery of care. However, the design, deployment and use of AI in healthcare must be guided by a set of ethical principles that prioritize the well-being of patients and the community, and ensure that care is delivered equitably. A growing body of literature on algorithmic injustice illustrates that AI systems in healthcare have the potential to cause social harm, especially to vulnerable communities. Existing ethical principles in healthcare are based on, and mostly influenced by, Western epistemology, which emphasizes individual rights, often at the expense of collective well-being. The African philosophy of Ubuntu, which emphasises the interconnectedness and interdependence of all people, is an attractive framework for addressing ethical concerns in AI for healthcare because healthcare is intrinsically a community-wide issue. This paper discusses the relevance of Ubuntu ethics in the context of AI for healthcare and proposes principles to reinvigorate the spirit of \u201a\u00c4\u00faI am because we are\u201a\u00c4\u00f9 in design, deployment and use. These principles are fairness, community good, safeguarding humanity, respect for others and trust, and we believe their application will support co-designing, deploying and using inclusive AI systems that will enable clinicians to deliver equitable care for all. Highlighting the relational aspect of Ubuntu, this paper not only calls for rethinking AI ethics in healthcare but also endorses discussions about the need for non-Western ethical approaches to be utilised in AI ethics more broadly.", "keywords": "'algorithmic fairness', 'algorithmic trust', 'equitable care', 'relational ethics'", "ccs_concepts": "'Relational theory _ Ubuntu ethics', 'Relational theory~Artificial Intelligence', 'Artificial intelligence~Healthcare'", "author_names": "'Lameck Mbangula Amugongo', 'Nicola J. Bidwell', 'Caitlin C. Corrigan'", "author_affiliations": "'Institute of Ethics in Artificial Intelligence (IEAI), Technical University of Munich, School of Social Sciences and Technology, Germany and Software Engineering, Namibia University of Science & Technology', 'International University of Management, Namibia, Namibia and University of Melbourne', 'Institute of Ethics in Artificial Intelligence (IEAI), Technical University of Munich, School of Social Sciences and Technology'"}, {"link": "https://doi.org/10.1145/3593013.3594096", "title": "What's Fair Is\u201a\u00c4\u00b6 Fair? Presenting JustEFAB, an Ethical Framework for Operationalizing Medical Ethics and Social Justice in the Integration of Clinical Machine Learning: JustEFAB", "abstract": "The problem of algorithmic bias represents an ethical threat to the fair treatment of patients when their care involves machine learning (ML) models informing clinical decision-making. The design, development, testing, and integration of ML models therefore require a lifecycle approach to bias identification and mitigation efforts. Presently, most work focuses on the ML tool alone, neglecting the larger sociotechnical context in which these models operate. Moreover, the narrow focus on technical definitions of fairness must be integrated within the larger context of medical ethics in order to facilitate equitable care with ML. Drawing from principles of medical ethics, research ethics, feminist philosophy of science, and justice-based theories, we describe the Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness. This paper describes JustEFAB's development and vetting through multiple advisory groups and the lifecycle approach to addressing fairness in clinical ML tools. We present an ethical decision-making framework to support design and development, adjudication between ethical values as design choices, silent trial evaluation, and prospective clinical evaluation guided by medical ethics and social justice principles. We provide some preliminary considerations for oversight and safety to support ongoing attention to fairness issues. We envision this guideline as useful to many stakeholders, including ML developers, healthcare decision-makers, research ethics committees, regulators, and other parties who have interest in the fair and judicious use of clinical ML tools.", "keywords": "'algorithmic bias', 'fairness', 'clinical machine learning', 'ethics', 'organizational ethics', 'justice', 'accountability', 'healthcare', 'health policy', 'safe deployment'", "ccs_concepts": "'Social and professional topics', 'Computing/technology policy', 'Medical information policy', 'Medical technologies'", "author_names": "'Melissa Mccradden', 'Oluwadara Odusi', 'Shalmali Joshi', 'Ismail Akrout', 'Kagiso Ndlovu', 'Ben Glocker', 'Gabriel Maicas', 'Xiaoxuan Liu', 'Mjaye Mazwi', 'Tee Garnett', 'Lauren Oakden-Rayner', 'Myrtede Alfred', 'Irvine Sihlahla', 'Oswa Shafei', 'Anna Goldenberg'", "author_affiliations": "'The Hospital for Sick Children', 'The University of Sheffield Medical School', 'Columbia University', 'The Hospital for Sick Children', 'University of Botswana', 'Imperial College London', 'Australian Institute for Machine Learning', 'University of Birmingham', 'The Hospital for Sick Children', 'The Hospital for Sick Children', 'Australian Institute for Machine Learning', 'University of Toronto', 'University of Cape Town', 'The Hospital for Sick Children', 'The Hospital for Sick Children'"}, {"link": "https://doi.org/10.1145/3593013.3594102", "title": "Improving Fairness in AI Models on Electronic Health Records: The Case for Federated Learning Methods", "abstract": "Developing AI tools that preserve fairness is of critical importance, specifically in high-stakes applications such as those in healthcare. However, health AI models\u201a\u00c4\u00f4 overall prediction performance is often prioritized over the possible biases such models could have. In this study, we show one possible approach to mitigate bias concerns by having healthcare institutions collaborate through a federated learning paradigm (FL; which is a popular choice in healthcare settings). While FL methods with an emphasis on fairness have been previously proposed, their underlying model and local implementation techniques, as well as their possible applications to the healthcare domain remain widely underinvestigated. Therefore, we propose a comprehensive FL approach with adversarial debiasing and a fair aggregation method, suitable to various fairness metrics, in the healthcare domain where electronic health records are used. Not only our approach explicitly mitigates bias as part of the optimization process, but an FL-based paradigm would also implicitly help with addressing data imbalance and increasing the data size, offering a practical solution for healthcare applications. We empirically demonstrate our method\u201a\u00c4\u00f4s superior performance on multiple experiments simulating large-scale real-world scenarios and compare it to several baselines. Our method has achieved promising fairness performance with the lowest impact on overall discrimination performance (accuracy). Our code is available at https://github.com/healthylaife/FairFedAvg.", "keywords": "'Federated Learning', 'Algorithmic Fairness', 'Adversarial Fairness'", "ccs_concepts": NaN, "author_names": "'Raphael Poulain', 'Mirza Farhan Bin Tarek', 'Rahmatollah Beheshti'", "author_affiliations": "'University of Delaware', 'University of Delaware', 'University of Delaware'"}, {"link": "https://doi.org/10.1145/3593013.3593976", "title": "Optimization\u201a\u00c4\u00f4s Neglected Normative Commitments", "abstract": "Optimization is offered as an objective approach to resolving complex, real-world decisions involving uncertainty and conflicting interests. It drives business strategies as well as public policies and, increasingly, lies at the heart of sophisticated machine learning systems. A paradigm used to approach potentially high-stakes decisions, optimization relies on abstracting the real world to a set of decision(s), objective(s) and constraint(s). Drawing from the modeling process and a range of actual cases, this paper describes the normative choices and assumptions that are necessarily part of using optimization. It then identifies six emergent problems that may be neglected: 1) Misspecified values can yield optimizations that omit certain imperatives altogether or incorporate them incorrectly as a constraint or as part of the objective, 2) Problematic decision boundaries can lead to faulty modularity assumptions and feedback loops, 3) Failing to account for multiple agents\u201a\u00c4\u00f4 divergent goals and decisions can lead to policies that serve only certain narrow interests, 4) Mislabeling and mismeasurement can introduce bias and imprecision, 5) Faulty use of relaxation and approximation methods, unaccompanied by formal characterizations and guarantees, can severely impede applicability, and 6) Treating optimization as a justification for action, without specifying the necessary contextual information, can lead to ethically dubious or faulty decisions. Suggestions are given to further understand and curb the harms that can arise when optimization is used wrongfully.", "keywords": "'Optimization', 'ethics', 'modeling assumptions', 'values'", "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Applied computing _ Law', 'social and behavioral sciences', 'Social and professional topics _ Codes of ethics', 'Social and professional topics~Socio-technical systems', 'Theory of computation~Mathematical optimization'", "author_names": "'Benjamin Laufer', 'Thomas Gilbert', 'Helen Nissenbaum'", "author_affiliations": "'Cornell Tech', 'Cornell Tech', 'Cornell Tech'"}, {"link": "https://doi.org/10.1145/3593013.3594035", "title": "On the Site of Predictive Justice", "abstract": "Optimism about our ability to enhance societal decision-making by leaning on Machine Learning (ML) for cheap, accurate predictions has palled in recent years, as these \u201a\u00c4\u00f2cheap\u201a\u00c4\u00f4 predictions have come at significant social cost, contributing to systematic harms suffered by already disadvantaged populations. But what precisely goes wrong when ML goes wrong? We argue that, as well as more obvious concerns about the downstream effects of ML-based decision-making, there can be moral grounds for the criticism of these predictions themselves. We introduce and defend a theory of predictive justice, according to which differential model performance for systematically disadvantaged groups can be grounds for moral criticism of the model, independently of its downstream effects. As well as helping resolve some urgent disputes around algorithmic fairness, this theory points the way to a novel dimension of epistemic ethics, related to the recently discussed category of doxastic wrong. The full version of this paper is available at http://mintresearch.org/pj.", "keywords": "'Philosophy', 'Algorithmic fairness', 'Predictive justice', 'Justice', 'Epistemic ethics'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Applied computing _ Law', 'social and behavioral sciences'", "author_names": "'Seth Lazar', 'Jake Stone'", "author_affiliations": "'Australian National University', 'Australian National University'"}, {"link": "https://doi.org/10.1145/3593013.3594105", "title": "Reducing Access Disparities in Networks Using Edge Augmentation", "abstract": "In social networks, a node\u201a\u00c4\u00f4s position is, in and of itself, a form of social capital. Better-positioned members not only benefit from (faster) access to diverse information, but innately have more potential influence on information spread. Structural biases often arise from network formation, and can lead to significant disparities in information access based on position. Further, processes such as link recommendation can exacerbate this inequality by relying on network structure to augment connectivity.  In this paper, we argue that one can understand and quantify this social capital through the lens of information flow in the network. In contrast to prior work, we consider the setting where all nodes may be sources of distinct information, and a node\u201a\u00c4\u00f4s (dis)advantage takes into account its ability to access all information available on the network, not just that from a single source. We introduce three new measures of advantage (broadcast, influence, and control), which are quantified in terms of position in the network using access signatures \u201a\u00c4\u00ec vectors that represent a node\u201a\u00c4\u00f4s ability to share information with each other node in the network. We then consider the problem of improving equity by making interventions to increase the access of the least-advantaged nodes. Since all nodes are already sources of information in our model, we argue that edge augmentation is most appropriate for mitigating bias in the network structure, and frame a budgeted intervention problem for maximizing broadcast (minimum pairwise access) over the network.  Finally, we propose heuristic strategies for selecting edge augmentations and empirically evaluate their performance on a corpus of real-world social networks. We demonstrate that a small number of interventions can not only significantly increase the broadcast measure of access for the least-advantaged nodes (over 5 times more than random), but also simultaneously improve the minimum influence. Additional analysis shows that edge augmentations targeted at improving minimum pairwise access can also dramatically shrink the gap in advantage between nodes (over ) and reduce disparities between their access signatures.", "keywords": "'algorithmic fairness; information access; social networks; edge interventions'", "ccs_concepts": "'Theory of computation _ Graph algorithms analysis', 'Information systems _ Social networks', 'Information systems~Social recommendation'", "author_names": "'Ashkan Bashardoust', 'Sorelle Friedler', 'Carlos Scheidegger', 'Blair D. Sullivan', 'Suresh Venkatasubramanian'", "author_affiliations": "'University of Utah', 'Haverford College', 'Posit PBC', 'University of Utah', 'Brown University'"}, {"link": "https://doi.org/10.1145/3593013.3594091", "title": "Group Fairness without Demographics Using Social Networks", "abstract": "Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on access to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, and unforeseen biases. In this work, we propose a \u201a\u00c4\u00fagroup-free\" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering any form of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. We theoretically justify our measure by showing it is commensurate with the notion of additive decomposability in the economic inequality literature and also bound the impact of non-sensitive confounding attributes. Furthermore, we apply our measure to develop fair algorithms for classification, maximizing information access, and recommender systems. Our experimental results show that the proposed approach can reduce inequality among protected classes without knowledge of sensitive attribute labels. We conclude with a discussion of the limitations of our approach when applied in real-world settings.", "keywords": "'group fairness', 'social networks', 'homophily'", "ccs_concepts": "'Computing methodologies _ Artificial intelligence', 'Theory of computation _ Social networks'", "author_names": "'David Liu', 'Virginie Do', 'Nicolas Usunier', 'Maximilian Nickel'", "author_affiliations": "'Northeastern University', 'FAIR, Meta AI, France and LAMSADE, PSL, Universit\u221a\u00a9 Paris Dauphine', 'FAIR, Meta AI', 'FAIR, Meta AI'"}, {"link": "https://doi.org/10.1145/3593013.3594021", "title": "Delayed and Indirect Impacts of Link Recommendations", "abstract": "The impacts of link recommendations on social networks are challenging to evaluate, due to feedback loops between algorithmic recommendations and underlying network dynamics. Observational studies have limitations in answering causal questions; naive A/B experiments often result in biased evaluations due to unaccounted network interference and finally, existing simulations primarily employ static network models that do not take into account dynamics. Departing from existing approaches, we employ simulations to study dynamic impacts of link recommendations. Specifically, we propose an extension to the Jackson-Rogers network evolution model and investigate how link recommendations affect network evolution over time. Our experiments demonstrate that link recommendations can have surprising delayed and indirect effects on the structural properties of networks. Effects of recommendations vary in the short-term and long-term, such as the immediate reduction in degree inequality but eventual increase in degree inequality through friend-of-friend recommendations. Furthermore, even after recommendations are discontinued, their impacts can persist in the network, in part by altering natural network evolution dynamics. These results provide valuable insights into the interplay between algorithmic interventions and natural network dynamics and highlight the limitations of current evaluation paradigms.", "keywords": "'recommender systems', 'link recommendation', 'dynamic graphs'", "ccs_concepts": "'Computing methodologies _ Network science', 'Theory of computation _ Random network models', 'Mathematics of computing _ Graph algorithms'", "author_names": "'Han Zhang', 'Shangen Lu', 'Yixin Wang', 'Mihaela Curmei'", "author_affiliations": "'University of California, Berkeley', 'Wharton School University of Pennsylvania', 'University of Michigan', 'University of California, Berkeley'"}, {"link": "https://doi.org/10.1145/3593013.3594115", "title": "Discrimination through Image Selection by Job Advertisers on Facebook", "abstract": "Targeted advertising platforms are widely used by job advertisers to reach potential employees; thus issues of discrimination due to targeting that have surfaced have received widespread attention. Advertisers could misuse targeting tools to exclude people based on gender, race, location and other protected attributes from seeing their job ads. In response to legal actions, Facebook disabled the ability for explicit targeting based on many attributes for some ad categories, including employment. Although this is a step in the right direction, prior work has shown that discrimination can take place not just due to the explicit targeting tools of the platforms, but also due to the impact of the biased ad delivery algorithm. Thus, one must look at the potential for discrimination more broadly, and not merely through the lens of the explicit targeting tools.  In this work, we propose and investigate the prevalence of a new means for discrimination in job advertising, that combines both targeting and delivery \u201a\u00c4\u00ec through the disproportionate representation or exclusion of people of certain demographics in job ad images. We use the Facebook Ad Library to demonstrate the prevalence of this practice through: (1) evidence of advertisers running many campaigns using ad images of people of only one perceived gender, (2) systematic analysis for gender representation in all current ad campaigns for truck drivers and nurses, (3) longitudinal analysis of ad campaign image use by gender and race for select advertisers. After establishing that the discrimination resulting from a selective choice of people in job ad images, combined with algorithmic amplification of skews by the ad delivery algorithm, is of immediate concern, we discuss approaches and challenges for addressing it.", "keywords": NaN, "ccs_concepts": "'Social and professional topics _ Technology audits', 'Social and professional topics _ Employment issues', 'Human-centered computing _ Social media', 'Social and professional topics _ Socio-technical systems', 'Social and professional topics _ Race and ethnicity', 'Social and professional topics _ Gender', 'Information systems _ Online advertising', 'Information systems _ Display advertising'", "author_names": "'Varun Nagaraj Rao', 'Aleksandra Korolova'", "author_affiliations": "'Princeton University', 'Princeton University'"}, {"link": "https://doi.org/10.1145/3593013.3594080", "title": "Diverse Perspectives Can Mitigate Political Bias in Crowdsourced Content Moderation", "abstract": "In recent years, social media companies have grappled with defining and enforcing content moderation policies surrounding political content on their platforms, due in part to concerns about political bias, disinformation, and polarization. These policies have taken many forms, including disallowing political advertising, limiting the reach of political topics, fact-checking political claims, and enabling users to hide political content altogether. However, implementing these policies requires human judgement to label political content, and it is unclear how well human labelers perform at this task, or whether biases affect this process. Therefore, in this study we experimentally evaluate the feasibility and practicality of using crowd workers to identify political content, and we uncover biases that make it difficult to identify this content. Our results problematize crowds composed of seemingly interchangeable workers, and provide preliminary evidence that aggregating judgements from heterogeneous workers may help mitigate political biases. In light of these findings, we identify strategies to achieving fairer labeling outcomes, while also better supporting crowd workers at this task and potentially mitigating biases.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Jacob Thebault-Spieker', 'Sukrit Venkatagiri', 'Naomi Mine', 'Kurt Luther'", "author_affiliations": "'Information School, University of Wisconsin - Madison', 'Center for an Informed Public, University of Washington', 'University of Wisconsin - Madison', 'Virginia Tech'"}, {"link": "https://doi.org/10.1145/3593013.3593984", "title": "Preventing Discriminatory Decision-Making in Evolving Data Streams", "abstract": "Bias in machine learning has rightly received significant attention over the past decade. However, most fair machine learning (fair-ML) works to address bias in decision-making systems has focused solely on the offline setting. Despite the wide prevalence of online systems in the real world, work on identifying and correcting bias in the online setting is severely lacking. The unique challenges of the online environment make addressing bias more difficult than in the offline setting. First, Streaming Machine Learning (SML) algorithms must deal with the constantly evolving real-time data stream. Secondly, they need to adapt to changing data distributions (concept drift) to make accurate predictions on new incoming data. Incorporating fairness constraints into this already intricate task is not straightforward. In this work, we focus on the challenges of achieving fairness in biased data streams while accounting for the presence of concept drift, accessing one sample at a time. We present Fair Sampling over Stream (FS2), a novel fair rebalancing approach capable of being integrated with SML classification algorithms. Furthermore, we devise the first unified performance-fairness metric, Fairness Bonded Utility (FBU), to efficiently evaluate and compare the trade-offs between performance and fairness across various bias mitigation methods. FBU simplifies the comparison of fairness-performance trade-offs of multiple techniques through one unified and intuitive evaluation, allowing model designers to easily choose a technique. Overall, extensive evaluations show our measures surpass those of other fair online techniques previously reported in the literature.", "keywords": "'Fairness', 'Data Stream', 'Concept Drift', 'Fairness Drift'", "ccs_concepts": NaN, "author_names": "'Zichong Wang', 'Nripsuta Saxena', 'Tongjia Yu', 'Sneha Karki', 'Tyler Zetty', 'Israat Haque', 'Shan Zhou', 'Dukka Kc', 'Ian Stockwell', 'Xuyu Wang', 'Albert Bifet', 'Wenbin Zhang'", "author_affiliations": "'Michigan Technological University', 'University of Southern California', 'Columbia University', 'Michigan Technological University', 'Michigan Technological University', 'Dalhousie University', 'Michigan Technological University', 'Michigan Technological University', 'University of Maryland, Baltimore County', 'Florida International University', 'University of Waikato', 'Michigan Technological University'"}, {"link": "https://doi.org/10.1145/3593013.3594006", "title": "Group-Fair Classification with Strategic Agents", "abstract": "The use of algorithmic decision making systems in domains which impact the financial, social, and political well-being of people has created a demand for these to be \u201a\u00c4\u00fafair\u201a\u00c4\u00f9 under some accepted notion of equity. This demand has in turn inspired a large body of work focused on the development of fair learning algorithms which are then used in lieu of their conventional counterparts. Most analysis of such fair algorithms proceeds from the assumption that the people affected by the algorithmic decisions are represented as immutable feature vectors. However, strategic agents may possess both the ability and the incentive to manipulate this observed feature vector in order to attain a more favorable outcome. We explore the impact that strategic agent behavior can have on group-fair classification. We find that in many settings strategic behavior can lead to fairness reversal, with a conventional classifier exhibiting higher fairness than a classifier trained to satisfy group fairness. Further, we show that fairness reversal occurs as a result of a group-fair classifier becoming more selective, achieving fairness largely by excluding individuals from the advantaged group. In contrast, if group fairness is achieved by the classifier becoming more inclusive, fairness reversal does not occur.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Andrew Estornell', 'Sanmay Das', 'Yang Liu', 'Yevgeniy Vorobeychik'", "author_affiliations": "'Washington University in St Louis', 'George Mason University', 'University of California Santa Cruz', 'Washington University in Saint Louis'"}, {"link": "https://doi.org/10.1145/3593013.3594039", "title": "Add-Remove-or-Relabel: Practitioner-Friendly Bias Mitigation via Influential Fairness", "abstract": "Commensurate with the rise in algorithmic bias research, myriad algorithmic bias mitigation strategies have been proposed in the literature. Nonetheless, many voice concerns about the lack of transparency that accompanies mitigation methods and the paucity of mitigation methods that satisfy protocol and data limitations of practitioners. Influence functions from robust statistics provide a novel opportunity to overcome both issues. Previous work demonstrates the power of influence functions to improve fairness outcomes. This work proposes a novel family of fairness solutions, coined influential fairness (IF), that is human-understandable and also agnostic to the underlying machine learning model and choice of fairness metric. We conduct an investigation of practitioner profiles and design mitigation methods for practitioners whose limitations discourage them from utilizing existing bias mitigation methods.", "keywords": "'machine learning', 'fairness', 'ethics', 'bias mitigation'", "ccs_concepts": NaN, "author_names": "'Brianna Richardson', 'Prasanna Sattigeri', 'Dennis Wei', 'Karthikeyan Natesan Ramamurthy', 'Kush Varshney', 'Amit Dhurandhar', 'Juan E. Gilbert'", "author_affiliations": "'University of Florida', 'IBM', 'IBM', 'IBM', 'IBM', 'IBM', 'University of Florida'"}, {"link": "https://doi.org/10.1145/3593013.3594085", "title": "Fairer Together: Mitigating Disparate Exposure in Kemeny Rank Aggregation", "abstract": "In social choice, traditional Kemeny rank aggregation combines the preferences of voters, expressed as rankings, into a single consensus ranking without consideration for how this ranking may unfairly affect marginalized groups (i.e., racial or gender). Developing fair rank aggregation methods is critical due to their societal influence in applications prioritizing job applicants, funding proposals, and scheduling medical patients. In this work, we introduce the Fair Exposure Kemeny Aggregation Problem (FairExp-kap) for combining vast and diverse voter preferences into a single ranking that is not only a suitable consensus, but ensures opportunities are not withheld from marginalized groups. In formalizing FairExp-kap, we extend the fairness of exposure notion from information retrieval to the rank aggregation context and present a complimentary metric for voter preference representation. We design algorithms for solving FairExp-kap that explicitly account for position bias, a common ranking-based concern that end-users pay more attention to higher ranked candidates. epik solves FairExp-kap exactly by incorporating non-pairwise fairness of exposure into the pairwise Kemeny optimization; while the approximate epira is a candidate swapping algorithm, that guarantees ranked candidate fairness. Utilizing comprehensive synthetic simulations and six real-world datasets, we show the efficacy of our approach illustrating that we succeed in mitigating disparate group exposure unfairness in consensus rankings, while maximally representing voter preferences.", "keywords": "'fair exposure rank aggregation', 'group fairness', 'rank aggregation', 'voting rules'", "ccs_concepts": "'Social and professional topics _ User characteristics', 'Computing methodologies'", "author_names": "'Kathleen Cachel', 'Elke Rundensteiner'", "author_affiliations": "'Worcester Polytechnic Institute', 'Worcester Polytechnic Institute'"}, {"link": "https://doi.org/10.1145/3593013.3594008", "title": "Domain Adaptive Decision Trees: Implications for Accuracy and Fairness", "abstract": "In uses of pre-trained machine learning models, it is a known issue that the target population in which the model is being deployed may not have been reflected in the source population with which the model was trained. This can result in a biased model when deployed, leading to a reduction in model performance. One risk is that, as the population changes, certain demographic groups will be under-served or otherwise disadvantaged by the model, even as they become more represented in the target population. The field of domain adaptation proposes techniques for a situation where label data for the target population does not exist, but some information about the target distribution does exist. In this paper we contribute to the domain adaptation literature by introducing domain-adaptive decision trees (DADT). We focus on decision trees given their growing popularity due to their interpretability and performance relative to other more complex models. With DADT we aim to improve the accuracy of models trained in a source domain (or training data) that differs from the target domain (or test data). We propose an in-processing step that adjusts the information gain split criterion with outside information corresponding to the distribution of the target population. We demonstrate DADT on real data and find that it improves accuracy over a standard decision tree when testing in a shifted target population. We also study the change in fairness under demographic parity and equal opportunity. Results show an improvement in fairness with the use of DADT.", "keywords": "'Decision Trees', 'Information Gain', 'Domain Adaptation', 'Covariate Shift', 'Fairness', 'folktables'", "ccs_concepts": "'Computing methodologies _ Classification and regression trees', 'Computing methodologies _ Learning under covariate shift', 'Computing methodologies _ Transfer learning'", "author_names": "'Jose M. Alvarez', 'Kristen M. Scott', 'Bettina Berendt', 'Salvatore Ruggieri'", "author_affiliations": "'Scuola Normale Superiore, University of Pisa', 'KU Leuven', 'TU Berlin, Weizenbaum Institute, Germany and KU Leuven', 'University of Pisa'"}, {"link": "https://doi.org/10.1145/3593013.3594063", "title": "Ethical Considerations in the Early Detection of Alzheimer's Disease Using Speech and AI", "abstract": "While recent studies indicate that AI could play an important role in detecting early signs of Alzheimer's disease in speech, this use of data from individuals with cognitive decline raises numerous ethical concerns. In this paper, we identify and explain concerns related to autonomy (including consent, depersonalization and disclosure), privacy and data protection (including the handling of personal content and medical information), welfare (including distress, discrimination and reliability), transparency (including the interpretability of language features and AI-based decision-making for developers and clinicians), and fairness (including bias and the distribution of benefits). Our aim is to not only raise awareness of the ethical concerns posed by the use of AI in speech-based Alzheimer's detection, but also identify ways in which these concerns might be addressed. To this end, we conclude with a list of suggestions that could be incorporated into ethical guidelines for researchers and clinicians working in this area.", "keywords": "'ethics', '\"Alzheimers disease\"', 'speech', 'language', 'digital biomarkers', 'autonomy', 'privacy', 'welfare', 'transparency', 'fairness'", "ccs_concepts": "'Applied computing _ Health informatics', 'Computing methodologies _ Natural language processing', 'Social and professional topics _ Medical technologies'", "author_names": "'Ulla Petti', 'Rune Nyrup', 'Jeffrey M. Skopek', 'Anna Korhonen'", "author_affiliations": "'University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge'"}, {"link": "https://doi.org/10.1145/3593013.3594014", "title": "Algorithms as Social-Ecological-Technological Systems: An Environmental Justice Lens on Algorithmic Audits", "abstract": "This paper reframes algorithmic systems as intimately connected to and part of social and ecological systems, and proposes a first-of-its-kind methodology for environmental justice-oriented algorithmic audits. How do we consider environmental and climate justice dimensions of the way algorithmic systems are designed, developed, and deployed? These impacts are inherently emergent and can only be understood and addressed at the level of relations between an algorithmic system and the social (including institutional) and ecological components of the broader ecosystem it operates in. As a result, we claim that in absence of an integral ontology for algorithmic systems, we cannot do justice to the emergent nature of broader environmental impacts of algorithmic systems and their underlying computational infrastructure. Furthermore, an integral lens provides many lessons from the history of environmental justice that are of relevance in current day struggles for algorithmic justice. We propose to define algorithmic systems as ontologically indistinct from Social-Ecological-Technological Systems (SETS), framing emergent implications as couplings between social, ecological, and technical components of the broader fabric in which algorithms are integrated and operate. We draw upon prior work on SETS analysis as well as emerging themes in the literature and practices of Environmental Justice (EJ) to conceptualize and assess algorithmic impact. We then offer three policy recommendations to help establish a SETS-based EJ approach to algorithmic audits: (1) broaden the inputs and open-up the outputs of an audit, (2) enable meaningful access to redress, and (3) guarantee a place-based and relational approach to the process of evaluating impact. We operationalize these as a qualitative framework of questions for a spectrum of stakeholders. Doing so, this article aims to inspire stronger and more frequent interactions across policymakers, researchers, practitioners, civil society, and grassroots communities. https://arxiv.org/abs/2305.05733.", "keywords": NaN, "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Social and professional topics _ Technology audits', 'Software and its engineering _ Software development process management'", "author_names": "'Bogdana Rakova', 'Roel Dobbe'", "author_affiliations": "'Mozilla Foundation', 'Delft University of Technology'"}, {"link": "https://doi.org/10.1145/3593013.3594107", "title": "Cross-Institutional Transfer Learning for Educational Models: Implications for Model Performance, Fairness, and Equity", "abstract": "Modern machine learning increasingly supports paradigms that are multi-institutional (using data from multiple institutions during training) or cross-institutional (using models from multiple institutions for inference), but the empirical effects of these paradigms are not well understood. This study investigates cross-institutional learning via an empirical case study in higher education. We propose a framework and metrics for assessing the utility and fairness of student dropout prediction models that are transferred across institutions. We examine the feasibility of cross-institutional transfer under real-world data- and model-sharing constraints, quantifying model biases for intersectional student identities, characterizing potential disparate impact due to these biases, and investigating the impact of various cross-institutional ensembling approaches on fairness and overall model performance. We perform this analysis on data representing over 200,000 enrolled students annually from four universities without sharing training data between institutions.  We find that a simple zero-shot cross-institutional transfer procedure can achieve similar performance to locally-trained models for all institutions in our study, without sacrificing model fairness. We also find that stacked ensembling provides no additional benefits to overall performance or fairness compared to either a local model or the zero-shot transfer procedure we tested. We find no evidence of a fairness-accuracy tradeoff across dozens of models and transfer schemes evaluated. Our auditing procedure also highlights the importance of intersectional fairness analysis, revealing performance disparities at the intersection of sensitive identity groups that are concealed under one-dimensional analysis.1", "keywords": "'Algorithmic Fairness', 'Education', 'Dropout Prediction', 'Transfer Learning', 'Intersectionality'", "ccs_concepts": "'Applied computing _ Law', 'social and behavioral sciences', 'Applied computing _ Education', 'Computing methodologies _ Machine learning'", "author_names": "'Joshua Gardner', 'Renzhe Yu', 'Quan Nguyen', 'Christopher Brooks', 'Rene Kizilcec'", "author_affiliations": "'University of Washington', 'Teachers College, Columbia University', 'University of British Columbia', 'University of Michigan', 'Cornell University'"}, {"link": "https://doi.org/10.1145/3593013.3594109", "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks", "abstract": "Warning: The content of this paper may be upsetting or triggering. The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence \"They are people who have less than a high school education.\" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson\u201a\u00c4\u00f4s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.", "keywords": "'AI ethics', 'AI bias', 'stigma in language models', 'language models', 'representation learning', 'sentiment classification', 'prompting'", "ccs_concepts": "'Computing methodologies _ Natural language processing'", "author_names": "'Katelyn Mei', 'Sonia Fereidooni', 'Aylin Caliskan'", "author_affiliations": "'Information School, University of Washington', 'University of Washington', 'Information School, University of Washington'"}, {"link": "https://doi.org/10.1145/3593013.3594072", "title": "Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias", "abstract": "Warning: The content of this paper may be upsetting or triggering. Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person\u201a\u00c4\u00f4s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objectification and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model\u201a\u00c4\u00f4s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d > 0.80) and sadness (d > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of \"a [age] year old girl\" generates sexualized images (as determined by an NSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17), and up to 42% of the time for Stable Diffusion (ages 14 and 18); the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications.", "keywords": "'language-vision AI', 'generative AI', 'text-to-image generators', 'representation learning', 'AI bias', 'gender bias', 'sexualization', 'AI bias propagation', 'AI bias in applications'", "ccs_concepts": "'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Learning paradigms', 'Computing methodologies _ Learning latent representations'", "author_names": "'Robert Wolfe', 'Yiwei Yang', 'Bill Howe', 'Aylin Caliskan'", "author_affiliations": "'Information School, University of Washington', 'Information School, University of Washington', 'Information School, University of Washington', 'Information School, University of Washington'"}, {"link": "https://doi.org/10.1145/3593013.3593989", "title": "\u201a\u00c4\u00faI Wouldn\u201a\u00c4\u00f4t Say Offensive but...\u201a\u00c4\u00fa: Disability-Centered Perspectives on Large Language Models", "abstract": "Large language models (LLMs) trained on real-world data can inadvertently reflect harmful societal biases, particularly toward historically marginalized communities. While previous work has primarily focused on harms related to age and race, emerging research has shown that biases toward disabled communities exist. This study extends prior work exploring the existence of harms by identifying categories of LLM-perpetuated harms toward the disability community. We conducted 19 focus groups, during which 56 participants with disabilities probed a dialog model about disability and discussed and annotated its responses. Participants rarely characterized model outputs as blatantly offensive or toxic. Instead, participants used nuanced language to detail how the dialog model mirrored subtle yet harmful stereotypes they encountered in their lives and dominant media, e.g., inspiration porn and able-bodied saviors. Participants often implicated training data as a cause for these stereotypes and recommended training the model on diverse identities from disability-positive resources. Our discussion further explores representative data strategies to mitigate harm related to different communities through annotation co-design with ML researchers and developers.", "keywords": "'data annotation', 'large language models', 'algorithmic harms', 'disability representation', 'qualitative', 'artificial intelligence', 'dialog model', 'chatbot'", "ccs_concepts": "'Human-centered computing _ Empirical studies in accessibility'", "author_names": "'Vinitha Gadiraju', 'Shaun Kane', 'Sunipa Dev', 'Alex Taylor', 'Ding Wang', 'Emily Denton', 'Robin Brewer'", "author_affiliations": "'University of Colorado Boulder', 'Google Research', 'Google Research', 'City, University of London', 'Google Research', 'Google Research', 'University of Michigan'"}, {"link": "https://doi.org/10.1145/3593013.3594123", "title": "Disparities in Text-to-Image Model Concept Possession Across Languages", "abstract": "We propose the notion of conceptual possession in generative text-to-image (T2I) systems, wherein a model is considered to possess a concept if it can generate a distinctive, correct, and self-consistent population of images for a simple prompt containing that concept. We use this idea to develop a model benchmark of multilingual parity in conceptual possession across a set of almost 200 tangible nouns across 7 languages: English, Spanish, German, Chinese, Japanese, Hebrew, and Indonesian. This technique allows us to estimate how well-suited a model is to a target language as well as identify model-specific weaknesses, spurious correlations, and biases without a-priori assumptions. We demonstrate how it can be used to benchmark T2I models in terms of multilinguality, and that despite its simplicity our method captures the necessary conditions for the impressive \u201a\u00c4\u00facreative\u201a\u00c4\u00f9 generative abilities users expect from T2I models. Our benchmark will guide future work in reducing disparities across languages, improving accessibility of these technologies.", "keywords": "'text-to-image models', 'stable diffusion', 'dall-e', 'multilingual accessibility', 'benchmark'", "ccs_concepts": "'Computing methodologies _ Machine translation', 'Computing methodologies _ Language resources', 'Image representations', 'Human-centered computing _ Accessibility design and evaluation methods'", "author_names": "'Michael Saxon', 'William Yang Wang'", "author_affiliations": "'Computer Science, University of California, Santa Barbara', 'Computer Science, University of California, Santa Barbara'"}, {"link": "https://doi.org/10.1145/3593013.3594004", "title": "On the Independence of Association Bias and Empirical Fairness in Language Models", "abstract": "The societal impact of pre-trained language models has prompted researchers to probe them for strong associations between protected attributes and value-loaded terms, from slur to prestigious job titles. Such work is said to probe models for bias or fairness\u201a\u00c4\u00eeor such probes \u201a\u00c4\u00f2into representational biases\u201a\u00c4\u00f4 are said to be \u201a\u00c4\u00f2motivated by fairness\u201a\u00c4\u00f4\u201a\u00c4\u00eesuggesting an intimate connection between bias and fairness. We provide conceptual clarity by distinguishing between association biases [11] and empirical fairness [56] and show the two can be independent. Our main contribution, however, is showing why this should not come as a surprise. To this end, we first provide a thought experiment, showing how association bias and empirical fairness can be completely orthogonal. Next, we provide empirical evidence that there is no correlation between bias metrics and fairness metrics across the most widely used language models. Finally, we survey the sociological and psychological literature and show how this literature provides ample support for expecting these metrics to be uncorrelated.", "keywords": "'Representational Bias', 'Fairness', 'Natural Language Processing'", "ccs_concepts": "'Computing methodologies _ Natural language processing'", "author_names": "'Laura Cabello', 'Anna Katrine J\u221a\u220frgensen', 'Anders S\u221a\u220fgaard'", "author_affiliations": "'University of Copenhagen', 'University of Copenhagen', 'University of Copenhagen'"}, {"link": "https://doi.org/10.1145/3593013.3594086", "title": "Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity", "abstract": "Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from queries when Nk \u201a\u00e2\u2122 n using techniques from compressed sensing (n is the size of the test dataset and Nk is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individuals? To address this supposed violation of regulations and privacy, we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters).", "keywords": "'algorithmic fairness', 'compliance', 'compressed sensing', 'differential privacy', 'machine learning.'", "ccs_concepts": "'Social and professional topics _ Governmental regulations', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Social and professional topics _ User characteristics', 'General and reference _ Evaluation', 'Security and privacy _ Privacy-preserving protocols'", "author_names": "'Faisal Hamman', 'Jiahao Chen', 'Sanghamitra Dutta'", "author_affiliations": "'Department of Electrical and Computer Engineering, University of Maryland, College Park', 'Responsible AI LLC', 'Department of Electrical and Computer Engineering, University of Maryland, College Park'"}, {"link": "https://doi.org/10.1145/3593013.3593982", "title": "In the Name of Fairness: Assessing the Bias in Clinical Record De-Identification", "abstract": "Data sharing is crucial for open science and reproducible research, but the legal sharing of clinical data requires the removal of protected health information from electronic health records. This process, known as de-identification, is often achieved through the use of machine learning algorithms by many commercial and open-source systems. While these systems have shown compelling results on average, the variation in their performance across different demographic groups has not been thoroughly examined. In this work, we investigate the bias of de-identification systems on names in clinical notes via a large-scale empirical analysis. To achieve this, we create 16 name sets that vary along four demographic dimensions: gender, race, name popularity, and the decade of popularity. We insert these names into 100 manually curated clinical templates and evaluate the performance of nine public and private de-identification methods. Our findings reveal that there are statistically significant performance gaps along a majority of the demographic dimensions in most methods. We further illustrate that de-identification quality is affected by polysemy in names, gender context, and clinical note characteristics. To mitigate the identified gaps, we propose a simple and method-agnostic solution by fine-tuning de-identification methods with clinical context and diverse names. Overall, it is imperative to address the bias in existing methods immediately so that downstream stakeholders can build high-quality systems to serve all demographic parties fairly.", "keywords": "'Fairness', 'Named Entity Recognition', 'Clinical De-identification'", "ccs_concepts": "'Human-centered computing _ Fairness', 'Computing methodologies _ Natural language processing', 'Social and professional topics _ Patient privacy'", "author_names": "'Yuxin Xiao', 'Shulammite Lim', 'Tom Joseph Pollard', 'Marzyeh Ghassemi'", "author_affiliations": "'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology'"}, {"link": "https://doi.org/10.1145/3593013.3594015", "title": "The Privacy-Bias Tradeoff: Data Minimization and Racial Disparity Assessments in U.S. Government", "abstract": "An emerging concern in algorithmic fairness is the tension with privacy interests. Data minimization can restrict access to protected attributes, such as race and ethnicity, for bias assessment and mitigation. Less recognized is that for nearly 50 years, the federal government has been engaged in a large-scale experiment in data minimization, limiting (a) data sharing across federal agencies under the Privacy Act of 1974, and (b) data collection under the Paperwork Reduction Act. We document how this \u201a\u00c4\u00faprivacy-bias tradeoff\u201a\u00c4\u00f9 has become an important battleground for fairness assessments in the U.S. government and provides rich lessons for resolving these tradeoffs. President Biden\u201a\u00c4\u00f4s 2021 racial justice Executive Order 13,985 mandated that federal agencies conduct equity impact assessments (e.g., for racial disparities) of federal programs. We conduct a comprehensive assessment across high-volume claims agencies that affect many individuals, as well as all agencies filing \u201a\u00c4\u00faequity action plans,\u201a\u00c4\u00f9 with three findings. First, there is broad agreement in principle that equity impact assessments are important, with few parties raising privacy challenges in theory and many agencies proposing substantial efforts. Second, in practice, major agencies do not collect and may be affirmatively prohibited under the Privacy Act from linking demographic information. This has led to pathological results: until 2022, for instance, the US Dept. of Agriculture imputed race by \u201a\u00c4\u00favisual observation\u201a\u00c4\u00f9 when race information was not collected. Data minimization has meant that even where agencies want to acquire demographic information in principle, the legal, data infrastructure, and bureaucratic hurdles are severe. Third, we derive policy implications to address these barriers.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Jennifer King', 'Daniel Ho', 'Arushi Gupta', 'Victor Wu', 'Helen Webley-Brown'", "author_affiliations": "'Stanford Institute for Human-Centered Artificial Intelligence, Stanford University', 'Stanford Law School, Stanford University', 'Stanford University', 'Stanford Law School, Stanford University', 'Massachusetts Institute of Technology'"}, {"link": "https://doi.org/10.1145/3593013.3594000", "title": "\u201a\u00c4\u00f2Affordances\u201a\u00c4\u00f4 for Machine Learning", "abstract": "The field of machine learning (ML) has long struggled with a principles-to-practice gap, whereby careful codes and commitments dissipate on their way to practical application. The present work bridges this gap through an applied affordance framework. \u201a\u00c4\u00f2Affordances\u201a\u00c4\u00f4 are how the features of a technology shape, but do not determine, the functions and effects of that technology. Here, I demonstrate the value of an affordance framework as applied to ML, considering ML systems through the prism of design studies. Specifically, I apply the mechanisms and conditions framework of affordances, which models the way technologies request, demand, encourage, discourage, refuse, and allow technical and social outcomes. Illustrated through three case examples across work, policing, and housing justice, the mechanisms and conditions framework reveals the social nature of technical choices, clarifying how and for whom those choices manifest. This approach displaces vagaries and general claims with the particularities of systems in context, empowering critically minded practitioners while holding power\u201a\u00c4\u00eeand the systems power relations produce\u201a\u00c4\u00eeto account. More broadly, this work pairs the design studies tradition with the ML domain, setting a foundation for deliberate and considered (re)making of sociotechnical futures.", "keywords": "'Affordances', 'Machine Learning', 'Design Studies', 'Mechanisms and Conditions Framework', 'AI Alignment', 'Principles-to-Practice'", "ccs_concepts": "'Human-centered computing _ Interaction design process and methods'", "author_names": "'Jenny L Davis'", "author_affiliations": "'School of Sociology, The Australian National University'"}, {"link": "https://doi.org/10.1145/3593013.3594046", "title": "UNFair: Search Engine Manipulation, Undetectable by Amortized Inequity", "abstract": "Modern society increasingly relies on Information Retrieval systems to answer various information needs. Since this impacts society in many ways, there has been a great deal of work to ensure the fairness of these systems, and to prevent societal harms. There is a prevalent risk of failing to model the entire system, where nefarious actors can produce harm outside the scope of fairness metrics. We demonstrate the practical possibility of this risk through UNFair, a ranking system that achieves performance and measured fairness competitive with current state-of-the-art, while simultaneously being manipulative in setup. UNFair demonstrates how adhering to a fairness metric, Amortized Equity, can be insufficient to prevent Search Engine Manipulation. This possibility of manipulation bypassing a fairness metric discourages imposing a fairness metric ahead of time, and motivates instead a more holistic approach to fairness assessments.", "keywords": "'Fairness', 'Information Retrieval', 'Search Engine Manipulation Effect', 'Exposure', 'UNFair'", "ccs_concepts": "'Information systems _ Learning to rank'", "author_names": "'Tim De Jonge', 'Djoerd Hiemstra'", "author_affiliations": "'Radboud University', 'Radboud University'"}, {"link": "https://doi.org/10.1145/3593013.3594081", "title": "The Devil is in the Details: Interrogating Values Embedded in the Allegheny Family Screening Tool", "abstract": "The design decisions of developers and researchers in creating algorithmic tools \u201a\u00c4\u00ee like constructing variables, performing feature selection, and binning model outputs \u201a\u00c4\u00ee are sometimes cast as objective technical processes. In reality, these decisions are far from objective, and they are sometimes even made arbitrarily. In this work, we examine how algorithmic design choices can function as policy decisions through an audit of a deployed algorithmic tool, the Allegheny Family Screening Tool (AFST), used to screen calls to a child welfare agency about alleged child neglect in Allegheny County, Pennsylvania. We analyze design decisions in the AFST\u201a\u00c4\u00f4s development process related to feature selection, data collection, and post-processing, highlighting three values implicitly embedded in the tool through these decisions. By aggregating risk scores at the household level, the AFST effectively treats families as \u201a\u00c4\u00farisky\u201a\u00c4\u00f9 by association. In choosing to use training data from the criminal legal system and behavioral health agencies, the AFST prioritizes \u201a\u00c4\u00famaking decisions based on as much information as possible,\u201a\u00c4\u00f9 even when that information is potentially biased across race, disability, and other protected statuses. Finally, by including static features in the model that identify whether a person has ever been affected by the criminal legal system or relied on public benefits, the AFST chooses to mark families in perpetuity, compounding the impacts of systemic discrimination and foreclosing opportunities for recourse for families impacted by the tool. We explore the impacts of these decisions, individually and together, arguing that they function as policy choices that may have discriminatory effects and raise concerns about lack of democratic oversight.", "keywords": "'Algorithm', 'audit', 'policy', 'design', 'values', 'accountability'", "ccs_concepts": "'Applied computing _ Law', 'social and behavioral sciences', 'Social and professional topics _ Computing / technology policy'", "author_names": "'Marissa Gerchick', 'Tobi Jegede', 'Tarak Shah', 'Ana Gutierrez', 'Sophie Beiers', 'Noam Shemtov', 'Kath Xu', 'Anjana Samant', 'Aaron Horowitz'", "author_affiliations": "'American Civil Liberties Union', 'American Civil Liberties Union', 'Human Rights Data Analysis Group', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union'"}, {"link": "https://doi.org/10.1145/3593013.3594120", "title": "Bias as Boundary Object: Unpacking The Politics Of An Austerity Algorithm Using Bias Frameworks", "abstract": "Whether bias is an appropriate lens for analysis and critique remains a subject of debate among scholars. This paper contributes to this conversation by unpacking the use of bias in a critical analysis of a controversial austerity algorithm introduced by the Austrian public employment service in 2018. It was envisioned to classify the unemployed into three risk categories based on predicted prospects for re-employment. The system promised to increase efficiency and effectivity of counseling while objectifying a new austerity support measure allocation scheme. This approach was intended to cut spending for those deemed at highest risk of long term unemployment. Our in-depth analysis, based on internal documentation not available to the public, systematically traces and categorizes various problematic biases to illustrate harms to job seekers and challenge promises used to justify the adoption of the system. The classification is guided by a long-established bias framework for computer systems developed by Friedman and Nissenbaum, which provides three sensitizing basic categories. We identified in our analysis \"technical biases,\" like issues around measurement, rigidity, and coarseness of variables, \"emergent biases,\" such as disruptive events that change the labor market, and, finally, \"preexisting biases,\" like the use of variables that act as proxies for inequality.  Grounded in our case study, we argue that articulated biases can be strategically used as boundary objects to enable different actors to critically debate and challenge problematic systems without prior consensus building. We unpack benefits and risks of using bias classification frameworks to guide analysis. They have recently received increased scholarly attention and thereby may influence the identification and construction of biases. By comparing four bias frameworks and drawing on our case study, we illustrate how they are political by prioritizing certain aspects in analysis while disregarding others. Furthermore, we discuss how they vary in their granularity and how this can influence analysis. We also problematize how these frameworks tend to favor explanations for bias that center the algorithm instead of social structures. We discuss several recommendations to make bias analyses more emancipatory, arguing that biases should be seen as starting points for reflection on harmful impacts, questioning the framing imposed by the imagined \u201a\u00c4\u00faunbiased\" center that the bias is supposed to distort, and seeking out deeper explanations and histories that also center bigger social structures, power dynamics, and marginalized perspectives. Finally, we reflect on the risk that these frameworks may stabilize problematic notions of bias, for example, when they become a standard or enshrined in law.", "keywords": "'public employment services', 'job seeker profiling', 'algorithmic bias', 'infrastructure studies'", "ccs_concepts": "'Applied computing _ Computing in government', 'Human-centered computing', 'Social and professional topics _ Government technology policy', 'Computing methodologies _ Machine learning approaches'", "author_names": "'Gabriel Grill', 'Fabian Fischer', 'Florian Cech'", "author_affiliations": "'University of Michigan', 'University of Applied Arts Vienna', 'TU Wien'"}, {"link": "https://doi.org/10.1145/3593013.3594099", "title": "The Progression of Disparities within the Criminal Justice System: Differential Enforcement and Risk Assessment Instruments", "abstract": "Algorithmic risk assessment instruments (RAIs) increasingly inform decision-making in criminal justice. RAIs largely rely on arrest records as a proxy for underlying crime. Problematically, the extent to which arrests reflect overall offending can vary with the person\u201a\u00c4\u00f4s characteristics. We examine how the disconnect between crime and arrest rates impacts RAIs and their evaluation. Our main contribution is a method for quantifying this bias via estimation of the amount of unobserved offenses associated with particular demographics. These unobserved offenses are then used to augment real-world arrest records to create part real, part synthetic crime records. Using this data, we estimate that four currently deployed RAIs assign 0.5\u201a\u00c4\u00ec2.8 percentage points higher risk scores to Black individuals than to White individuals with a similar arrest record, but the gap grows to 4.5\u201a\u00c4\u00ec11.0 percentage points when we match on the semi-synthetic crime record. We conclude by discussing the potential risks around the use of RAIs, highlighting how they may exacerbate existing inequalities if the underlying disparities of the criminal justice system are not taken into account. In light of our findings, we provide recommendations to improve the development and evaluation of such tools.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Miri Zilka', 'Riccardo Fogliato', 'Jiri Hron', 'Bradley Butcher', 'Carolyn Ashurst', 'Adrian Weller'", "author_affiliations": "'University of Cambridge', 'Carnegie Mellon University', 'University of Cambridge', 'University of Sussex', 'The Alan Turing Institute', 'University of Cambridge, United Kingdom and The Alan Turing Institute'"}, {"link": "https://doi.org/10.1145/3593013.3594067", "title": "Regulating ChatGPT and Other Large Generative AI Models", "abstract": "Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.", "keywords": NaN, "ccs_concepts": "'Social and professional topics_Computing / technology policy_Government / technology policy_Governmental regulations', 'Additional Keywords and Phrases: LGAIMs', 'LGAIM regulation', 'general-purpose AI systems', 'GPAIS', 'foundation models', 'large language models', 'LLMs', 'AI regulation', 'AI Act', 'direct AI regulation', 'data protection', 'GDPR', 'Digital Services Act', 'content moderation'", "author_names": "'Philipp Hacker', 'Andreas Engel', 'Marco Mauer'", "author_affiliations": "'European New School of Digital Studies, European University Viadrina', 'Faculty of Law, Heidelberg University', 'Faculty of Law, Humboldt-University of Berlin, Germany and European New School of Digital Studies, European University Viadrina'"}, {"link": "https://doi.org/10.1145/3593013.3594078", "title": "\u201a\u00c4\u00faI\u201a\u00c4\u00f4m Fully Who I Am\u201a\u00c4\u00f9: Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation", "abstract": "Warning: This paper contains examples of gender non-affirmative language which could be offensive, upsetting, and/or triggering. Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of TGNB persons contributes to and persists within Open Language Generation (OLG). This social knowledge serves as a guide for evaluating popular large language models (LLMs) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. To do this, we introduce TANGO, a dataset of template-based real-world text curated from a TGNB-oriented community. We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on average. Our findings warrant further research on how TGNB harms manifest in LLMs and serve as a broader case study toward concretely grounding the design of gender-inclusive AI in community voices and interdisciplinary literature.", "keywords": "'Algorithmic Fairness', 'Natural Language Generation', 'AI Fairness Auditing', 'Queer Harms in AI'", "ccs_concepts": "'Computing methodologies _ Natural language generation'", "author_names": "'Anaelia Ovalle', 'Palash Goyal', 'Jwala Dhamala', 'Zachary Jaggers', 'Kai-Wei Chang', 'Aram Galstyan', 'Richard Zemel', 'Rahul Gupta'", "author_affiliations": "'Computer Science, University of California, Los Angeles', 'Alexa AI-NU, Amazon', 'Alexa AI-NU, Amazon', 'Amazon Global Diversity, Equity, & Inclusion, Amazon', 'Alexa AI-NU, Amazon, USA and Department of Computer Science, University of California, Los Angeles', 'Alexa AI-NU, Amazon', 'Alexa AI-NU, Amazon', 'Alexa AI-NU, Amazon'"}, {"link": "https://doi.org/10.1145/3593013.3594095", "title": "Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale", "abstract": "Machine learning models that convert user-written text descriptions into images are now widely available online and used by millions of users to generate millions of images a day. We investigate the potential for these models to amplify dangerous and complex stereotypes. We find a broad range of ordinary prompts produce stereotypes, including prompts simply mentioning traits, descriptors, occupations, or objects. For example, we find cases of prompting for basic traits or social roles resulting in images reinforcing whiteness as ideal, prompting for occupations resulting in amplification of racial and gender disparities, and prompting for objects resulting in reification of American norms. Stereotypes are present regardless of whether prompts explicitly mention identity and demographic language or avoid such language. Moreover, stereotypes persist despite mitigation strategies; neither user attempts to counter stereotypes by requesting images with specific counter-stereotypes nor institutional attempts to add system \u201a\u00c4\u00faguardrails\u201a\u00c4\u00f9 have prevented the perpetuation of stereotypes. Our analysis justifies concerns regarding the impacts of today\u201a\u00c4\u00f4s models, presenting striking exemplars, and connecting these findings with deep insights into harms drawn from social scientific and humanist disciplines. This work contributes to the effort to shed light on the uniquely complex biases in language-vision models and demonstrates the ways that the mass deployment of text-to-image generation models results in mass dissemination of stereotypes and resulting harms.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Federico Bianchi', 'Pratyusha Kalluri', 'Esin Durmus', 'Faisal Ladhak', 'Myra Cheng', 'Debora Nozza', 'Tatsunori Hashimoto', 'Dan Jurafsky', 'James Zou', 'Aylin Caliskan'", "author_affiliations": "'Stanford University', 'Stanford University', 'Stanford University', 'Columbia University', 'Stanford University', 'Bocconi University', 'Stanford University', 'Stanford University', 'Stanford University', 'University of Washington'"}, {"link": "https://doi.org/10.1145/3593013.3594116", "title": "On The Impact of Machine Learning Randomness on Group Fairness", "abstract": "Statistical measures for group fairness in machine learning reflect the gap in performance of algorithms across different groups. These measures, however, exhibit a high variance between different training instances, which makes them unreliable for empirical evaluation of fairness. What causes this high variance? We investigate the impact on group fairness of different sources of randomness in training neural networks. We show that the variance in group fairness measures is rooted in the high volatility of the learning process on under-represented groups. Further, we recognize the dominant source of randomness as the stochasticity of data order during training. Based on these findings, we show how one can control group-level accuracy (i.e., model fairness), with high efficiency and negligible impact on the model\u201a\u00c4\u00f4s overall performance, by simply changing the data order for a single epoch.", "keywords": "'neural networks', 'fairness', 'randomness in training', 'evaluation'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'General and reference _ Evaluation', 'Social and professional topics _ Computing / technology policy'", "author_names": "'Prakhar Ganesh', 'Hongyan Chang', 'Martin Strobel', 'Reza Shokri'", "author_affiliations": "'School of Computing, National University of Singapore', 'School of Computing, National University of Singapore', 'School of Computing, National University of Singapore', 'School of Computing, National University of Singapore'"}, {"link": "https://doi.org/10.1145/3593013.3594117", "title": "Detection and Mitigation of Algorithmic Bias via Predictive Parity", "abstract": "Predictive parity (PP), also known as sufficiency, is a core definition of algorithmic fairness essentially stating that model outputs must have the same interpretation of expected outcomes regardless of group. Testing and satisfying PP is especially important in many settings where model scores are interpreted by humans or directly provide access to opportunity, such as healthcare or banking. Solutions for PP violations have primarily been studied through the lens of model calibration. However, we find that existing calibration-based tests and mitigation methods are designed for independent data, which is often not assumable in large-scale applications such as social media or medical testing. In this work, we address this issue by developing a statistically rigorous non-parametric regression based test for PP with dependent observations. We then apply our test to illustrate that PP testing can significantly vary under the two assumptions. Lastly, we provide a mitigation solution to provide a minimally-biased post-processing transformation function to achieve PP.", "keywords": "'algorithmic fairness', 'dependent data', 'testing for fairness', 'mitigation of bias'", "ccs_concepts": NaN, "author_names": "'Cyrus DiCiccio', 'Brian Hsu', 'Yinyin Yu', 'Preetam Nandy', 'Kinjal Basu'", "author_affiliations": "'Independent', 'LinkedIn', 'LinkedIn', 'LinkedIn', 'LinkedIn'"}, {"link": "https://doi.org/10.1145/3593013.3594036", "title": "Ground(Less) Truth: A Causal Framework for Proxy Labels in Human-Algorithm Decision-Making", "abstract": "A growing literature on human-AI decision-making investigates strategies for combining human judgment with statistical models to improve decision-making. Research in this area often evaluates proposed improvements to models, interfaces, or workflows by demonstrating improved predictive performance on \u201a\u00c4\u00faground truth\u201a\u00c4\u00f4\u201a\u00c4\u00f4 labels. However, this practice overlooks a key difference between human judgments and model predictions. Whereas humans commonly reason about broader phenomena of interest in a decision \u201a\u00c4\u00ec including latent constructs that are not directly observable, such as disease status, the \u201a\u00c4\u00fatoxicity\u201a\u00c4\u00f9 of online comments, or future \u201a\u00c4\u00fajob performance\u201a\u00c4\u00f9 \u201a\u00c4\u00ec predictive models target proxy labels that are readily available in existing datasets. Predictive models\u201a\u00c4\u00f4 reliance on simplistic proxies for these nuanced phenomena makes them vulnerable to various sources of statistical bias. In this paper, we identify five sources of target variable bias that can impact the validity of proxy labels in human-AI decision-making tasks. We develop a causal framework to disentangle the relationship between each bias and clarify which are of concern in specific human-AI decision-making tasks. We demonstrate how our framework can be used to articulate implicit assumptions made in prior modeling work, and we recommend evaluation strategies for verifying whether these assumptions hold in practice. We then leverage our framework to re-examine the designs of prior human subjects experiments that investigate human-AI decision-making, finding that only a small fraction of studies examine factors related to target variable bias. We conclude by discussing opportunities to better address target variable bias in future research.", "keywords": "'algorithmic decision support', 'measurement', 'validity', 'causal diagrams', 'label bias', 'human-AI decision-making'", "ccs_concepts": "'Human-centered computing _ Human computer interaction (HCI)', 'Human-centered computing _ User studies', 'Computing methodologies _ Machine learning'", "author_names": "'Luke Guerdan', 'Amanda Coston', 'Zhiwei Steven Wu', 'Kenneth Holstein'", "author_affiliations": "'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University'"}, {"link": "https://doi.org/10.1145/3593013.3594059", "title": "ACROCPoLis: A Descriptive Framework for Making Sense of Fairness", "abstract": "Fairness is central to the ethical and responsible development and use of AI systems, with a large number of frameworks and formal notions of algorithmic fairness being available. However, many of the fairness solutions proposed revolve around technical considerations and not the needs of and consequences for the most impacted communities. We therefore want to take the focus away from definitions and allow for the inclusion of societal and relational aspects to represent how the effects of AI systems impact and are experienced by individuals and social groups. In this paper, we do this by means of proposing the ACROCPoLis framework to represent allocation processes with a modeling emphasis on fairness aspects. The framework provides a shared vocabulary in which the factors relevant to fairness assessments for different situations and procedures are made explicit, as well as their interrelationships. This enables us to compare analogous situations, to highlight the differences in dissimilar situations, and to capture differing interpretations of the same situation by different stakeholders.", "keywords": "'Algorithmic fairness; socio-technical processes; social impact of AI; responsible AI'", "ccs_concepts": "'Computer systems organization _ Embedded systems', 'Computer systems organization~Robotics', 'Networks~Network reliability'", "author_names": "'Andrea Aler Tubella', 'Dimitri Coelho Mollo', 'Adam Dahlgren Lindstr\u221a\u2202m', 'Hannah Devinney', 'Virginia Dignum', 'Petter Ericson', 'Anna Jonsson', 'Timotheus Kampik', 'Tom Lenaerts', 'Julian Alfredo Mendez', 'Juan Carlos Nieves'", "author_affiliations": "'Department of Computing Science, Ume\u221a\u2022 University', 'Department of Historical, Philosophical and Religious Studies, Ume\u221a\u2022 University', 'Department of Computing Science, Ume\u221a\u2022 University', 'Department of Computing Science, Ume\u221a\u2022 University', 'Department of Computing Science, Ume\u221a\u2022 University', 'Department of Computing Science, Ume\u221a\u2022 University', 'Department of Computing Science, Ume\u221a\u2022 University', 'Department of Computing Science, Ume\u221a\u2022 University, Sweden and SAP Signavio', 'Universit\u221a\u00a9 Libre de Bruxelles, Belgium and University of California, Berkeley', 'Department of Computing Science, Ume\u221a\u2022 University', 'Department of Computing Science, Ume\u221a\u2022 University'"}, {"link": "https://doi.org/10.1145/3593013.3593983", "title": "\u201a\u00c4\u00faHow Biased Are Your Features?\u201a\u00c4\u00f9: Computing Fairness Influence Functions with Global Sensitivity Analysis", "abstract": "Fairness in machine learning has attained significant focus due to the widespread application in high-stake decision-making tasks. Unregulated machine learning classifiers can exhibit bias towards certain demographic groups in data, thus the quantification and mitigation of classifier bias is a central concern in fairness in machine learning. In this paper, we aim to quantify the influence of different features in a dataset on the bias of a classifier. To do this, we introduce the Fairness Influence Function (FIF). This function breaks down bias into its components among individual features and the intersection of multiple features. The key idea is to represent existing group fairness metrics as the difference of the scaled conditional variances in the classifier\u201a\u00c4\u00f4s prediction and apply a decomposition of variance according to global sensitivity analysis. To estimate FIFs, we instantiate an algorithm that applies variance decomposition of classifier\u201a\u00c4\u00f4s prediction following local regression. Experiments demonstrate that captures FIFs of individual feature and intersectional features, provides a better approximation of bias based on FIFs, demonstrates higher correlation of FIFs with fairness interventions, and detects changes in bias due to fairness affirmative/punitive actions in the classifier.  The code is available at https://github.com/ReAILe/bias-explainer. The extended version of the paper is at https://arxiv.org/pdf/2206.00667.pdf.", "keywords": "'Fair Machine Learning', 'Bias', 'Explainability', 'Global Sensitivity Analysis', 'Variance Decomposition', 'Influence Functions.'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Computing methodologies _ Artificial intelligence'", "author_names": "'Bishwamittra Ghosh', 'Debabrota Basu', 'Kuldeep S. Meel'", "author_affiliations": "'National University of Singapore', 'Equipe Scool, Univ. Lille, Inria, UMR 9189 - CRIStAL, CNRS Centrale Lille, France', 'National University of Singapore'"}, {"link": "https://doi.org/10.1145/3593013.3594101", "title": "Counterfactual Prediction Under Outcome Measurement Error", "abstract": "Across domains such as medicine, employment, and criminal justice, predictive models often target labels that imperfectly reflect the outcomes of interest to experts and policymakers. For example, clinical risk assessments deployed to inform physician decision-making often predict measures of healthcare utilization (e.g., costs, hospitalization) as a proxy for patient medical need. These proxies can be subject to outcome measurement error when they systematically differ from the target outcome they are intended to measure. However, prior modeling efforts to characterize and mitigate outcome measurement error overlook the fact that the decision being informed by a model often serves as a risk-mitigating intervention that impacts the target outcome of interest and its recorded proxy. Thus, in these settings, addressing measurement error requires counterfactual modeling of treatment effects on outcomes. In this work, we study intersectional threats to model reliability introduced by outcome measurement error, treatment effects, and selection bias from historical decision-making policies. We develop an unbiased risk minimization method which, given knowledge of proxy measurement error properties, corrects for the combined effects of these challenges. We also develop a method for estimating treatment-dependent measurement error parameters when these are unknown in advance. We demonstrate the utility of our approach theoretically and via experiments on real-world data from randomized controlled trials conducted in healthcare and employment domains. As importantly, we demonstrate that models correcting for outcome measurement error or treatment effects alone suffer from considerable reliability limitations. Our work underscores the importance of considering intersectional threats to model validity during the design and evaluation of predictive models for decision support.", "keywords": "'algorithmic decision support', 'measurement', 'validity', 'causal inference', 'model evaluation'", "ccs_concepts": "'Computing methodologies _ Machine learning approaches', 'Computing methodologies _ Model verification and validation'", "author_names": "'Luke Guerdan', 'Amanda Coston', 'Kenneth Holstein', 'Zhiwei Steven Wu'", "author_affiliations": "'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University'"}, {"link": "https://doi.org/10.1145/3593013.3594042", "title": "Gender Animus Can Still Exist Under Favorable Disparate Impact: A Cautionary Tale from Online P2P Lending", "abstract": "This paper investigates gender discrimination and its underlying drivers on a prominent Chinese online peer-to-peer (P2P) lending platform. While existing studies on P2P lending focus on disparate treatment (DT), DT narrowly recognizes direct discrimination and overlooks indirect and proxy discrimination, providing an incomplete picture. In this work, we measure a broadened discrimination notion called disparate impact (DI), which encompasses any disparity in the loan\u201a\u00c4\u00f4s funding rate that does not commensurate with the actual return rate. We develop a two-stage predictor substitution approach to estimate DI from observational data. Our findings reveal (i) female borrowers, given identical actual return rates, are 3.97% more likely to receive funding, (ii) at least of this DI favoring female is indirect or proxy discrimination, and (iii) DT indeed underestimates the overall female favoritism by . However, we also identify the overall female favoritism can be explained by one specific discrimination driver, rational statistical discrimination, wherein investors accurately predict the expected return rate from imperfect observations. Furthermore, female borrowers still require 2% higher expected return rate to secure funding, indicating another driver taste-based discrimination co-exists and is against female. These results altogether tell a cautionary tale: on one hand, P2P lending provides a valuable alternative credit market where the affirmative action to support female naturally emerges from the rational crowd; on the other hand, while the overall discrimination effect (both in terms of DI or DT) favors female, concerning taste-based discrimination can persist and can be obscured by other co-existing discrimination drivers, such as statistical discrimination.", "keywords": "'Gender Discrimination', 'Disparate Impact', 'Statistical Discrimination', 'Taste-base discrimination', 'P2P Lending'", "ccs_concepts": "'Human-centered computing _ Empirical studies in collaborative and social computing', 'Applied computing _ Economics'", "author_names": "'Xudong Shen', 'Tianhui Tan', 'Tuan Phan', 'Jussi Keppo'", "author_affiliations": "'National University of Singapore', 'National University of Singapore', 'The University of Hong Kong', 'National University of Singapore'"}, {"link": "https://doi.org/10.1145/3593013.3593979", "title": "Multi-Dimensional Discrimination in Law and Machine Learning - A Comparative Overview", "abstract": "AI-driven decision-making can lead to discrimination against certain individuals or social groups based on protected characteristics/attributes such as race, gender, or age. The domain of fairness-aware machine learning focuses on methods and algorithms for understanding, mitigating, and accounting for bias in AI/ML models. Still, thus far, the vast majority of the proposed methods assess fairness based on a single protected attribute, e.g. only gender or race. In reality, though, human identities are multi-dimensional, and discrimination can occur based on more than one protected characteristic, leading to the so-called \u201a\u00c4\u00famulti-dimensional discrimination\u201a\u00c4\u00f9 or \u201a\u00c4\u00famulti-dimensional fairness\u201a\u00c4\u00f9 problem. While well-elaborated in legal literature, the multi-dimensionality of discrimination is less explored in the machine learning community. Recent approaches in this direction mainly follow the so-called intersectional fairness definition from the legal domain, whereas other notions like additive and sequential discrimination are less studied or not considered thus far. In this work, we overview the different definitions of multi-dimensional discrimination/fairness in the legal domain as well as how they have been transferred/ operationalized (if) in the fairness-aware machine learning domain. By juxtaposing these two domains, we draw the connections, identify the limitations, and point out open research directions.", "keywords": "'multi-discrimination', 'multi-fairness', 'intersectional fairness', 'sequential fairness', 'additive fairness'", "ccs_concepts": NaN, "author_names": "'Arjun Roy', 'Jan Horstmann', 'Eirini Ntoutsi'", "author_affiliations": "'Institute of Computer Science, Free University of Berlin, Germany and Research Institute CODE, Bundeswehr University Munich', 'Institute for Legal Informatics, Leibniz University of Hanover', 'Research Institute CODE, Bundeswehr University Munich'"}, {"link": "https://doi.org/10.1145/3593013.3594044", "title": "Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is Not a Decision Tree", "abstract": "Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set out the normative underpinnings of fairness metrics and technical interventions and compare these to the legal reasoning of the Court of Justice of the EU. Specifically, we show how normative assumptions often remain implicit in both disciplinary approaches and explain the ensuing limitations of current AI practice and non-discrimination law. We conclude with implications for AI practitioners and regulators.", "keywords": "'EU non-discrimination law', 'algorithmic fairness', 'machine learning', 'artificial intelligence'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Computing methodologies _ Artificial intelligence', 'Applied computing _ Law'", "author_names": "'Hilde Weerts', 'Rapha\u221a\u00b4le Xenidis', 'Fabien Tarissan', 'Henrik Palmer Olsen', 'Mykola Pechenizkiy'", "author_affiliations": "'Eindhoven University of Technology', 'Sciences Po Law School', 'CNRS & ENS Paris-Saclay', 'University of Copenhagen', 'Eindhoven University of Technology'"}, {"link": "https://doi.org/10.1145/3593013.3594121", "title": "Legal Taxonomies of Machine Bias: Revisiting Direct Discrimination", "abstract": "Previous literature on \u201a\u00c4\u00f2fair\u201a\u00c4\u00f4 machine learning has appealed to legal frameworks of discrimination law to motivate a variety of discrimination and fairness metrics and de-biasing measures. Such work typically applies the US doctrine of disparate impact rather than the alternative of disparate treatment, and scholars of EU law have largely followed along similar lines, addressing algorithmic bias as a form of indirect rather than direct discrimination. In recent work, we have argued that such focus is unduly narrow in the context of European law: certain forms of algorithmic bias will constitute direct discrimination [1]. In this paper, we explore the ramifications of this argument for existing taxonomies of machine bias and algorithmic fairness, how existing fairness metrics might need to be adapted, and potentially new measures may need to be introduced. We outline how the mappings between fairness measures and discrimination definitions implied hitherto may need to be revised and revisited.", "keywords": "'Direct Discrimination', 'Disparate Treatment', 'Bias', 'Fairness', 'Machine Learning'", "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Applied computing _ Law'", "author_names": "'Reuben Binns', 'Jeremias Adams-Prassl', 'Aislinn Kelly-Lyth'", "author_affiliations": "'Computer Science, University of Oxford', 'Law, University of Oxford', 'Law, University of Oxford'"}, {"link": "https://doi.org/10.1145/3593013.3594093", "title": "Co-Designing for Transparency: Lessons from Building a Document Organization Tool in the Criminal Justice Domain", "abstract": "Investigative journalists and public defenders conduct the essential work of examining, reporting, and arguing critical cases around police use-of-force and misconduct. In an ideal world, they would have access to well-organized records they can easily navigate and search. In reality, records can come as large, disorganized data dumps, increasing the burden on the already resource-constrained teams. In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented tool that addresses challenges in working with such data dumps. Our Document Organization Tool (DOT) is a Python library that has data cleaning, extraction, and organization features. Our collaborative design process gave us insights into the needs of under-resourced teams who work with large data dumps, such as how some domain experts became self-taught programmers to automate their tasks. To understand what type of programming paradigm could support our target users, we conducted a user study (n=18) comparing visual, programming-by-example, and traditional text-based programming tools. From our user study, we found that once users passed the initial learning stage, they could comfortably use all three paradigms. Our work offers insights for designers working with under-resourced teams who want to consolidate cutting-edge algorithms and AI techniques into unified, expressive tools. We argue user-centered tool design can contribute to the broader fight for accountability and transparency by supporting existing practitioners in their work in domains like criminal justice.", "keywords": "'Co-Design', 'Document Organization', 'User-Centered Design', 'Collaborative Design'", "ccs_concepts": "'Human-centered computing _ Interaction paradigms', 'Human-centered computing _ User studies', 'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Machine learning'", "author_names": "'Hellina Hailu Nigatu', 'Lisa Pickoff-White', 'John Canny', 'Sarah Chasins'", "author_affiliations": "'EECS, UC Berkeley', 'KQED', 'EECS, UC Berkeley', 'EECS, UC Berkeley'"}, {"link": "https://doi.org/10.1145/3593013.3594045", "title": "On (Assessing) the Fairness of Risk Score Models", "abstract": "Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such decisions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and meaningful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology \u201a\u00c4\u00ec which is widely applicable in many other settings \u201a\u00c4\u00ec in two case studies, one in recidivism risk prediction, and one in risk of major depressive disorder (MDD) prediction.", "keywords": "'Algorithmic fairness', 'Risk scores', 'Ethics', 'Ranking', 'Recidivism', 'Major depressive disorder', 'Calibration'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Applied computing _ Health informatics', 'Applied computing _ Law', 'social and behavioral sciences', 'Social and professional topics _ Computing / technology policy', 'Social and professional topics _ User characteristics'", "author_names": "'Eike Petersen', 'Melanie Ganz', 'Sune Holm', 'Aasa Feragen'", "author_affiliations": "'DTU Compute, Technical University of Denmark (DTU), Denmark and Pioneer Centre for Artificial Intelligence', 'Department of Computer Science (DIKU), University of Copenhagen, Denmark and Neurobiology Research Unit (NRU), Copenhagen University Hospital (Rigshospitalet)', 'Department of Food and Resource Economics (IFRO), University of Copenhagen, Denmark and Pioneer Centre for Artificial Intelligence', 'DTU Compute, Technical University of Denmark (DTU), Denmark and Pioneer Centre for Artificial Intelligence'"}, {"link": "https://doi.org/10.1145/3593013.3594028", "title": "Runtime Monitoring of Dynamic Fairness Properties", "abstract": "A machine-learned system that is fair in static decision-making tasks may have biased societal impacts in the long-run. This may happen when the system interacts with humans and feedback patterns emerge, reinforcing old biases in the system and creating new biases. While existing works try to identify and mitigate long-run biases through smart system design, we introduce techniques for monitoring fairness in real time. Our goal is to build and deploy a monitor that will continuously observe a long sequence of events generated by the system in the wild, and will output, with each event, a verdict on how fair the system is at the current point in time. The advantages of monitoring are two-fold. Firstly, fairness is evaluated at run-time, which is important because unfair behaviors may not be eliminated a priori, at design-time, due to partial knowledge about the system and the environment, as well as uncertainties and dynamic changes in the system and the environment, such as the unpredictability of human behavior. Secondly, monitors are by design oblivious to how the monitored system is constructed, which makes them suitable to be used as trusted third-party fairness watchdogs. They function as computationally lightweight statistical estimators, and their correctness proofs rely on the rigorous analysis of the stochastic process that models the assumptions about the underlying dynamics of the system. We show, both in theory and experiments, how monitors can warn us (1) if a bank\u201a\u00c4\u00f4s credit policy over time has created an unfair distribution of credit scores among the population, and (2) if a resource allocator\u201a\u00c4\u00f4s allocation policy over time has made unfair allocations. Our experiments demonstrate that the monitors introduce very low overhead. We believe that runtime monitoring is an important and mathematically rigorous new addition to the fairness toolbox.", "keywords": "'algorithmic fairness', 'dynamic fairness', 'runtime monitor', 'online statistical estimator'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Social and professional topics _ Computing / technology policy'", "author_names": "'Thomas Henzinger', 'Mahyar Karimi', 'Konstantin Kueffner', 'Kaushik Mallik'", "author_affiliations": "'IST Austria', 'IST Austria', 'IST Austria', 'IST Austria'"}, {"link": "https://doi.org/10.1145/3593013.3594068", "title": "On the Richness of Calibration", "abstract": "Probabilistic predictions can be evaluated through comparisons with observed label frequencies, that is, through the lens of calibration. Recent scholarship on algorithmic fairness has started to look at a growing variety of calibration-based objectives under the name of multi-calibration but has still remained fairly restricted. In this paper, we explore and analyse forms of evaluation through calibration by making explicit the choices involved in designing calibration scores. We organise these into three grouping choices and a choice concerning the agglomeration of group errors. This provides a framework for comparing previously proposed calibration scores and helps to formulate novel ones with desirable mathematical properties. In particular, we explore the possibility of grouping datapoints based on their input features rather than on predictions and formally demonstrate advantages of such approaches. We also characterise the space of suitable agglomeration functions for group errors, generalising previously proposed calibration scores. Complementary to such population-level scores, we explore calibration scores at the individual level and analyse their relationship to choices of grouping. We draw on these insights to introduce and axiomatise fairness deviation measures for population-level scores. We demonstrate that with appropriate choices of grouping, these novel global fairness scores can provide notions of (sub-)group or individual fairness.", "keywords": "'calibration', 'multicalibration', 'fairness', 'forecasting', 'evaluation'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Mathematics of computing _ Probability and statistics'", "author_names": "'Benedikt H\u221a\u2202ltgen', 'Robert C Williamson'", "author_affiliations": "'University of T\u221a\u00babingen', 'University of T\u221a\u00babingen, Germany and T\u221a\u00babingen AI Center'"}, {"link": "https://doi.org/10.1145/3593013.3594058", "title": "Bias on Demand: A Modelling Framework That Generates Synthetic Data With Bias", "abstract": "Nowadays, Machine Learning (ML) systems are widely used in various businesses and are increasingly being adopted to make decisions that can significantly impact people\u201a\u00c4\u00f4s lives. However, these decision-making systems rely on data-driven learning, which poses a risk of propagating the bias embedded in the data. Despite various attempts by the algorithmic fairness community to outline different types of bias in data and algorithms, there is still a limited understanding of how these biases relate to the fairness of ML-based decision-making systems. In addition, efforts to mitigate bias and unfairness are often agnostic to the specific type(s) of bias present in the data. This paper explores the nature of fundamental types of bias, discussing their relationship to moral and technical frameworks. To prevent harmful consequences, it is essential to comprehend how and where bias is introduced throughout the entire modelling pipeline and possibly how to mitigate it. Our primary contribution is a framework for generating synthetic datasets with different forms of biases. We use our proposed synthetic data generator to perform experiments on different scenarios to showcase the interconnection between biases and their effect on performance and fairness evaluations. Furthermore, we provide initial insights into mitigating specific types of bias through post-processing techniques. The implementation of the synthetic data generator and experiments can be found at https://github.com/rcrupiISP/BiasOnDemand.", "keywords": "'bias', 'fairness', 'synthetic data', 'moral worldviews'", "ccs_concepts": "'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Computing methodologies _ Machine learning', 'General and reference _ Metrics'", "author_names": "'Joachim Baumann', 'Alessandro Castelnovo', 'Riccardo Crupi', 'Nicole Inverardi', 'Daniele Regoli'", "author_affiliations": "'Department of Informatics, University of Zurich, Switzerland and Zurich University of Applied Sciences', 'Data Science & Artificial Intelligence, Intesa Sanpaolo, Italy and Dept. of Informatics, Systems and Communication, University Milano Bicocca', 'Data Science & Artificial Intelligence, Intesa Sanpaolo', 'Data Science & Artificial Intelligence, Intesa Sanpaolo', 'Data Science & Artificial Intelligence, Intesa Sanpaolo'"}, {"link": "https://doi.org/10.1145/3593013.3593988", "title": "The Dataset Multiplicity Problem: How Unreliable Data Impacts Predictions", "abstract": "We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfactual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets\u201a\u00c4\u00f4 factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label errors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predictions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted.", "keywords": "'Dataset multiplicity', 'procedural fairness', 'model robustness', 'data bias', 'model multiplicity'", "ccs_concepts": "'Computing methodologies _ Machine learning approaches', 'General and reference _ Evaluation', 'Social and professional topics _ Computing / technology policy', 'Theory of computation~Machine learning theory'", "author_names": "'Anna P. Meyer', 'Aws Albarghouthi', \"Loris D'Antoni\"", "author_affiliations": "'Department of Computer Sciences, University of Wisconsin - Madison', 'Department of Computer Sciences, University of Wisconsin - Madison', 'Department of Computer Sciences, University of Wisconsin - Madison'"}, {"link": "https://doi.org/10.1145/3593013.3593999", "title": "Ghosting the Machine: Judicial Resistance to a Recidivism Risk Assessment Instrument", "abstract": "Recidivism risk assessment instruments are presented as an \u201a\u00c4\u00f2evidence-based\u201a\u00c4\u00f4 strategy for criminal justice reform \u201a\u00c4\u00ec a way of increasing consistency in sentencing, replacing cash bail, and reducing mass incarceration. In practice, however, AI-centric reforms can simply add another layer to the sluggish, labyrinthine machinery of bureaucratic systems and are met with internal resistance. Through a community-informed interview-based study of 23 criminal judges and other criminal legal bureaucrats in Pennsylvania, I find that judges overwhelmingly ignore a recently-implemented sentence risk assessment instrument, which they disparage as \u201a\u00c4\u00fauseless,\u201a\u00c4\u00f9 \u201a\u00c4\u00faworthless,\u201a\u00c4\u00f9 \u201a\u00c4\u00faboring,\u201a\u00c4\u00f9 \u201a\u00c4\u00faa waste of time,\u201a\u00c4\u00f9 \u201a\u00c4\u00faa non-thing,\u201a\u00c4\u00f9 and simply \u201a\u00c4\u00fanot helpful.\u201a\u00c4\u00f9 I argue that this algorithm aversion cannot be accounted for by individuals\u201a\u00c4\u00f4 distrust of the tools or automation anxieties, per the explanations given by existing scholarship. Rather, the instrument\u201a\u00c4\u00f4s non-use is the result of an interplay between three organizational factors: county-level norms about pre-sentence investigation reports; alterations made to the instrument by the Pennsylvania Sentencing Commission in response to years of public and internal resistance; and problems with how information is disseminated to judges. These findings shed new light on the important role of organizational influences on professional resistance to algorithms, which helps explain why algorithm-centric reforms can fail to have their desired effect. This study also contributes to an empirically-informed argument against the use of risk assessment instruments: they are resource-intensive and have not demonstrated positive on-the-ground impacts.", "keywords": "'criminal justice; risk assessment instruments; algorithm aversion; human-AI interaction; community-informed'", "ccs_concepts": NaN, "author_names": "'Dasha Pruss'", "author_affiliations": "'Department of History and Philosophy of Science, University of Pittsburgh'"}, {"link": "https://doi.org/10.1145/3593013.3594027", "title": "Power and Resistance in the Twitter Bias Discourse", "abstract": "In 2020, the saliency-based image cropping tool deployed by Twitter to generate image previews was suspected of carrying a racial bias: Twitter users complained that Black people were systematically cropped out and, thus, made invisible by the cropping tool. As a response, Twitter conducted bias analyses, concluded that the cropping tool was indeed biased, and subsequently removed it. Soon after, Twitter hosted the first \"algorithmic bias bounty challenge\", inviting the general public to detect algorithmic harm in the cropping tool.  Twitter\u201a\u00c4\u00f4s image cropping algorithm is a fascinating case study for exploring the push-and-pull dynamics of power relations between, firstly, algorithmic knowledge production inherent in machine learning systems, secondly, the bias discourse as resistance, and, thirdly, ensuing corporate responses as stabilization measures towards said resistance. In order to account for this three-part narrative of the case study, this paper is structured along the examination of the following three questions: (1) How is algorithmic, and especially, data-based knowledge production entrenched in power relations? (2) In what way does the discourse around bias serve as a vehicle for resistance against said power? Why and in what way is it effective? (3) How did Twitter as a company stabilize its position within and in relation to the bias discourse?  This paper explores these questions along the following steps: Section 2 lays out the interdisciplinary theoretical perspective of the analysis, combining, firstly, a mathematical-epistemic perspective that examines the mathematics underlying both machine learning systems and bias analyses with, secondly, Foucauldian concepts that make it possible to view mathematical tools as articulations of power relations. The subsequent three sections engage with the three questions posed above: Section 3, Power, is concerned with the first question, and it focuses on the algorithmic knowledge production in relation to Twitter\u201a\u00c4\u00f4s cropping tool and its mathematical-epistemic foundations. Section 4, Resistance, addresses the second question, and it examines three bias analyses of the cropping tool, as well as their epistemic limitations, and it continues by conceptualizing the bias discourse in academic scholarship and activism as resistance to power. Section 5, Stabilization, engages with the third question, discussing Twitter\u201a\u00c4\u00f4s response to the bias accusations and the way in which the company was able to effectively stabilize its position \u201a\u00c4\u00ec rendering the bias discourse a vehicle for counter-resistance, too. This paper will be published in the open access volume Algorithmic Regimes: Methods, Interactions, and Politics (Amsterdam University Press, forthcoming), as well as on SSRN as a preprint.", "keywords": "'bias', 'power', 'discourse', 'resistance', 'Twitter', 'image cropping', 'Foucault'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Applied computing _ Law', 'social and behavioral sciences', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence'", "author_names": "'Paola Lopez'", "author_affiliations": "'University of Vienna, Austria and Weizenbaum Institute'"}, {"link": "https://doi.org/10.1145/3593013.3594057", "title": "Robustness Implies Fairness in Causal Algorithmic Recourse", "abstract": "Algorithmic recourse discloses the internal procedures of a black-box decision process where decisions have significant consequences by providing recommendations to empower beneficiaries to achieve a more favorable outcome. To ensure an effective remedy, suggested interventions must not only be cost-effective but also robust and fair. To that end, it is essential to provide similar explanations to similar individuals. This study explores the concept of individual fairness and adversarial robustness in causal algorithmic recourse and addresses the challenge of achieving both. To resolve the challenges, we propose a new framework for defining adversarially robust recourse. That setting observes the protected feature as a pseudometric and demonstrates that individual fairness is a special case of adversarial robustness. Finally, we introduce the fair robust recourse problem and establish solutions to achieve both desirable properties both theoretically and empirically.", "keywords": "'explainable AI', 'algorithmic recourse', 'counterfactual explanation', 'fairness', 'robustness'", "ccs_concepts": "'Computing methodologies _ Causal reasoning and diagnostics', 'Information systems _ Decision support systems'", "author_names": "'Ahmad-Reza Ehyaei', 'Amir-Hossein Karimi', 'Bernhard Schoelkopf', 'Setareh Maghsudi'", "author_affiliations": "'Department of Computer Science', 'Max Planck Institute for Intelligent Systems Tuebingen', 'Max Planck Institute for Intelligent Systems Tuebingen', 'Department of Computer Science, University of Tuebingen'"}, {"link": "https://doi.org/10.1145/3593013.3594097", "title": "Personalized Pricing with Group Fairness Constraint", "abstract": "In the big data era, personalized pricing has become a popular strategy that sets different prices for the same product according to individual customers\u201a\u00c4\u00f4 features. Despite its popularity among companies, this practice is controversial due to the concerns over fairness that can be potentially caused by price discrimination. In this paper, we consider the problem of single-product personalized pricing for different groups under fairness constraints. Specifically, we define group fairness constraints under different distance metrics in the personalized pricing context. We then establish a stochastic formulation that maximizes the revenue. Under the discrete price setting, we reformulate this problem as a linear program and obtain the optimal pricing policy efficiently. To bridge the gap between the discrete and continuous price setting, theoretically, we prove a general gap between the optimal revenue with continuous and discrete price set of size l. Under some mild conditions, we improve this bound to . Empirically, we demonstrate the benefits of our approach over several baseline approaches on both synthetic data and real-world data. Our results also provide managerial insights into setting a proper fairness degree as well as an appropriate size of discrete price set.", "keywords": "'personalized pricing', 'group fairness', 'statistical parity', 'social welfare'", "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Applied computing _ Electronic commerce', 'Applied computing _ Operations research'", "author_names": "'Xin Chen', 'Zexing Xu', 'Zishuo Zhao', 'Yuan Zhou'", "author_affiliations": "'Georgia Institute of Technology', 'University of Illinois Urbana-Champaign', 'University of Illinois Urbana-Champaign', 'Tsinghua University'"}, {"link": "https://doi.org/10.1145/3593013.3593994", "title": "Certification Labels for Trustworthy AI: Insights From an Empirical Mixed-Method Study", "abstract": "Auditing plays a pivotal role in the development of trustworthy AI. However, current research primarily focuses on creating auditable AI documentation, which is intended for regulators and experts rather than end-users affected by AI decisions. How to communicate to members of the public that an AI has been audited and considered trustworthy remains an open challenge. This study empirically investigated certification labels as a promising solution. Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users\u201a\u00c4\u00f4 attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios. Based on the survey results, we demonstrate that labels can significantly increase end-users\u201a\u00c4\u00f4 trust and willingness to use AI in both low- and high-stakes scenarios. However, end-users\u201a\u00c4\u00f4 preferences for certification labels and their effect on trust and willingness to use AI were more pronounced in high-stake scenarios. Qualitative content analysis of the interviews revealed opportunities and limitations of certification labels, as well as facilitators and inhibitors for the effective use of labels in the context of AI. For example, while certification labels can mitigate data-related concerns expressed by end-users (e.g., privacy and data protection), other concerns (e.g., model performance) are more challenging to address. Our study provides valuable insights and recommendations for designing and implementing certification labels as a promising constituent within the trustworthy AI ecosystem.", "keywords": "'AI', 'Audit', 'Documentation', 'Label', 'Seal', 'Certification', 'Trust', 'Trustworthy', 'User study'", "ccs_concepts": "'Human-centered computing _ Empirical studies in HCI'", "author_names": "'Nicolas Scharowski', 'Michaela Benk', 'Swen J. K\u221a\u00bahne', 'L\u221a\u00a9ane Wettstein', 'Florian Br\u221a\u00bahlmann'", "author_affiliations": "'University of Basel, Center for General Psychology and Methodology', 'ETH Zurich, Mobiliar Lab for Analytics', 'Zurich University of Applied Sciences, School of Applied Psychology', 'University of Basel, Center for General Psychology and Methodology', 'University of Basel, Center for General Psychology and Methodology'"}, {"link": "https://doi.org/10.1145/3593013.3594063", "title": "Ethical Considerations in the Early Detection of Alzheimer's Disease Using Speech and AI", "abstract": "While recent studies indicate that AI could play an important role in detecting early signs of Alzheimer's disease in speech, this use of data from individuals with cognitive decline raises numerous ethical concerns. In this paper, we identify and explain concerns related to autonomy (including consent, depersonalization and disclosure), privacy and data protection (including the handling of personal content and medical information), welfare (including distress, discrimination and reliability), transparency (including the interpretability of language features and AI-based decision-making for developers and clinicians), and fairness (including bias and the distribution of benefits). Our aim is to not only raise awareness of the ethical concerns posed by the use of AI in speech-based Alzheimer's detection, but also identify ways in which these concerns might be addressed. To this end, we conclude with a list of suggestions that could be incorporated into ethical guidelines for researchers and clinicians working in this area.", "keywords": "'ethics', '\"Alzheimers disease\"', 'speech', 'language', 'digital biomarkers', 'autonomy', 'privacy', 'welfare', 'transparency', 'fairness'", "ccs_concepts": "'Applied computing _ Health informatics', 'Computing methodologies _ Natural language processing', 'Social and professional topics _ Medical technologies'", "author_names": "'Ulla Petti', 'Rune Nyrup', 'Jeffrey M. Skopek', 'Anna Korhonen'", "author_affiliations": "'University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge'"}, {"link": "https://doi.org/10.1145/3593013.3594086", "title": "Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity", "abstract": "Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from queries when Nk \u201a\u00e2\u2122 n using techniques from compressed sensing (n is the size of the test dataset and Nk is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individuals? To address this supposed violation of regulations and privacy, we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters).", "keywords": "'algorithmic fairness', 'compliance', 'compressed sensing', 'differential privacy', 'machine learning.'", "ccs_concepts": "'Social and professional topics _ Governmental regulations', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Social and professional topics _ User characteristics', 'General and reference _ Evaluation', 'Security and privacy _ Privacy-preserving protocols'", "author_names": "'Faisal Hamman', 'Jiahao Chen', 'Sanghamitra Dutta'", "author_affiliations": "'Department of Electrical and Computer Engineering, University of Maryland, College Park', 'Responsible AI LLC', 'Department of Electrical and Computer Engineering, University of Maryland, College Park'"}, {"link": "https://doi.org/10.1145/3593013.3594029", "title": "Data Collaboratives with the Use of Decentralised Learning", "abstract": "The endeavor to find appropriate data governance frameworks capable of reconciling conflicting interests in data has dramatically gained importance across disciplines and has been discussed among legal scholars, computer scientists as well as policy-makers alike. The predominant part of the current discussion is centered around the challenging task of creating a data governance framework where data is \u201a\u00c4\u00f2as open as possible and as closed as necessary\u201a\u00c4\u00f4. In this article, we elaborate on modern approaches to data governance and their limitations. It analyses how propositions evolved from property rights in data towards the creation of data access and data sharing obligations and how the corresponding debates reflect the difficulty of developing approaches that reconcile seemingly opposite objectives \u201a\u00c4\u00ec such as giving individuals and businesses more control over \u201a\u00c4\u00f2their\u201a\u00c4\u00f4 data while at the same time ensuring its availability to different stakeholders. Furthermore, we propose a wider acknowledgement of data collaboratives powered by decentralised learning techniques as a possible remedy to the shortcomings of current data governance schemes. Hence, we propose a mild formalization of the set of existing technological solutions that could inform existing approaches to data governance issues. Our proposition is based on an abstractive notion of collaborative computation as well as on several principles that are essential for our definition of data collaboratives. By adopting an interdisciplinary perspective on data governance, this article highlights how innovative technological solutions can enhance control over data while at the same time ensuring its availability to other stakeholders and thereby contributing to the achievement of the policy goals of the European Strategy for Data.", "keywords": "'Data Governance', 'Decentralised Learning', 'Data Access', 'Data Sharing', 'European Strategy for Data'", "ccs_concepts": "'Collaborative and social computing theory', 'concepts and paradigms', 'Collaborative and social computing systems and tools', 'Privacy policies'", "author_names": "'Maciej Krzysztof Zuziak', 'Onntje Hinrichs', 'Aizhan Abdrassulova', 'Salvatore Rinzivillo'", "author_affiliations": "'Knowledge Discovery and Data Mining Laboratory (KKD), National Research Council of Italy (CNR)', 'Research Group on Law, Science, Technology and Society (LSTS), Vrije Universiteit Brussel (VUB)', 'Jagiellonian University', 'Knowledge Discovery and Data Mining Laboratory (KKD), National Research Council of Italy (CNR)'"}, {"link": "https://doi.org/10.1145/3593013.3594067", "title": "Regulating ChatGPT and Other Large Generative AI Models", "abstract": "Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.", "keywords": NaN, "ccs_concepts": "'Social and professional topics_Computing / technology policy_Government / technology policy_Governmental regulations', 'Additional Keywords and Phrases: LGAIMs', 'LGAIM regulation', 'general-purpose AI systems', 'GPAIS', 'foundation models', 'large language models', 'LLMs', 'AI regulation', 'AI Act', 'direct AI regulation', 'data protection', 'GDPR', 'Digital Services Act', 'content moderation'", "author_names": "'Philipp Hacker', 'Andreas Engel', 'Marco Mauer'", "author_affiliations": "'European New School of Digital Studies, European University Viadrina', 'Faculty of Law, Heidelberg University', 'Faculty of Law, Humboldt-University of Berlin, Germany and European New School of Digital Studies, European University Viadrina'"}, {"link": "https://doi.org/10.1145/3593013.3594103", "title": "Arbitrary Decisions Are a Hidden Cost of Differentially Private Training", "abstract": "Mechanisms used in privacy-preserving machine learning often aim to guarantee differential privacy (DP) during model training. Practical DP-ensuring training methods use randomization when fitting model parameters to privacy-sensitive data (e.g., adding Gaussian noise to clipped gradients). We demonstrate that such randomization incurs predictive multiplicity: for a given input example, the output predicted by equally-private models depends on the randomness used in training. Thus, for a given input, the predicted output can vary drastically if a model is re-trained, even if the same training dataset is used. The predictive-multiplicity cost of DP training has not been studied, and is currently neither audited for nor communicated to model designers and stakeholders. We derive a bound on the number of re-trainings required to estimate predictive multiplicity reliably. We analyze\u201a\u00c4\u00eeboth theoretically and through extensive experiments\u201a\u00c4\u00eethe predictive-multiplicity cost of three DP-ensuring algorithms: output perturbation, objective perturbation, and DP-SGD. We demonstrate that the degree of predictive multiplicity rises as the level of privacy increases, and is unevenly distributed across individuals and demographic groups in the data. Because randomness used to ensure DP during training explains predictions for some examples, our results highlight a fundamental challenge to the justifiability of decisions supported by differentially-private models in high-stakes settings. We conclude that practitioners should audit the predictive multiplicity of their DP-ensuring algorithms before deploying them in applications of individual-level consequence.", "keywords": NaN, "ccs_concepts": "'Computing methodologies _ Machine learning', 'Neural networks', 'Computing methodologies _ Model verification and validation', 'Security and privacy _ Privacy protections', 'Security and privacy _ Social aspects of security and privacy'", "author_names": "'Bogdan Kulynych', 'Hsiang Hsu', 'Carmela Troncoso', 'Flavio P. Calmon'", "author_affiliations": "'SPRING Lab, EPFL', 'Harvard University', 'SPRING Lab, EPFL', 'Harvard University'"}, {"link": "https://doi.org/10.1145/3593013.3594087", "title": "Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies", "abstract": "AI systems are adopted in numerous domains due to their increasingly strong predictive performance. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time-consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our work highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other\u201a\u00c4\u00f4s work and produce generalizable scientific knowledge. We also hope this work will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Vivian Lai', 'Chacha Chen', 'Alison Smith-Renner', 'Q. Vera Liao', 'Chenhao Tan'", "author_affiliations": "'University of Colorado Boulder', 'University of Chicago', 'Dataminr Inc.', 'Microsoft Research', 'University of Chicago'"}, {"link": "https://doi.org/10.1145/3593013.3594003", "title": "Simplicity Bias Leads to Amplified Performance Disparities", "abstract": "Which parts of a dataset will a given model find difficult? Recent work has shown that SGD-trained models have a bias towards simplicity, leading them to prioritize learning a majority class, or to rely upon harmful spurious correlations. Here, we show that the preference for \u201a\u00c4\u00f2easy\u201a\u00c4\u00f4 runs far deeper: A model may prioritize any class or group of the dataset that it finds simple\u201a\u00c4\u00eeat the expense of what it finds complex\u201a\u00c4\u00eeas measured by performance difference on the test set. When subsets with different levels of complexity align with demographic groups, we term this difficulty disparity, a phenomenon that occurs even with balanced datasets that lack group/label associations. We show how difficulty disparity is a model-dependent quantity, and is further amplified in commonly-used models as selected by typical average performance scores. We quantify an amplification factor across a range of settings in order to compare disparity of different models on a fixed dataset. Finally, we present two real-world examples of difficulty amplification in action, resulting in worse-than-expected performance disparities between groups even when using a balanced dataset. The existence of such disparities in balanced datasets demonstrates that merely balancing sample sizes of groups is not sufficient to ensure unbiased performance. We hope this work presents a step towards measurable understanding of the role of model bias as it interacts with the structure of data, and call for additional model-dependent mitigation methods to be deployed alongside dataset audits.", "keywords": "'neural networks', 'simplicity bias', 'performance disparities', 'fairness'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Computing methodologies _ Neural networks', 'Computing methodologies _ Computer vision', 'Social and professional topics _ Socio-technical systems', 'General and reference _ Empirical studies', 'General and reference _ Evaluation'", "author_names": "'Samuel James Bell', 'Levent Sagun'", "author_affiliations": "'FAIR, Meta AI', 'FAIR, Meta AI'"}, {"link": "https://doi.org/10.1145/3593013.3594084", "title": "A Sociotechnical Audit: Assessing Police Use of Facial Recognition", "abstract": "Algorithmic audits are increasingly used to hold people accountable for the algorithms they implement. However, much work remains to integrate ethical and legal evaluations of how algorithms are used into audits. In this paper, we present a sociotechnical audit to help external stakeholders evaluate the ethics and legality of police use of facial recognition technology. We developed this audit for the specific legal context of England and Wales, and to bring attention to broader concerns such as whether police consult affected communities and comply with human rights law. To design this audit, we compiled ethical and legal standards for governing facial recognition, based on existing literature and feedback from academia, government, civil society, and police organizations. We then applied the resulting audit tool to three facial recognition deployments by police forces in the UK and found that all three failed to meet these standards. Developing this audit helps us provide insights to researchers in designing their own sociotechnical audits, specifically how audits shift power, how to make audits context-specific, how audits reveal what is not transparent, and how audits lead to accountability.", "keywords": "'algorithmic audits', 'accountability', 'ethical and legal considerations', 'facial recognition technology'", "ccs_concepts": "'Social and professional topics _ Surveillance', 'Social and professional topics _ Technology audits', 'Security and privacy _ Human and societal aspects of security and privacy'", "author_names": "'Evani Radiya-Dixit', 'Gina Neff'", "author_affiliations": "'Minderoo Centre for Technology and Democracy, University of Cambridge', 'Minderoo Centre for Technology and Democracy, University of Cambridge'"}, {"link": "https://doi.org/10.1145/3593013.3593987", "title": "In Her Shoes: Gendered Labelling in Crowdsourced Safety Perceptions Data from India", "abstract": "In recent years, a proliferation of women\u201a\u00c4\u00f4s safety mobile applications have emerged in India that crowdsource street safety perceptions to generate \u201a\u00c4\u00f2safety maps\u201a\u00c4\u00f4 used by policy makers for urban design and academics for studying mobility patterns. Men and women\u201a\u00c4\u00f4s differential access to information and communication technologies (ICTs), however, and the distinctions between their social and cultural subjective experiences may mitigate the value of crowdsourced safety perceptions data and the predictive ability of machine learning (ML) models utilizing such data. We explore this by collecting and analyzing primary data on safety perceptions from New Delhi, India. Our curated dataset consists of streetviews covering a wide range of neighborhoods for which we obtain subjective safety ratings from both female and male respondents. Simulation experiments where varying the proportion of ratings from each gender are assumed missing demonstrate that the predictive ability of standard ML techniques relies crucially on the distribution of data producers. We find that obtaining large amounts of crowdsourced safety labels from male respondents for predicting female safety perceptions is inefficient in a number of scenarios and even undesirable in others. Detailed comparisons between female and male respondents\u201a\u00c4\u00f4 data demonstrate significant gender differences in safety perceptions and associated vocabularies. Our results have important implications for the design of platforms relying on crowdsourced data and the insights generated from them.", "keywords": "'crowdsourced ratings', 'safety', 'gender', 'algorithmic bias', 'India'", "ccs_concepts": "'Human-centered computing _ HCI design and evaluation methods', 'Human-centered computing _ Empirical studies in collaborative and social computing'", "author_names": "'Nandana Sengupta', 'Ashwini Vaidya', 'James Evans'", "author_affiliations": "'Indian Institute of Technology Delhi', 'Indian Institute of Technology Delhi', 'University of Chicago'"}, {"link": "https://doi.org/10.1145/3593013.3594096", "title": "What's Fair Is\u201a\u00c4\u00b6 Fair? Presenting JustEFAB, an Ethical Framework for Operationalizing Medical Ethics and Social Justice in the Integration of Clinical Machine Learning: JustEFAB", "abstract": "The problem of algorithmic bias represents an ethical threat to the fair treatment of patients when their care involves machine learning (ML) models informing clinical decision-making. The design, development, testing, and integration of ML models therefore require a lifecycle approach to bias identification and mitigation efforts. Presently, most work focuses on the ML tool alone, neglecting the larger sociotechnical context in which these models operate. Moreover, the narrow focus on technical definitions of fairness must be integrated within the larger context of medical ethics in order to facilitate equitable care with ML. Drawing from principles of medical ethics, research ethics, feminist philosophy of science, and justice-based theories, we describe the Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness. This paper describes JustEFAB's development and vetting through multiple advisory groups and the lifecycle approach to addressing fairness in clinical ML tools. We present an ethical decision-making framework to support design and development, adjudication between ethical values as design choices, silent trial evaluation, and prospective clinical evaluation guided by medical ethics and social justice principles. We provide some preliminary considerations for oversight and safety to support ongoing attention to fairness issues. We envision this guideline as useful to many stakeholders, including ML developers, healthcare decision-makers, research ethics committees, regulators, and other parties who have interest in the fair and judicious use of clinical ML tools.", "keywords": "'algorithmic bias', 'fairness', 'clinical machine learning', 'ethics', 'organizational ethics', 'justice', 'accountability', 'healthcare', 'health policy', 'safe deployment'", "ccs_concepts": "'Social and professional topics', 'Computing/technology policy', 'Medical information policy', 'Medical technologies'", "author_names": "'Melissa Mccradden', 'Oluwadara Odusi', 'Shalmali Joshi', 'Ismail Akrout', 'Kagiso Ndlovu', 'Ben Glocker', 'Gabriel Maicas', 'Xiaoxuan Liu', 'Mjaye Mazwi', 'Tee Garnett', 'Lauren Oakden-Rayner', 'Myrtede Alfred', 'Irvine Sihlahla', 'Oswa Shafei', 'Anna Goldenberg'", "author_affiliations": "'The Hospital for Sick Children', 'The University of Sheffield Medical School', 'Columbia University', 'The Hospital for Sick Children', 'University of Botswana', 'Imperial College London', 'Australian Institute for Machine Learning', 'University of Birmingham', 'The Hospital for Sick Children', 'The Hospital for Sick Children', 'Australian Institute for Machine Learning', 'University of Toronto', 'University of Cape Town', 'The Hospital for Sick Children', 'The Hospital for Sick Children'"}, {"link": "https://doi.org/10.1145/3593013.3594038", "title": "Your Browsing History May Cost You: A Framework for Discovering Differential Pricing in Non-Transparent Markets", "abstract": "In many online markets we \u201a\u00c4\u00fashop alone\u201a\u00c4\u00f9 \u201a\u00c4\u00ee there is no way for us to know the prices other consumers paid for the same goods. Could this lack of price transparency lead to differential pricing? To answer this question, we present a generalized framework to audit online markets for differential pricing using automated agents. Consensus is a key idea in our work: for a successful black-box audit, both the experimenter and seller must agree on the agents\u201a\u00c4\u00f4 attributes. We audit two competitive online travel markets on kayak.com (flight and hotel markets) and construct queries representative of the demand for goods. Crucially, we assume ignorance of the sellers\u201a\u00c4\u00f4 pricing mechanisms while conducting these audits. We conservatively implement consensus with nine distinct profiles based on behavior, not demographics. We use a structural causal model for price differences and estimate model parameters using Bayesian inference. We can unambiguously show that many sellers (but not all) demonstrate behavior-driven differential pricing. In the flight market, some profiles are nearly more likely to see a worse price than the best performing profile, and nearly more likely in the hotel market. While the control profile (with no browsing history) was on average offered the best prices in the flight market, surprisingly, other profiles outperformed the control in the hotel market. The price difference between any pair of profiles occurring by chance is $\u201a\u00c4\u00e20.44 in the flight market and $\u201a\u00c4\u00e20.09 for hotels. However, the expected loss of welfare for any profile when compared to the best profile can be as much as $\u201a\u00c4\u00e26.00 for flights and $\u201a\u00c4\u00e23.00 for hotels (i.e., 15 \u221a\u00f3 and 33 \u221a\u00f3 the price difference by chance respectively). This illustrates the need for new market designs or policies that encourage more transparent market design to overcome differential pricing practices.", "keywords": NaN, "ccs_concepts": "'General and reference _ Empirical studies', 'Information systems _ E-commerce infrastructure', 'Theory of computation _ Bayesian analysis'", "author_names": "'Aditya Karan', 'Naina Balepur', 'Hari Sundaram'", "author_affiliations": "'Department of Computer Science, University of Illinois at Urbana-Champaign', 'Department of Computer Science, University of Illinois at Urbana-Champaign', 'Department of Computer Science, University of Illinois at Urbana-Champaign'"}, {"link": "https://doi.org/10.1145/3593013.3594014", "title": "Algorithms as Social-Ecological-Technological Systems: An Environmental Justice Lens on Algorithmic Audits", "abstract": "This paper reframes algorithmic systems as intimately connected to and part of social and ecological systems, and proposes a first-of-its-kind methodology for environmental justice-oriented algorithmic audits. How do we consider environmental and climate justice dimensions of the way algorithmic systems are designed, developed, and deployed? These impacts are inherently emergent and can only be understood and addressed at the level of relations between an algorithmic system and the social (including institutional) and ecological components of the broader ecosystem it operates in. As a result, we claim that in absence of an integral ontology for algorithmic systems, we cannot do justice to the emergent nature of broader environmental impacts of algorithmic systems and their underlying computational infrastructure. Furthermore, an integral lens provides many lessons from the history of environmental justice that are of relevance in current day struggles for algorithmic justice. We propose to define algorithmic systems as ontologically indistinct from Social-Ecological-Technological Systems (SETS), framing emergent implications as couplings between social, ecological, and technical components of the broader fabric in which algorithms are integrated and operate. We draw upon prior work on SETS analysis as well as emerging themes in the literature and practices of Environmental Justice (EJ) to conceptualize and assess algorithmic impact. We then offer three policy recommendations to help establish a SETS-based EJ approach to algorithmic audits: (1) broaden the inputs and open-up the outputs of an audit, (2) enable meaningful access to redress, and (3) guarantee a place-based and relational approach to the process of evaluating impact. We operationalize these as a qualitative framework of questions for a spectrum of stakeholders. Doing so, this article aims to inspire stronger and more frequent interactions across policymakers, researchers, practitioners, civil society, and grassroots communities. https://arxiv.org/abs/2305.05733.", "keywords": NaN, "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Social and professional topics _ Technology audits', 'Software and its engineering _ Software development process management'", "author_names": "'Bogdana Rakova', 'Roel Dobbe'", "author_affiliations": "'Mozilla Foundation', 'Delft University of Technology'"}, {"link": "https://doi.org/10.1145/3593013.3593981", "title": "The Gradient of Generative AI Release: Methods and Considerations", "abstract": "As increasingly powerful generative AI systems are developed, the release method greatly varies. We propose a framework to assess six levels of access to generative AI systems: fully closed; gradual or staged access; hosted access; cloud-based or API access; downloadable access; and fully open. Each level, from fully closed to fully open, can be viewed as an option along a gradient. We outline key considerations across this gradient: release methods come with tradeoffs, especially around the tension between concentrating power and mitigating risks. Diverse and multidisciplinary perspectives are needed to examine and mitigate risk in generative AI systems from conception to deployment. We show trends in generative system release over time, noting closedness among large companies for powerful systems and openness among organizations founded on principles of openness. We also enumerate safety controls and guardrails for generative systems and necessary investments to improve future releases.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Irene Solaiman'", "author_affiliations": "'Hugging Face'"}, {"link": "https://doi.org/10.1145/3593013.3594066", "title": "Emotions and Dynamic Assemblages: A Study of Automated Social Security Using Qualitative Longitudinal Research", "abstract": "In this paper we argue that qualitative longitudinal research (QLLR) is a crucial research method for studying automated decision-making (ADM) systems as complex, dynamic digital assemblages. QLLR provides invaluable insight into the lived experiences of users as data subjects of ADMs as well as into the broader digital assemblage in which these systems operate. To demonstrate the utility of this method, we draw on an ongoing, empirical study examining Universal Credit (UC), an automated social security payment used in the United Kingdom. UC is digital-by-default and uses a dynamic, means-testing payment system to determine the monthly amount of claim people are entitled to.  We first provide a brief overview of the key epistemological challenges of studying ADMs before situating our study in relation to existing qualitative analyses of ADMs and their users, as well as qualitative longitudinal research. We highlight that, thus far, QLLR has been severely under-utilized in studying ADM systems. After a brief description of our study, aims and methodology, we present our findings illustrated through empirical cases that demonstrate the potential of QLLR in this area.  Overall, we argue that QLLR provides a unique opportunity to gather information on ADMs, both over time and in real time. Capturing information real-time allows for more granular accounts and provides an opportunity for gathering in situ data on emotions and attitudes of users and data subjects. The ability to record qualitative data over time has the potential to capture dynamic trajectories, including the fluctuations and uncertainties comprising users\u201a\u00c4\u00f4 lived experiences. Through the personal accounts of data subjects, QLLR also gives researchers insight into how the emotional dimensions of users\u201a\u00c4\u00f4 interactions with ADMs shapes their actions responding to these systems.", "keywords": "'Automated Social Security', 'Digital Social Security', 'Qualitative Research', 'Longitudinal Research', 'Interviews'", "ccs_concepts": "'Social and professional topics', 'Social and professional topics _ Computing / technology policy'", "author_names": "'Morgan Currie', 'Lena Podoletz'", "author_affiliations": "'University of Edinburgh', 'University of Edinburgh'"}, {"link": "https://doi.org/10.1145/3593013.3594050", "title": "To Be High-Risk, or Not To Be\u201a\u00c4\u00eeSemantic Specifications and Implications of the AI Act\u201a\u00c4\u00f4s High-Risk AI Applications and Harmonised Standards", "abstract": "The EU\u201a\u00c4\u00f4s proposed AI Act sets out a risk-based regulatory framework to govern the potential harms emanating from use of AI systems. Within the AI Act\u201a\u00c4\u00f4s hierarchy of risks, the AI systems that are likely to incur \u201a\u00c4\u00fahigh-risk\u201a\u00c4\u00f9 to health, safety, and fundamental rights are subject to the majority of the Act\u201a\u00c4\u00f4s provisions. To include uses of AI where fundamental rights are at stake, Annex III of the Act provides a list of applications wherein the conditions that shape high-risk AI are described. For high-risk AI systems, the AI Act places obligations on providers and users regarding use of AI systems and keeping appropriate documentation through the use of harmonised standards. In this paper, we analyse the clauses defining the criteria for high-risk AI in Annex III to simplify identification of potential high-risk uses of AI by making explicit the \u201a\u00c4\u00facore concepts\u201a\u00c4\u00f9 whose combination makes them high-risk. We use these core concepts to develop an open vocabulary for AI risks (VAIR) to represent and assist with AI risk assessments in a form that supports automation and integration. VAIR is intended to assist with identification and documentation of risks by providing a common vocabulary that facilitates knowledge sharing and interoperability between actors in the AI value chain. Given that the AI Act relies on harmonised standards for much of its compliance and enforcement regarding high-risk AI systems, we explore the implications of current international standardisation activities undertaken by ISO and emphasise the necessity of better risk and impact knowledge bases such as VAIR that can be integrated with audits and investigations to simplify the AI Act\u201a\u00c4\u00f4s application.", "keywords": "'AI Act', 'high-risk AI', 'harmonised standards', 'taxonomy', 'semantic web'", "ccs_concepts": "'Social and professional topics _ Governmental regulations', 'Computing methodologies _ Knowledge representation and reasoning', 'Information systems _ Resource Description Framework (RDF)'", "author_names": "'Delaram Golpayegani', 'Harshvardhan J. Pandit', 'Dave Lewis'", "author_affiliations": "'ADAPT Centre, Trinity College Dublin', 'ADAPT Centre, Dublin City University', 'ADAPT Centre, Trinity College Dublin'"}, {"link": "https://doi.org/10.1145/3593013.3593973", "title": "\u201a\u00c4\u00f2We Are Adults and Deserve Control of Our Phones\u201a\u00c4\u00f4: Examining the Risks and Opportunities of a Right to Repair for Mobile Apps", "abstract": "Many mobile apps are designed not just to support end-users\u201a\u00c4\u00f4 needs, but also commercial aims. This can result in app designs that compromise end-user privacy, safety, and well-being. Since apps nowadays provide vital digital information and services, users often have no choice but to accept potentially harmful or manipulative app designs. What if, instead, individuals could customise their apps to make them safer and better suit their needs? This exploratory work examines this question through a multi-faceted approach; first, to understand user needs, we conducted a survey (n = 100) of changes users wanted in their apps, and of perceptions of risks in app repair. Second, to identify technical challenges, we developed a prototype that enables end-users to change their apps, and realised several modifications suggested by survey participants. Finally, we conduct a set of expert interviews (n = 8) to delve into the ethical and legal aspects of such a tool, and synthesise a framework of risks and opportunities of app repair.", "keywords": "'mobile apps', 'dark patterns', 'digital harms', 'right to repair', 'privacy'", "ccs_concepts": "'Human-centered computing _ Empirical studies in ubiquitous and mobile computing', 'Security and privacy _ Software and application security'", "author_names": "'Konrad Kollnig', 'Siddhartha Datta', 'Thomas Serban Von Davier', 'Max Van Kleek', 'Reuben Binns', 'Ulrik Lyngs', 'Nigel Shadbolt'", "author_affiliations": "'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford'"}, {"link": "https://doi.org/10.1145/3531146.3533237", "title": "How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India", "abstract": "Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a \u201a\u00c4\u00f2high-risk\u201a\u00c4\u00f4 AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the \u201a\u00c4\u00f2boon\u201a\u00c4\u00f4 of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.", "keywords": "'algorithmic accountability', 'algorithmic fairness', 'human-ai interaction', 'instant loans', 'socio-technical systems'", "ccs_concepts": "'Computer systems organization _ Embedded systems', 'Computer systems organization~Robotics', 'Networks~Network reliability'", "author_names": "'Divya Ramesh', 'Vaishnav Kameswaran', 'Ding Wang', 'Nithya Sambasivan'", "author_affiliations": "'Computer Science and Engineering, University of Michigan, Ann Arbor', 'School of Information, University of Michigan, Ann Arbor', 'Google Research', 'Unaffiliated'"}, {"link": "https://doi.org/10.1145/3531146.3534639", "title": "A Review of Taxonomies of Explainable Artificial Intelligence (XAI) Methods", "abstract": "The recent surge in publications related to explainable artificial intelligence (XAI) has led to an almost insurmountable wall if one wants to get started or stay up to date with XAI. For this reason, articles and reviews that present taxonomies of XAI methods seem to be a welcomed way to get an overview of the field. Building on this idea, there is currently a trend of producing such taxonomies, leading to several competing approaches to construct them. In this paper, we will review recent approaches to constructing taxonomies of XAI methods and discuss general challenges concerning them as well as their individual advantages and limitations. Our review is intended to help scholars be aware of challenges current taxonomies face. As we will argue, when charting the field of XAI, it may not be sufficient to rely on one of the approaches we found. To amend this problem, we will propose and discuss three possible solutions: a new taxonomy that incorporates the reviewed ones, a database of XAI methods, and a decision tree to help choose fitting methods.", "keywords": "'explainability', 'interpretability', 'explainable artificial intelligence', 'XAI', 'transparency', 'taxonomy', 'review'", "ccs_concepts": "'General and reference _ Surveys and overviews', 'Computing methodologies _ Artificial intelligence'", "author_names": "'Timo Speith'", "author_affiliations": "'Institute of Philosophy and Department of Computer Science, Saarland University'"}, {"link": "https://doi.org/10.1145/3531146.3533201", "title": "Accountable Data: The Politics and Pragmatics of Disclosure Datasets", "abstract": "This paper attends specifically to what I call \u201a\u00c4\u00fadisclosure datasets\u201a\u00c4\u00f9 - tabular datasets produced in accordance with laws requiring various kinds of disclosure. For the purposes of this paper, the most significant defining feature of disclosure datasets is that they aggregate information produced and reported by the same institutions they are meant to hold accountable. Through a series of case studies of disclosure datasets in the United States, I specifically draw attention to two concerns with disclosure datasets: First, for disclosure datasets, there is often political and social mobilization around the definitions that determine reporting thresholds, which in turn implicates what observations end up in the dataset. Changes in reporting thresholds can be traced along changes in political party power as the aims to promote accountability through mandated disclosure often get pitted against the aims to reduce regulatory burden. Second, for disclosure datasets, the observational unit \u201a\u00c4\u00ec what is ultimately being counted in the data \u201a\u00c4\u00ec is often not a person, institution, or action but instead a form that the reporting institution is required by law to fill out. Forms infrastructure the information that ends up in the dataset in notable ways. This work contributes to recent calls to promote the transparency and accountability of data science work through improved inquiry into and documentation of the social lineages of source datasets. The analysis of disclosure datasets presented in this paper poses important questions regarding what ultimately gets documented in the data, along with the representativeness and usefulness of these accountability mechanisms.", "keywords": "'disclosure', 'accountability', 'infrastructure', 'data provenance'", "ccs_concepts": "'Theory of computation _ Data provenance', 'Theory of computation _ Incomplete', 'inconsistent', 'and uncertain databases', 'Information systems _ Data dictionaries'", "author_names": "'Lindsay Poirier'", "author_affiliations": "'Statistical and Data Sciences, Smith College'"}, {"link": "https://doi.org/10.1145/3531146.3533780", "title": "AI Ethics Statements: Analysis and Lessons Learnt from NeurIPS Broader Impact Statements", "abstract": "Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by requiring that all papers include a broader impact statement. This requirement was removed in 2021, in favour of a checklist approach. The 2020 statements therefore provide a unique opportunity to learn from the broader impact experiment: to investigate the benefits and challenges of this and similar governance mechanisms, as well as providing an insight into how ML researchers think about the societal impacts of their own work. Such learning is needed as NeurIPS and other venues continue to question and adapt their policies. To enable this, we have created a dataset containing the impact statements from all NeurIPS 2020 papers, along with additional information such as affiliation type, location and subject area, and a simple visualisation tool for exploration. We also provide an initial quantitative analysis of the dataset, covering representation, engagement, common themes, and willingness to discuss potential harms alongside benefits. We investigate how these vary by geography, affiliation type and subject area. Drawing on these findings, we discuss the potential benefits and negative outcomes of ethics statement requirements, and their possible causes and associated challenges. These lead us to several lessons to be learnt from the 2020 requirement: (i) the importance of creating the right incentives, (ii) the need for clear expectations and guidance, and (iii) the importance of transparency and constructive deliberation. We encourage other researchers to use our dataset to provide additional analysis, to further our understanding of how researchers responded to this requirement, and to investigate the benefits and challenges of this and related mechanisms.", "keywords": "'ethics statements', 'broader impacts', 'research ethics', 'conference policies', 'AI governance', 'NeurIPS policies'", "ccs_concepts": "'Human-centered computing _ HCI design and evaluation methods'", "author_names": "'Carolyn Ashurst', 'Emmie Hine', 'Paul Sedille', 'Alexis Carlier'", "author_affiliations": "'Alan Turing Institute', 'Oxford Internet Institute', 'Harvard Kennedy School', 'Centre for the Governance of AI'"}, {"link": "https://doi.org/10.1145/3531146.3533204", "title": "Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models", "abstract": "This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity\u201a\u00c4\u00eeappropriately accounting for relevant differences across individuals\u201a\u00c4\u00eewhich is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods\u201a\u00c4\u00eeas opposed to simpler models\u201a\u00c4\u00eeshapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Emily Black', 'Hadi Elzayn', 'Alexandra Chouldechova', 'Jacob Goldin', 'Daniel Ho'", "author_affiliations": "'Computer Science Dept., Carngie Mellon University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University', 'Stanford University'"}, {"link": "https://doi.org/10.1145/3531146.3533212", "title": "Algorithms Off-limits?: If digital trade law restricts access to source code of software then accountability will suffer", "abstract": "Free trade agreements are increasingly used to construct an additional layer of protection for source code of software. This comes in the shape of a new prohibition for governments to require access to, or transfer of, source code of software, subject to certain exceptions. A clause on software source code is also part and parcel of an ambitious set of new rules on trade-related aspects of electronic commerce currently negotiated by 86 members of the World Trade Organization. Our understanding to date of how such a commitment inside trade law impacts on governments right to regulate digital technologies and the policy space that is allowed under trade law is limited. Access to software source code is for example necessary to meet regulatory and judicial needs in order to ensure that digital technologies are in conformity with individuals\u201a\u00c4\u00f4 human rights and societal values. This article will unpack and analyze the implications of such a source code clause for current and future digital policies by governments that aim to ensure transparency, fairness and accountability of computer and machine learning algorithms.", "keywords": "'Software', 'Source code', 'Computer algorithms', 'Application Programming Interface', 'International trade law', 'Digital policy', 'Transparency', 'Fairness', 'Accountability'", "ccs_concepts": "'Social and professional topics', 'Computing / technology policy', 'Government technology policy', 'Governmental regulations'", "author_names": "'Kristina Irion'", "author_affiliations": "'Institute for Information Law, University of Amsterdam'"}, {"link": "https://doi.org/10.1145/3531146.3533102", "title": "An Outcome Test of Discrimination for Ranked Lists", "abstract": "This paper extends Becker [3]\u201a\u00c4\u00f4s outcome test of discrimination to settings where a (human or algorithmic) decision-maker produces a ranked list of candidates. Ranked lists are particularly relevant in the context of online platforms that produce search results or feeds, and also arise when human decisionmakers express ordinal preferences over a list of candidates. We show that non-discrimination implies a system of moment inequalities, which intuitively impose that one cannot permute the position of a lower-ranked candidate from one group with a higher-ranked candidate from a second group and systematically improve the objective. Moreover, we show that that these moment inequalities are the only testable implications of non-discrimination when the auditor observes only outcomes and group membership by rank. We show how to statistically test the implied inequalities, and validate our approach in an application using data from LinkedIn.1", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Jonathan Roth', 'Guillaume Saint-Jacques', 'YinYin Yu'", "author_affiliations": "'Brown University', 'Apple', 'LinkedIn'"}, {"link": "https://doi.org/10.1145/3531146.3533174", "title": "Auditing for Gerrymandering by Identifying Disenfranchised Individuals", "abstract": "Gerrymandering is the practice of drawing congressional districts to advantage or disadvantage particular electoral outcomes or population groups. We study the problem of computationally auditing a districting for evidence of gerrymandering. Our approach is novel in its emphasis on identifying individual voters disenfranchised by packing and cracking in local fine-grained geographic regions. We define a local score based on comparison with a representative sample of alternative districtings and use simulated annealing to algorithmically generate a witness districting to show that the score can be substantially reduced by simple local alterations. Unlike commonly studied metrics for gerrymandering such as proportionality and compactness, our framework is inspired by the legal context for voting rights in the United States. We demonstrate the use of our framework to analyze the congressional districting of the state of North Carolina in 2016. We identify a substantial number of geographically localized disenfranchised individuals, mostly Democrats in the central and north-eastern parts of the state. Our simulated annealing algorithm is able to generate a witness districting with a roughly 50% reduction in the number of disenfranchised individuals, suggesting that the 2016 districting was not predetermined by North Carolina\u201a\u00c4\u00f4s spatial structure.", "keywords": "'gerrymandering', 'fairness', 'audit', 'simulated annealing'", "ccs_concepts": "'Theory of computation _ Random search heuristics', 'Applied computing _ Law'", "author_names": "'Jerry Lin', 'Carolyn Chen', 'Marc Chmielewski', 'Samia Zaman', 'Brandon Fain'", "author_affiliations": "'Duke University', 'Duke University', 'Duke University', 'Duke University', 'Computer Science, Duke University'"}, {"link": "https://doi.org/10.1145/3531146.3533176", "title": "Brain Computer Interfaces and Human Rights: Brave new rights for a brave new world", "abstract": "Digital health applications include a wide range of wearable, implantable, injectable and ingestible digital medical devices. Many of these devices use machine learning algorithms to assist medical prognosis and decision-making. One of the most compelling digital medical device developments is brain-computer interfaces (BCIs) which entails the connecting of a person's brain to a computer, or to another device outside the human body. BCIs allow bidirectional communication and control between the human brain and the outside world by exporting brain data or altering brain activity. Although being marveled at for its clinical promises, this technological advancement also raises novel ethical, legal, social and technical implications (ELSTI). Debates in this regard centers around patient autonomy, equity, trustworthiness in healthcare, data protection and security, risks of dehumanization, the limitations of machine learning-based decision-making, and the influence that BCIs have on what it means to be human and human rights. Since the adoption of the Universal Declaration of Human Rights (UDHR) after World War II, the landscape that give rise to these human rights has evolved enormously. Human life and humans\u201a\u00c4\u00f4 role in society are being transformed and threatened by technologies that were never imagined at the time the UDHR was adopted. BCIs, in particular, harbor the greatest possibility of social and individual disruption through its capability to record, interpret, manipulate, or alter brain activity that may potentially alter what it means to be human and how we control humans in future. Cutting edge technological innovations that increasingly blur the lines between human and computer beg the rethinking and extension of existing human rights to remain relevant in a digitized world. In this paper sui generis human rights such as mental privacy, the right to identity or self, agency or free will and fair access to cognitive augmentation will be discussed and how a regulatory framework must be adapted to act as technology enablers, whilst ensuring fairness, accountability, and transparency in sociotechnical systems.", "keywords": "'brain computer interfaces', 'human rights', 'neurological privacy', 'autonomy', 'identity'", "ccs_concepts": NaN, "author_names": "'Marietjie Wilhelmina Maria Botes'", "author_affiliations": "'Computer Sciences, SnT Interdisciplinary Centre for Security Reliability and Trust'"}, {"link": "https://doi.org/10.1145/3531146.3533219", "title": "Characterizing Properties and Trade-offs of Centralized Delegation Mechanisms in Liquid Democracy", "abstract": "Liquid democracy is a form of transitive delegative democracy that has received a flurry of scholarly attention from the computer science community in recent years. In its simplest form, every agent starts with one vote and may have other votes assigned to them via delegation from other agents. They can choose to delegate all votes assigned to them to another agent or vote directly with all votes assigned to them. However, many proposed realizations of liquid democracy allow for agents to express their delegation/voting preferences in more complex ways (e.g., a ranked list of potential delegates) and employ a centralized delegation mechanism to compute the final vote tally. In doing so, centralized delegation mechanisms can make decisions that affect the outcome of a vote and where/whether agents are able to delegate their votes. Much of the analysis thus far has focused on the ability of these mechanisms to make a correct choice. We extend this analysis by introducing and formalizing other important properties of a centralized delegation mechanism in liquid democracy with respect to crucial features such as accountability, transparency, explainability, fairness, and user agency. In addition, we evaluate existing methods in terms of these properties, show how some prior work can be augmented to achieve desirable properties, prove impossibility results for achieving certain sets of properties simultaneously, and highlight directions for future work.", "keywords": "'liquid democracy', 'voting', 'computational social choice', 'fairness', 'accountability', 'transparency'", "ccs_concepts": "'Applied computing _ Voting / election technologies', 'Applied computing _ E-government', 'Theory of computation _ Algorithmic game theory and mechanism design'", "author_names": "'Brian Brubach', 'Audrey Ballarin', 'Heeba Nazeer'", "author_affiliations": "'Computer Science Department, Wellesley College', 'Wellesley College', 'Wellesley College'"}, {"link": "https://doi.org/10.1145/3531146.3533133", "title": "Disclosure by Design: Designing information disclosures to support meaningful transparency and accountability", "abstract": "There is a strong push for organisations to become more transparent and accountable for their undertakings. Towards this, various transparency regimes oblige organisations to disclose certain information to relevant stakeholders (individuals, regulators, etc). This information intends to empower and support the monitoring, oversight, scrutiny and challenge of organisational practices. Importantly, however, these disclosures are of limited benefit if they are not meaningful for their recipients. Yet, in practice, the disclosures of tech/data-driven organisations are often highly technical, fragmented, and therefore of limited utility to all but experts. This undermines a disclosure\u201a\u00c4\u00f4s effectiveness, works to disempower, and ultimately hinders broader transparency aims.  This paper argues for a paradigm shift towards reconceptualising disclosures as \u201a\u00c4\u00f2interfaces\u201a\u00c4\u00f4 \u201a\u00c4\u00ec designed for the needs, expectations and requirements of the recipients they serve to inform. In making this case, and to provide a practical way forward, we demonstrate Document Engineering as one potential methodology for specifying, designing, and deploying more effective information disclosures. Focusing on data protection disclosures, we illustrate and explore how designing disclosures as interfaces can better support greater oversight of organisational data and practices, and thus better align with broader transparency and accountability aims.", "keywords": "'transparency', 'accountability', 'GDPR', 'document engineering', 'interfaces', 'data rights', 'usability'", "ccs_concepts": "'Security and privacy _ Human and societal aspects of security and privacy', 'Human-centered computing', 'Human-centered computing _ Interaction design'", "author_names": "'Chris Norval', 'Kristin Cornelius', 'Jennifer Cobbe', 'Jatinder Singh'", "author_affiliations": "'Compliant & Accountable Systems Group, University of Cambridge', 'Informatics Department, UCLA, Thousand Oaks, California, United States', 'Compliant & Accountable Systems Group, University of Cambridge', 'Compliant & Accountable Systems Group, University of Cambridge'"}, {"link": "https://doi.org/10.1145/3531146.3533112", "title": "Equi-explanation Maps: Concise and Informative Global Summary Explanations", "abstract": "We attempt to summarize the model logic of a black-box classification model in order to generate concise and informative global explanations. We propose equi-explanation maps, a new explanation data-structure that presents the region of interest as a union of equi-explanation subspaces along with their explanation vectors. We then propose E-Map, a method to generate equi-explanation maps. We demonstrate the broad utility of our approach by generating equi-explanation maps for various binary classification models (Logistic Regression, SVM, MLP, and XGBoost) on the UCI Heart disease dataset and the Pima Indians diabetes dataset. Each subspace in our generated map is the union of d-dimensional hyper-cuboids which can be compactly represented for the sake of interpretability. For each of these subspaces, we present linear explanations assigning a weight to each explanation feature. We justify the use of equi-explanation maps in comparison to other global explanation methods by evaluating in terms of interpretability, fidelity, and informativeness. A user study further corroborates the use of equi-explanation maps to generate compact and informative global explanations.", "keywords": "'explainability', 'subspace interpretability', 'global explanations', 'explaining classifiers', 'model-logic subspaces'", "ccs_concepts": "'Computer systems organization _ Embedded systems', 'Redundancy', 'Computer systems organization~Robotics', 'Networks~Network reliability'", "author_names": "'Tanya Chowdhury', 'Razieh Rahimi', 'James Allan'", "author_affiliations": "'CIIR, CICS, UMass Amherst', 'CIIR, CICS, UMass Amherst', 'CIIR, CICS, UMass Amherst'"}, {"link": "https://doi.org/10.1145/3531146.3533076", "title": "FAccT-Check on AI regulation: Systematic Evaluation of AI Regulation on the Example of the Legislation on the Use of AI in the Public Sector in the German Federal State of Schleswig-Holstein", "abstract": "In the framework of the current discussions about regulating Artificial Intelligence (AI) and machine learning (ML), the small Federal State of Schleswig-Holstein in Northern Germany hurries ahead and adopts legislation on the Use of AI in the public sector. The legislation aims on the one hand to enable the use of AI in the public sector by creating a legal framework and to limit its potential discriminatory effect on the other hand. Contrary to the European AI Act, which is valid for all companies and organizations in Europe, and contrary to the Chinese administrative rule on Internet information recommender systems, the Schleswig-Holstein \u201a\u00c4\u00faIT Deployment Law\u201a\u00c4\u00f9 (ITDL) would therefore only apply to public administrations and agencies in the federal state. The legislation addresses several AI risks, including fairness and transparency, and mitigates them with approaches quite different from the proposed European AI Act (AIA). In this paper, the legislation will be systematically reviewed and discussed with regards to its definition of AI, risk handling, fairness, accountability, and transparency.", "keywords": "'AI regulation', 'AI fairness', 'AI transparency'", "ccs_concepts": "'Computing methodologies _ Artificial intelligence'", "author_names": "'Katharina Simbeck'", "author_affiliations": "'HTW Berlin'"}, {"link": "https://doi.org/10.1145/3531146.3533074", "title": "Fairness Indicators for Systematic Assessments of Visual Feature Extractors", "abstract": "Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds.  Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world. We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models.  To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to \u201a\u00c4\u00faoff-the-shelf\u201a\u00c4\u00f9 models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size. The results of our fairness indicators on these systems suggest that blatant disparities still exist, which highlight the importance on the relationship between the context of the task and contents of a datasets. The code will be released to encourage the use of indicators.", "keywords": "'Fairness', 'Computer Vision', 'benchmarks', 'metrics'", "ccs_concepts": "'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Computer vision'", "author_names": "'Priya Goyal', 'Adriana Romero Soriano', 'Caner Hazirbas', 'Levent Sagun', 'Nicolas Usunier'", "author_affiliations": "'Meta', 'Meta', 'Meta', 'Meta', 'Meta'"}, {"link": "https://doi.org/10.1145/3531146.3533156", "title": "German AI Start-Ups and \u00d4\u00f8\u03a9AI Ethics\u00d4\u00f8\u03a9: Using A Social Practice Lens for Assessing and Implementing Socio-Technical Innovation", "abstract": "The current AI ethics discourse focuses on developing computational interpretations of ethical concerns, normative frameworks, and concepts for socio-technical innovation. There is less emphasis on understanding how AI practitioners themselves understand ethics and socially organize to operationalize ethical concerns. This is particularly true for AI start-ups, despite their significance as a conduit for the cultural production of innovation and progress, especially in the US and European context.  This gap in empirical research intensifies the risk of a disconnect between scholarly research, innovation and application. This risk materializes acutely as mounting pressures to identify and mitigate the potential harms of AI systems have created an urgent need to rapidly assess and implement socio-technical innovation focused on fairness, accountability, and transparency.  In this paper, we address this need. Building on social practice theory, we propose a framework that allows AI researchers, practitioners, and regulators to systematically analyze existing cultural understandings, histories, and social practices of \u201a\u00c4\u00faethical AI\u201a\u00c4\u00f9 to define appropriate strategies for effectively implementing socio-technical innovations. We argue that this approach is needed because socio-technical innovation \u201a\u00c4\u00fasticks\u201a\u00c4\u00f9 better if it sustains the cultural meaning of socially shared (ethical) AI practices, rather than breaking them. By doing so, it creates pathways for technical and socio-technical innovations to be integrated into already existing routines.  Against that backdrop, our contributions are threefold: (1) we introduce a practice-based approach for understanding \u201a\u00c4\u00faethical AI\u201a\u00c4\u00f9; (2) we present empirical findings from our study on the operationalization of \u201a\u00c4\u00faethics\u201a\u00c4\u00f9 in German AI start-ups to underline that AI ethics and social practices must be understood in their specific cultural and historical contexts; and (3) based on our empirical findings, suggest that \u201a\u00c4\u00faethical AI\u201a\u00c4\u00f9 practices can be broken down into principles, needs, narratives, materializations, and cultural genealogies to form a useful backdrop for considering socio-technical innovations. We conclude with critical reflections and practical implications of our work, as well as recommendations for future research.", "keywords": "'AI ethics', 'start-ups', 'social practice', 'organizations', 'fairness', 'accountability', 'transparency', 'innovation', 'regulation', 'socio-cultural history'", "ccs_concepts": "'Social and professional topics', 'Social and professional topics _ Codes of ethics', 'Socio-technical systems', 'Socio-technical systems'", "author_names": "'Mona Sloane', 'Janina Zakrzewski'", "author_affiliations": "'University of T\u221a\u00babingen, Germany and New York University', 'University of T\u221a\u00babingen'"}, {"link": "https://doi.org/10.1145/3531146.3533202", "title": "How Explainability Contributes to Trust in AI", "abstract": "We provide a philosophical explanation of the relation between artificial intelligence (AI) explainability and trust in AI, providing a case for expressions, such as \u201a\u00c4\u00faexplainability fosters trust in AI,\u201a\u00c4\u00f9 that commonly appear in the literature. This explanation relates the justification of the trustworthiness of an AI with the need to monitor it during its use. We discuss the latter by referencing an account of trust, called \u201a\u00c4\u00fatrust as anti-monitoring,\u201a\u00c4\u00f9 that different authors contributed developing. We focus our analysis on the case of medical AI systems, noting that our proposal is compatible with internalist and externalist justifications of trustworthiness of medical AI and recent accounts of warranted contractual trust. We propose that \u201a\u00c4\u00faexplainability fosters trust in AI\u201a\u00c4\u00f9 if and only if it fosters justified and warranted paradigmatic trust in AI, i.e., trust in the presence of the justified belief that the AI is trustworthy, which, in turn, causally contributes to rely on the AI in the absence of monitoring. We argue that our proposed approach can intercept the complexity of the interactions between physicians and medical AI systems in clinical practice, as it can distinguish between cases where humans hold different beliefs on the trustworthiness of the medical AI and exercise varying degrees of monitoring on them. Finally, we apply our account to user\u201a\u00c4\u00f4s trust in AI, where, we argue, explainability does not contribute to trust. By contrast, when considering public trust in AI as used by a human, we argue, it is possible for explainability to contribute to trust. Our account can explain the apparent paradox that in order to trust AI, we must trust AI users not to trust AI completely. Summing up, we can explain how explainability contributes to justified trust in AI, without leaving a reliabilist framework, but only by redefining the trusted entity as an AI-user dyad.", "keywords": "'artificial intelligence', 'explainable artificial intelligence', 'trust', 'healthcare', 'trustworthiness', 'ethics of artificial intelligence'", "ccs_concepts": "'Human-centered computing _ HCI theory', 'concepts and models', 'Applied computing _ Sociology', 'Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Artificial intelligence'", "author_names": "'Andrea Ferrario', 'Michele Loi'", "author_affiliations": "'ETH Zurich', 'Politecnico di Milano'"}, {"link": "https://doi.org/10.1145/3531146.3533108", "title": "Interactive Model Cards: A Human-Centered Approach to Model Documentation", "abstract": "Deep learning models for natural language processing (NLP) are increasingly adopted and deployed by analysts without formal training in NLP or machine learning (ML). However, the documentation intended to convey the model\u201a\u00c4\u00f4s details and appropriate use is tailored primarily to individuals with ML or NLP expertise. To address this gap, we conduct a design inquiry into interactive model cards, which augment traditionally static model cards with affordances for exploring model documentation and interacting with the models themselves. Our investigation consists of an initial conceptual study with experts in ML, NLP, and AI Ethics, followed by a separate evaluative study with non-expert analysts who use ML models in their work. Using a semi-structured interview format coupled with a think-aloud protocol, we collected feedback from a total of 30 participants who engaged with different versions of standard and interactive model cards. Through a thematic analysis of the collected data, we identified several conceptual dimensions that summarize the strengths and limitations of standard and interactive model cards, including: stakeholders; design; guidance; understandability & interpretability; sensemaking & skepticism; and trust & safety. Our findings demonstrate the importance of carefully considered design and interactivity for orienting and supporting non-expert analysts using deep learning models, along with a need for consideration of broader sociotechnical contexts and organizational dynamics. We have also identified design elements, such as language, visual cues, and warnings, among others, that support interactivity and make non-interactive content accessible. We summarize our findings as design guidelines and discuss their implications for a human-centered approach towards AI/ML documentation.", "keywords": "'model cards', 'human centered design', 'interactive data visualization'", "ccs_concepts": "'Computing methodologies _ Natural language processing', 'Human-centered computing _ Visualization', 'Human-centered computing _ Human computer interaction (HCI)', 'Interaction design process and methods'", "author_names": "'Anamaria Crisan', 'Margaret Drouhard', 'Jesse Vig', 'Nazneen Rajani'", "author_affiliations": "'Tableau Research', 'Tableau Software', 'Salesforce Research', 'Salesforce Research'"}, {"link": "https://doi.org/10.1145/3531146.3533090", "title": "It\u00d4\u00f8\u03a9s Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-off in Machine Learning for Public Policy", "abstract": "To achieve high accuracy in machine learning (ML) systems, practitioners often use complex \u201a\u00c4\u00fablack-box\u201a\u00c4\u00f9 models that are not easily understood by humans. The opacity of such models has resulted in public concerns about their use in high-stakes contexts and given rise to two conflicting arguments about the nature \u201a\u00c4\u00ee and even the existence \u201a\u00c4\u00ee of the accuracy-explainability trade-off. One side postulates that model accuracy and explainability are inversely related, leading practitioners to use black-box models when high accuracy is important. The other side of this argument holds that the accuracy-explainability trade-off is rarely observed in practice and consequently, that simpler interpretable models should always be preferred. Both sides of the argument operate under the assumption that some types of models, such as low-depth decision trees and linear regression are more explainable, while others such as neural networks and random forests, are inherently opaque.  Our main contribution is an empirical quantification of the trade-off between model accuracy and explainability in two real-world policy contexts. We quantify explainability in terms of how well a model is understood by a human-in-the-loop (HITL) using a combination of objectively measurable criteria, such as a human\u201a\u00c4\u00f4s ability to anticipate a model\u201a\u00c4\u00f4s output or identify the most important feature of a model, and subjective measures, such as a human\u201a\u00c4\u00f4s perceived understanding of the model. Our key finding is that explainability is not directly related to whether a model is a black-box or interpretable and is more nuanced than previously thought. We find that black-box models may be as explainable to a HITL as interpretable models and identify two possible reasons: (1) that there are weaknesses in the intrinsic explainability of interpretable models and (2) that more information about a model may confuse users, leading them to perform worse on objectively measurable explainability tasks. In summary, contrary to both positions in the literature, we neither observed a direct trade-off between accuracy and explainability nor found interpretable models to be superior in terms of explainability. It\u201a\u00c4\u00f4s just not that simple!", "keywords": "'machine learning', 'explainability', 'public policy', 'responsible AI'", "ccs_concepts": "'Human-centered computing _ Human computer interaction (HCI)', 'Computing methodologies _ Machine learning'", "author_names": "'Andrew Bell', 'Ian Solano-Kamaiko', 'Oded Nov', 'Julia Stoyanovich'", "author_affiliations": "'New York University', 'New York University', 'New York University', 'New York University'"}, {"link": "https://doi.org/10.1145/3531146.3533205", "title": "Justice in Misinformation Detection Systems: An Analysis of Algorithms, Stakeholders, and Potential Harms", "abstract": "Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.", "keywords": "'algorithmic fairness', 'justice', 'misinformation detection', 'machine learning', 'informational justice'", "ccs_concepts": NaN, "author_names": "'Terrence Neumann', 'Maria De-Arteaga', 'Sina Fazelpour'", "author_affiliations": "'University of Texas at Austin', 'University of Texas at Austin', 'Northeastern University'"}, {"link": "https://doi.org/10.1145/3531146.3534630", "title": "Keep Your Friends Close and Your Counterfactuals Closer: Improved Learning From Closest Rather Than Plausible Counterfactual Explanations in an Abstract Setting", "abstract": "Counterfactual explanations (CFEs) highlight changes to a model\u201a\u00c4\u00f4s input that alter its prediction in a particular way. s have gained considerable traction as a psychologically grounded solution for explainable artificial intelligence (XAI). Recent innovations introduce the notion of plausibility for automatically generated s, enhancing their robustness by exclusively creating plausible explanations. However, practical benefits of this constraint on user experience are yet unclear. In this study, we evaluate objective and subjective usability of plausible s in an iterative learning task. We rely on a game-like experimental design, revolving around an abstract scenario. Our results show that novice users benefit less from receiving plausible rather than closest s that induce minimal changes leading to the desired outcome. Responses in a post-game survey reveal no differences for subjective usability between both groups. Following the view of psychological plausibility as comparative similarity, users in the closest condition may experience their s as more psychologically plausible than the computationally plausible counterpart. In sum, our work highlights a little-considered divergence of definitions of computational plausibility and psychological plausibility, critically confirming the need to incorporate human behavior, preferences and mental models already at the design stages of XAI. All source code and data of the current study are available: https://github.com/ukuhl/PlausibleAlienZoo", "keywords": "'XAI', 'Counterfactual Explanations', 'Quantitative User Study', 'Algorithm Evaluation', 'Human Factors'", "ccs_concepts": NaN, "author_names": "'Ulrike Kuhl', 'Andr\u221a\u00a9 Artelt', 'Barbara Hammer'", "author_affiliations": "'HammerLab for Machine Learning, Bielefeld University', 'HammerLab for Machine Learning, Bielefeld University', 'HammerLab for Machine Learning, Bielefeld University'"}, {"link": "https://doi.org/10.1145/3531146.3533779", "title": "Limits and Possibilities for \u00d4\u00f8\u03a9Ethical AI\u00d4\u00f8\u03a9 in Open Source: A Study of Deepfakes", "abstract": "Open source software communities are a significant site of AI development, but \u201a\u00c4\u00faEthical AI\u201a\u00c4\u00f9 discourses largely focus on the problems that arise in software produced by private companies. Design, policy and tooling interventions to encourage \u201a\u00c4\u00faEthical AI\u201a\u00c4\u00f9 based on studies in private companies risk being ill-suited for an open source context, which operates under radically different organizational structures, cultural norms, and incentives.  In this paper, we show that significant and understudied harms and possibilities originate from differing practices of transparency and accountability in the open source community. We conducted an interview study of an AI-enabled open source Deepfake project to understand how members of that community reason about the ethics of their work. We found that notions of the \u201a\u00c4\u00faFreedom 0\u201a\u00c4\u00f9 to use code without any restriction, alongside beliefs about technology neutrality and technological inevitability, were central to how community members framed their responsibilities, and the actions they believed were and were not available to them. We propose a continuum between harms resulting from how a system is implemented versus how it is used, and show how commitments to radical transparency in open source allow great ethical scrutiny for harms wrought by implementation bugs, but allow harms through (mis)use to proliferate, requiring a deeper toolbox for disincentivizing harmful use. We discuss how an assumption of control over downstream uses is often implicit in discourses of \u201a\u00c4\u00faEthical AI\u201a\u00c4\u00f9, but outline alternative possibilities for action in cases such as open source where this assumption may not hold.", "keywords": "'deepfakes', 'ethics', 'open source', 'free software', 'agency', 'responsibility', 'interview'", "ccs_concepts": "'Human-centered computing _ Empirical studies in HCI', 'Social and professional topics _ Socio-technical systems', 'Security and privacy _ Social aspects of security and privacy'", "author_names": "'David Gray Widder', 'Dawn Nafus', 'Laura Dabbish', 'James Herbsleb'", "author_affiliations": "'School of Computer Science, Carnegie Mellon University', 'Intel Labs, Intel Corporation', 'School of Computer Science, Carnegie Mellon University', 'School of Computer Science, Carnegie Mellon University'"}, {"link": "https://doi.org/10.1145/3531146.3533235", "title": "Model Explanations with Differential Privacy", "abstract": "Using machine learning models in critical decision-making processes has given rise to a call for algorithmic transparency. Model explanations, however, might leak information about the sensitive data used to train and explain the model, undermining data privacy. We focus on black-box feature-based model explanations, which locally approximate the model around the point of interest, using potentially sensitive data. We design differentially private local approximation mechanisms, and evaluate their effect on explanation quality. To protect training data, we use existing differentially private learning algorithms. However, to protect the privacy of data which is used during the local approximation, we design an adaptive differentially private algorithm, which finds the minimal privacy budget required to produce accurate explanations. Both empirically and analytically, we evaluate the impact of the randomness needed in differential privacy algorithms on the fidelity of model explanations.", "keywords": "'Differential Privacy', 'Model Explainations'", "ccs_concepts": NaN, "author_names": "'Neel Patel', 'Reza Shokri', 'Yair Zick'", "author_affiliations": "'Viterbi School of engineering, University of Southern California', 'National University of Singapore', 'University of Massachusetts, Amherst'"}, {"link": "https://doi.org/10.1145/3531146.3533149", "title": "Model Multiplicity: Opportunities, Concerns, and Solutions", "abstract": "Recent scholarship has brought attention to the fact that there often exist multiple models for a given prediction task with equal accuracy that differ in their individual-level predictions or aggregate properties. This phenomenon\u201a\u00c4\u00eewhich we call model multiplicity\u201a\u00c4\u00eecan introduce a good deal of flexibility into the model selection process, creating a range of exciting opportunities. By demonstrating that there are many different ways of making equally accurate predictions, multiplicity gives model developers the freedom to prioritize other values in their model selection process without having to abandon their commitment to maximizing accuracy. However, multiplicity also brings to light a concerning truth: model selection on the basis of accuracy alone\u201a\u00c4\u00eethe default procedure in many deployment scenarios\u201a\u00c4\u00eefails to consider what might be meaningful differences between equally accurate models with respect to other criteria such as fairness, robustness, and interpretability. Unless these criteria are taken into account explicitly, developers might end up making unnecessary trade-offs or could even mask intentional discrimination. Furthermore, the prospect that there might exist another model of equal accuracy that flips a prediction for a particular individual may lead to a crisis in justifiability: why should an individual be subject to an adverse model outcome if there exists an equally accurate model that treats them more favorably? In this work, we investigate how to take advantage of the flexibility afforded by model multiplicity while addressing the concerns with justifiability that it might raise?", "keywords": "'Model multiplicity', 'predictive multiplicity', 'procedural multiplicity', 'fairness', 'discrimination', 'recourse', 'arbitrariness'", "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Machine learning', 'Theory of computation _ Machine learning theory', 'General and reference _ Evaluation', 'General and reference _ Performance'", "author_names": "'Emily Black', 'Manish Raghavan', 'Solon Barocas'", "author_affiliations": "'Carngie Mellon University', 'Harvard SEAS', 'Microsoft Research'"}, {"link": "https://doi.org/10.1145/3531146.3533224", "title": "NeuroView-RNN: It\u00d4\u00f8\u03a9s About Time", "abstract": "Recurrent Neural Networks (RNNs) are important tools for processing sequential data such as time-series or video. Interpretability is defined as the ability to be understood by a person and is different from explainability, which is the ability to be explained in a mathematical formulation. A key interpretability issue with RNNs is that it is not clear how each hidden state per time step contributes to the decision-making process in a quantitative manner. We propose NeuroView-RNN as a family of new RNN architectures that explains how all the time steps are used for the decision-making process. Each member of the family is derived from a standard RNN architecture by concatenation of the hidden steps into a global linear classifier. The global linear classifier has all the hidden states as the input, so the weights of the classifier have a linear mapping to the hidden states. Hence, from the weights, NeuroView-RNN can quantify how important each time step is to a particular decision. As a bonus, NeuroView-RNN also offers higher accuracy in many cases compared to the RNNs and their variants. We showcase the benefits of NeuroView-RNN by evaluating on a multitude of diverse time-series datasets.", "keywords": "'Recurrent neural networks', 'interpretability', 'time series'", "ccs_concepts": NaN, "author_names": "'Cj Barberan', 'Sina Alemmohammad', 'Naiming Liu', 'Randall Balestriero', 'Richard Baraniuk'", "author_affiliations": "'Rice University', 'Rice University', 'Rice University', 'Rice University', 'Rice University'"}, {"link": "https://doi.org/10.1145/3531146.3533077", "title": "News from Generative Artificial Intelligence Is Believed Less", "abstract": "Artificial Intelligence (AI) can generate text virtually indistinguishable from text written by humans. A key question, then, is whether people believe news headlines generated by AI as much as news headlines generated by humans. AI is viewed as lacking human motives and emotions, suggesting that people might view news written by AI as more accurate. By contrast, two pre-registered experiments on representative U.S. samples (N = 4,034) showed that people rated news headlines written by AI as less accurate than those written by humans. People were more likely to incorrectly rate news headlines written by AI (vs. a human) as inaccurate when they were actually true, and more likely to correctly rate them as inaccurate when they were indeed false. Our findings are important given the increasing adoption of AI in news generation, and the associated ethical and governance pressures to disclose it use and address standards of transparency and accountability.", "keywords": "'generative artificial intelligence', 'algorithmic transparency', 'fairness', 'news', 'news generation'", "ccs_concepts": "'Human-centered computing _ Empirical studies in HCI', 'Computing methodologies~Cognitive science', 'General and reference _ Empirical studies', 'Applied computing _ Psychology'", "author_names": "'Chiara Longoni', 'Andrey Fradkin', 'Luca Cian', 'Gordon Pennycook'", "author_affiliations": "'Boston University, Questrom School of Business', 'Boston University, Questrom School of Business', 'University of Virginia, Darden School of Business', 'Hill/Levene Schools of Business, University of Regina'"}, {"link": "https://doi.org/10.1145/3531146.3533232", "title": "On the Existence of Simpler Machine Learning Models", "abstract": "It is almost always easier to find an accurate-but-complex model than an accurate-yet-simple model. Finding optimal, sparse, accurate models of various forms (linear models with integer coefficients, decision sets, rule lists, decision trees) is generally NP-hard. We often do not know whether the search for a simpler model will be worthwhile, and thus we do not go to the trouble of searching for one. In this work, we ask an important practical question: can accurate-yet-simple models be proven to exist, or shown likely to exist, before explicitly searching for them? We hypothesize that there is an important reason that simple-yet-accurate models often do exist. This hypothesis is that the size of the Rashomon set is often large, where the Rashomon set is the set of almost-equally-accurate models from a function class. If the Rashomon set is large, it contains numerous accurate models, and perhaps at least one of them is the simple model we desire. In this work, we formally present the Rashomon ratio as a new gauge of simplicity for a learning problem, depending on a function class and a data set. The Rashomon ratio is the ratio of the volume of the set of accurate models to the volume of the hypothesis space, and it is different from standard complexity measures from statistical learning theory. Insight from studying the Rashomon ratio provides an easy way to check whether a simpler model might exist for a problem before finding it, namely whether several different machine learning methods achieve similar performance on the data. In that sense, the Rashomon ratio is a powerful tool for understanding why and when an accurate-yet-simple model might exist. If, as we hypothesize in this work, many real-world data sets admit large Rashomon sets, the implications are vast: it means that simple or interpretable models may often be used for high-stakes decisions without losing accuracy.", "keywords": "'Rashomon Set', 'Model Multiplicity', 'Simplicity', 'Generalization', 'Interpretable Machine Learning'", "ccs_concepts": "'Computing methodologies _ Supervised learning by classification', 'Computing methodologies _ Classification and regression trees', 'Human-centered computing _ Empirical studies in visualization'", "author_names": "'Lesia Semenova', 'Cynthia Rudin', 'Ronald Parr'", "author_affiliations": "'Duke University', 'Duke University', 'Duke University'"}, {"link": "https://doi.org/10.1145/3531146.3533099", "title": "Measuring Representational Harms in Image Captioning", "abstract": "Previous work has largely considered the fairness of image captioning systems through the underspecified lens of \u201a\u00c4\u00fabias.\u201a\u00c4\u00f9 In contrast, we present a set of techniques for measuring five types of representational harms, as well as the resulting measurements obtained for two of the most popular image captioning datasets using a state-of-the-art image captioning system. Our goal was not to audit this image captioning system, but rather to develop normatively grounded measurement techniques, in turn providing an opportunity to reflect on the many challenges involved. We propose multiple measurement techniques for each type of harm. We argue that by doing so, we are better able to capture the multi-faceted nature of each type of harm, in turn improving the (collective) validity of the resulting measurements. Throughout, we discuss the assumptions underlying our measurement approach and point out when they do not hold.", "keywords": "'fairness measurement', 'image captioning', 'harm propagation'", "ccs_concepts": "'Social and professional topics _ User characteristics', 'Computing methodologies _ Computer vision problems', 'Computing methodologies _ Natural language processing'", "author_names": "'Angelina Wang', 'Solon Barocas', 'Kristen Laird', 'Hanna Wallach'", "author_affiliations": "'Princeton University', 'Microsoft Research', 'Microsoft', 'Microsoft Research'"}, {"link": "https://doi.org/10.1145/3531146.3533153", "title": "Post-Hoc Explanations Fail to Achieve their Purpose in Adversarial Contexts", "abstract": "Existing and planned legislation stipulates various obligations to provide information about machine learning algorithms and their functioning, often interpreted as obligations to \u201a\u00c4\u00faexplain\u201a\u00c4\u00f9. Many researchers suggest using post-hoc explanation algorithms for this purpose. In this paper, we combine legal, philosophical and technical arguments to show that post-hoc explanation algorithms are unsuitable to achieve the law\u201a\u00c4\u00f4s objectives. Indeed, most situations where explanations are requested are adversarial, meaning that the explanation provider and receiver have opposing interests and incentives, so that the provider might manipulate the explanation for her own ends. We show that this fundamental conflict cannot be resolved because of the high degree of ambiguity of post-hoc explanations in realistic application scenarios. As a consequence, post-hoc explanation algorithms are unsuitable to achieve the transparency objectives inherent to the legal norms. Instead, there is a need to more explicitly discuss the objectives underlying \u201a\u00c4\u00faexplainability\u201a\u00c4\u00f9 obligations as these can often be better achieved through other mechanisms. There is an urgent need for a more open and honest discussion regarding the potential and limitations of post-hoc explanations in adversarial contexts, in particular in light of the current negotiations of the European Union\u201a\u00c4\u00f4s draft Artificial Intelligence Act.", "keywords": "'Explainability', 'Transparency', 'Regulation', 'Artificial Intelligence Act', 'GDPR', 'Counterfactual Explanations', 'SHAP', 'LIME'", "ccs_concepts": NaN, "author_names": "'Sebastian Bordt', 'Mich\u221a\u00aele Finck', 'Eric Raidl', 'Ulrike von Luxburg'", "author_affiliations": "'Department of Computer Science, University of T\u221a\u00babingen', 'Law Faculty, University of T\u221a\u00babingen', 'Ethics and Philosophy Lab, University of T\u221a\u00babingen', 'Department of Computer Science, University of T\u221a\u00babingen'"}, {"link": "https://doi.org/10.1145/3531146.3533151", "title": "Promoting Ethical Awareness in Communication Analysis: Investigating Potentials and Limits of Visual Analytics for Intelligence Applications", "abstract": "Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies.  Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science & Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts\u201a\u00c4\u00f4 understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations.", "keywords": "'Communication Analysis', 'Visual Analytics', 'Intelligence Analysis', 'Ethic Awareness', 'Science & Technology Studies', 'Critical Algorithm Studies', 'Critical Data Studies', 'Machine Learning', 'Interdisciplinary Research'", "ccs_concepts": "'Computing methodologies _ Visual analytics', 'Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Natural language processing'", "author_names": "'Maximilian T. Fischer', 'Simon David Hirsbrunner', 'Wolfgang Jentner', 'Matthias Miller', 'Daniel A. Keim', 'Paula Helm'", "author_affiliations": "'University of Konstanz', 'International Center for Ethics in the Sciences (IZEW), University of T\u221a\u00babingen', 'University of Konstanz', 'University of Konstanz', 'University of Konstanz', 'International Center for Ethics in the Sciences (IZEW), University of T\u221a\u00babingen'"}, {"link": "https://doi.org/10.1145/3531146.3533170", "title": "Rational Shapley Values", "abstract": "Explaining the predictions of opaque machine learning algorithms is an important and challenging task, especially as complex models are increasingly used to assist in high-stakes decisions such as those arising in healthcare and finance. Most popular tools for post-hoc explainable artificial intelligence (XAI) are either insensitive to context (e.g., feature attributions) or difficult to summarize (e.g., counterfactuals). In this paper, I introduce rational Shapley values, a novel XAI method that synthesizes and extends these seemingly incompatible approaches in a rigorous, flexible manner. I leverage tools from decision theory and causal modeling to formalize and implement a pragmatic approach that resolves a number of known challenges in XAI. By pairing the distribution of random variables with the appropriate reference class for a given explanation task, I illustrate through theory and experiments how user goals and knowledge can inform and constrain the solution set in an iterative fashion. The method compares favorably to state of the art XAI tools in a range of quantitative and qualitative comparisons.", "keywords": "'Explainable artificial intelligence', 'Interpretable machine learning', 'Shapley values', 'Counterfactuals', 'Decision theory'", "ccs_concepts": "'Theory of computation _ Machine learning theory', 'Mathematics of computing _ Probability and statistics'", "author_names": "'David Watson'", "author_affiliations": "'Department of Statistical Science, University College London'"}, {"link": "https://doi.org/10.1145/3531146.3533138", "title": "Robots Enact Malignant Stereotypes", "abstract": "Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV)\u00ac\u2020[18, 80], Natural Language Processing (NLP)\u00ac\u2020[6], or both, in the case of large image and caption models such as OpenAI CLIP\u00ac\u2020[14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called \u201a\u00c4\u00fafoundation models\u201a\u00c4\u00f9, e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Andrew Hundt', 'William Agnew', 'Vicky Zeng', 'Severin Kacianka', 'Matthew Gombolay'", "author_affiliations": "'School of Interactive Computing, Georgia Institute of Technology', 'University of Washington', 'Johns Hopkins University', 'Technical University of Munich', 'School of Interactive Computing, Georgia Institute of Technology'"}, {"link": "https://doi.org/10.1145/3531146.3533135", "title": "Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory", "abstract": "Understanding how ML models work is a prerequisite for responsibly designing, deploying, and using ML-based systems. With interpretability approaches, ML can now offer explanations for its outputs to aid human understanding. Though these approaches rely on guidelines for how humans explain things to each other, they ultimately solve for improving the artifact\u201a\u00c4\u00eean explanation. In this paper, we propose an alternate framework for interpretability grounded in Weick\u201a\u00c4\u00f4s sensemaking theory, which focuses on who the explanation is intended for. Recent work has advocated for the importance of understanding stakeholders\u201a\u00c4\u00f4 needs\u201a\u00c4\u00eewe build on this by providing concrete properties (e.g., identity, social context, environmental cues, etc.) that shape human understanding. We use an application of sensemaking in organizations as a template for discussing design guidelines for sensible AI, AI that factors in the nuances of human cognition when trying to explain itself.", "keywords": "'interpretability', 'explainability', 'sensemaking', 'organizations'", "ccs_concepts": "'Human-centered computing _ HCI theory', 'concepts and models', 'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Machine learning'", "author_names": "'Harmanpreet Kaur', 'Eytan Adar', 'Eric Gilbert', 'Cliff Lampe'", "author_affiliations": "'University of Michigan', 'University of Michigan', 'University of Michigan', 'University of Michigan'"}, {"link": "https://doi.org/10.1145/3531146.3533194", "title": "Confronting Power and Corporate Capture at the FAccT Conference", "abstract": "Fields such as medicine and public health attest to deep conflict of interest concerns present when private companies fund evaluation of their own products and services. We draw on these lessons to consider corporate capture of the ACM Fairness, Accountability, and Transparency (FAccT) conference. We situate our analysis within scholarship on the entanglement of industry and academia and focus on the silences it produces in the research record. Our analysis of the institutional design at FAccT indicates the conference\u201a\u00c4\u00f4s neglect of those people most negatively impacted by algorithmic systems. We focus on a 2021 paper by Wilson et al., \u201a\u00c4\u00faBuilding and auditing fair algorithms: A case study in candidate screening\u201a\u00c4\u00f9 as a key example of conflicted research accepted via peer review at FAccT. We call on the conference to (1) lead on models for how to manage conflicts of interest in the field of computing beyond individual disclosure of funding sources, (2) hold space for advocates and activists able to speak directly to questions of algorithmic harm, and (3) reconstitute the conference with attention to fostering agonistic dissensus\u201a\u00c4\u00eeun-making the present manufactured consensus and nurturing challenges to power. These changes will position our community to contend with the political dimensions of research on AI harms.", "keywords": "'pymetrics', 'conflict of interest', 'corporate capture', 'industry engagement', 'research funding', 'agonism'", "ccs_concepts": "'Social and professional topics _ Codes of ethics', 'Social and professional topics _ Funding', 'Political speech', 'Governmental regulations'", "author_names": "'Meg Young', 'Michael Katell', 'P.M. Krafft'", "author_affiliations": "'Cornell Tech', 'The Alan Turing Institute', 'University of the Arts London'"}, {"link": "https://doi.org/10.1145/3531146.3533186", "title": "The Algorithmic Imprint", "abstract": "When algorithmic harms emerge, a reasonable response is to stop using the algorithm to resolve concerns related to fairness, accountability, transparency, and ethics (FATE). However, just because an algorithm is removed does not imply its FATE-related issues cease to exist. In this paper, we introduce the notion of the \u201a\u00c4\u00faalgorithmic imprint\u201a\u00c4\u00f9 to illustrate how merely removing an algorithm does not necessarily undo or mitigate its consequences. We operationalize this concept and its implications through the 2020 events surrounding the algorithmic grading of the General Certificate of Education (GCE) Advanced (A) Level exams, an internationally recognized UK-based high school diploma exam administered in over 160 countries. While the algorithmic standardization was ultimately removed due to global protests, we show how the removal failed to undo the algorithmic imprint on the sociotechnical infrastructures that shape students\u201a\u00c4\u00f4, teachers\u201a\u00c4\u00f4, and parents\u201a\u00c4\u00f4 lives. These events provide a rare chance to analyze the state of the world both with and without algorithmic mediation. We situate our case study in Bangladesh to illustrate how algorithms made in the Global North disproportionately impact stakeholders in the Global South. Chronicling more than a year-long community engagement consisting of 47 interviews, we present the first coherent timeline of \u201a\u00c4\u00fawhat\u201a\u00c4\u00f9 happened in Bangladesh, contextualizing \u201a\u00c4\u00fawhy\u201a\u00c4\u00f9 and \u201a\u00c4\u00fahow\u201a\u00c4\u00f9 they happened through the lenses of the algorithmic imprint and situated algorithmic fairness. Analyzing these events, we highlight how the contours of the algorithmic imprints can be inferred at the infrastructural, social, and individual levels. We share conceptual and practical implications around how imprint-awareness can (a) broaden the boundaries of how we think about algorithmic impact, (b) inform how we design algorithms, and (c) guide us in AI governance. The imprint-aware design mindset can make the algorithmic development process more human-centered and sociotechnically-informed.", "keywords": "'Algorithmic Imprint', 'Algorithmic Impact Assessment', 'Situated Fairness', 'Infrastructure', 'Global South', 'Folk Theories of Algorithms', 'User Perceptions'", "ccs_concepts": "'Human-centered computing _ Collaborative and social computing'", "author_names": "'Upol Ehsan', 'Ranjit Singh', 'Jacob Metcalf', 'Mark Riedl'", "author_affiliations": "'Georgia Institute of Technology', 'AI on the Ground Initiative, Data & Society Research Institute', 'AI on the Ground Initiative, Data & Society Research Institute', 'Georgia Institute of Technology'"}, {"link": "https://doi.org/10.1145/3531146.3534628", "title": "The Conflict Between Explainable and Accountable Decision-Making Algorithms", "abstract": "Decision-making algorithms are being used in important decisions, such as who should be enrolled in health care programs and be hired. Even though these systems are currently deployed in high-stakes scenarios, many of them cannot explain their decisions. This limitation has prompted the Explainable Artificial Intelligence (XAI) initiative, which aims to make algorithms explainable to comply with legal requirements, promote trust, and maintain accountability. This paper questions whether and to what extent explainability can help solve the responsibility issues posed by autonomous AI systems. We suggest that XAI systems that provide post-hoc explanations could be seen as blameworthy agents, obscuring the responsibility of developers in the decision-making process. Furthermore, we argue that XAI could result in incorrect attributions of responsibility to vulnerable stakeholders, such as those who are subjected to algorithmic decisions (i.e., patients), due to a misguided perception that they have control over explainable algorithms. This conflict between explainability and accountability can be exacerbated if designers choose to use algorithms and patients as moral and legal scapegoats. We conclude with a set of recommendations for how to approach this tension in the socio-technical process of algorithmic decision-making and a defense of hard regulation to prevent designers from escaping responsibility.", "keywords": "'Responsibility', 'Accountability', 'Explainability', 'Artificial Intelligence', 'AI', 'Decision-Making', 'Algorithms', 'Blame', 'Designers', 'Patients', 'Users'", "ccs_concepts": "'Applied computing _ Law', 'Applied computing _ Psychology', 'Social and professional topics _ Governmental regulations', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence'", "author_names": "'Gabriel Lima', 'Nina Grgi\u0192\u00e1-Hla\u0192\u00e7a', 'Jin Keun Jeong', 'Meeyoung Cha'", "author_affiliations": "'School of Computing, KAIST, Republic of Korea and Data Science Group, Institute for Basic Science', 'Max Planck Institute for Software Systems, Germany and Max Planck Institute for Research on Collective Goods', 'School of Law, Kangwon National University', 'Data Science Group, Institute for Basic Science, Republic of Korea and School of Computing, KAIST'"}, {"link": "https://doi.org/10.1145/3531146.3533179", "title": "The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations", "abstract": "Machine learning models in safety-critical settings like healthcare are often \u201a\u00c4\u00fablackboxes\u201a\u00c4\u00f9: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.", "keywords": "'explainability', 'machine learning', 'fairness'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Human-centred computing _ explanations'", "author_names": "'Aparna Balagopalan', 'Haoran Zhang', 'Kimia Hamidieh', 'Thomas Hartvigsen', 'Frank Rudzicz', 'Marzyeh Ghassemi'", "author_affiliations": "'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute, Canada and Unity Health Toronto', 'Massachusetts Institute of Technology, USA and Vector Institute'"}, {"link": "https://doi.org/10.1145/3531146.3533118", "title": "Towards a multi-stakeholder value-based assessment framework for algorithmic systems", "abstract": "In an effort to regulate Machine Learning-driven (ML) systems, current auditing processes mostly focus on detecting harmful algorithmic biases. While these strategies have proven to be impactful, some values outlined in documents dealing with ethics in ML-driven systems are still underrepresented in auditing processes. Such unaddressed values mainly deal with contextual factors that cannot be easily quantified. In this paper, we develop a value-based assessment framework that is not limited to bias auditing and that covers prominent ethical principles for algorithmic systems. Our framework presents a circular arrangement of values with two bipolar dimensions that make common motivations and potential tensions explicit. In order to operationalize these high-level principles, values are then broken down into specific criteria and their manifestations. However, some of these value-specific criteria are mutually exclusive and require negotiation. As opposed to some other auditing frameworks that merely rely on ML researchers\u201a\u00c4\u00f4 and practitioners\u201a\u00c4\u00f4 input, we argue that it is necessary to include stakeholders that present diverse standpoints to systematically negotiate and consolidate value and criteria tensions. To that end, we map stakeholders with different insight needs, and assign tailored means for communicating value manifestations to them. We, therefore, contribute to current ML auditing practices with an assessment framework that visualizes closeness and tensions between values and we give guidelines on how to operationalize them, while opening up the evaluation and deliberation process to a wide range of stakeholders.", "keywords": "'values', 'ML development and deployment pipeline', 'algorithm assessment', 'multi-stakeholder'", "ccs_concepts": "'General and reference _ Evaluation', 'Human-centered computing _ Human computer interaction (HCI)', 'Social and professional topics _ User characteristics'", "author_names": "'Mireia Yurrita', 'Dave Murray-Rust', 'Agathe Balayn', 'Alessandro Bozzon'", "author_affiliations": "'Human Information Communication Design, TU Delft', 'Human Information Communication Design, TU Delft', 'Web Information Systems, TU Delft', 'Knowledge and Intelligence Design, TU Delft'"}, {"link": "https://doi.org/10.1145/3531146.3533182", "title": "Designing for Responsible Trust in AI Systems: A Communication Perspective", "abstract": "Current literature and public discourse on \u201a\u00c4\u00fatrust in AI\u201a\u00c4\u00f9 are often focused on the principles underlying trustworthy AI, with insufficient attention paid to how people develop trust. Given that AI systems differ in their level of trustworthiness, two open questions come to the fore: how should AI trustworthiness be responsibly communicated to ensure appropriate and equitable trust judgments by different users, and how can we protect users from deceptive attempts to earn their trust? We draw from communication theories and literature on trust in technologies to develop a conceptual model called MATCH, which describes how trustworthiness is communicated in AI systems through trustworthiness cues and how those cues are processed by people to make trust judgments. Besides AI-generated content, we highlight transparency and interaction as AI systems\u201a\u00c4\u00f4 affordances that present a wide range of trustworthiness cues to users. By bringing to light the variety of users\u201a\u00c4\u00f4 cognitive processes to make trust judgments and their potential limitations, we urge technology creators to make conscious decisions in choosing reliable trustworthiness cues for target users and, as an industry, to regulate this space and prevent malicious use. Towards these goals, we define the concepts of warranted trustworthiness cues and expensive trustworthiness cues, and propose a checklist of requirements to help technology creators identify appropriate cues to use. We present a hypothetical use case to illustrate how practitioners can use MATCH to design AI systems responsibly, and discuss future directions for research and industry efforts aimed at promoting responsible trust in AI.", "keywords": "'Trust in AI', 'human-AI interaction', 'human-centered AI', 'AI design'", "ccs_concepts": NaN, "author_names": "'Q.Vera Liao', 'S. Shyam Sundar'", "author_affiliations": "'Microsoft Research', 'Pennsylvania State University'"}, {"link": "https://doi.org/10.1145/3531146.3533132", "title": "Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Feminicide Counterdata Collection", "abstract": "Data ethics and fairness have emerged as important areas of research in recent years. However, much work in this area focuses on retroactively auditing and \u201a\u00c4\u00famitigating bias\u201a\u00c4\u00f9 in existing, potentially flawed systems, without interrogating the deeper structural inequalities underlying them. There are not yet examples of how to apply feminist and participatory methodologies from the start, to conceptualize and design machine learning-based tools that center and aim to challenge power inequalities. Our work targets this more prospective goal. Guided by the framework of data feminism, we co-design datasets and machine learning models to support the efforts of activists who collect and monitor data about feminicide\u00ac\u2020\u201a\u00c4\u00ee\u00ac\u2020gender-based killings of women and girls. We describe how intersectional feminist goals and participatory processes shaped each stage of our approach, from problem conceptualization to data collection to model evaluation. We highlight several methodological contributions, including 1) an iterative data collection and annotation process that targets model weaknesses and interrogates framing concepts (such as who is included/excluded in \u201a\u00c4\u00fafeminicide\u201a\u00c4\u00f9), 2) models that explicitly focus on intersectional identities rather than statistical majorities, and 3) a multi-step evaluation process\u00ac\u2020\u201a\u00c4\u00ee\u00ac\u2020with quantitative, qualitative and participatory steps\u00ac\u2020\u201a\u00c4\u00ee\u00ac\u2020focused on context-specific relevance. We also distill insights and tensions that arise from bridging intersectional feminist goals with ML. These include reflections on how ML may challenge power, embrace pluralism, rethink binaries and consider context, as well as the inherent limitations of any technology-based solution to address durable structural inequalities.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Harini Suresh', 'Rajiv Movva', 'Amelia Lee Dogan', 'Rahul Bhargava', 'Isadora Cruxen', 'Angeles Martinez Cuba', 'Guilia Taurino', 'Wonyoung So', \"Catherine D'Ignazio\"", "author_affiliations": "'Data + Feminism Lab, Massachusetts Institute of Technology, USA and CSAIL, Massachusetts Institute of Technology', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'School of Journalism, Northeastern University', 'School of Business and Management, Queen Mary University of London', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'Khoury College of Computer Sciences, Northeastern University', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'Data + Feminism Lab, Massachusetts Institute of Technology'"}, {"link": "https://doi.org/10.1145/3531146.3534638", "title": "Understanding and Being Understood: User Strategies for Identifying and Recovering From Mistranslations in Machine Translation-Mediated Chat", "abstract": "Machine translation (MT) is now widely and freely available, and has the potential to greatly improve cross-lingual communication. In order to use MT reliably and safely, end users must be able to assess the quality of system outputs and determine how much they can rely on them to guide their decisions and actions. However, it can be difficult for users to detect and recover from mistranslations due to limited language skills. In this work we collected 19 MT-mediated role-play conversations in housing and employment scenarios, and conducted in-depth interviews to understand how users identify and recover from translation errors. Participants communicated using four language pairs: English, and one of Spanish, Farsi, Igbo, or Tagalog. We conducted qualitative analysis to understand user challenges in light of limited system transparency, strategies for recovery, and the kinds of translation errors that proved more or less difficult for users to overcome. We found that users broadly lacked relevant and helpful information to guide their assessments of translation quality. Instances where a user erroneously thought they had understood a translation correctly were rare but held the potential for serious consequences in the real world. Finally, inaccurate and disfluent translations had social consequences for participants, because it was difficult to discern when a disfluent message was reflective of the other person\u201a\u00c4\u00f4s intentions, or an artifact of imperfect MT. We draw on theories of grounding and repair in communication to contextualize these findings, and propose design implications for explainable AI (XAI) researchers, MT researchers, as well as collaboration among them to support transparency and explainability in MT. These directions include handling typos and non-standard grammar common in interpersonal communication, making MT in interfaces more visible to help users evaluate errors, supporting collaborative repair of conversation breakdowns, and communicating model strengths and weaknesses to users.", "keywords": "'machine translation', 'human-AI interaction', 'computer-mediated communication', 'explainable machine learning'", "ccs_concepts": NaN, "author_names": "'Samantha Robertson', 'Mark D\u221a\u2260az'", "author_affiliations": "'University of California, Berkeley', 'Google'"}, {"link": "https://doi.org/10.1145/3531146.3533189", "title": "Why Am I Not Seeing It? Understanding Users\u00d4\u00f8\u03a9 Needs for Counterfactual Explanations in Everyday Recommendations", "abstract": "Intelligent everyday applications typically rely on automated Recommender Systems (RS) to generate recommendations that help users make decisions among a large number of options. Due to the increasing complexity of RS and the lack of transparency in its algorithmic decision-making, researchers have recognized the need to support users with explanations. While many traditional Explainable AI methods fall short in disclosing the internal intricacy of recommender systems, counterfactual explanations provide many desirable explainable features by offering human-like explanations that contrast an existing recommendation with alternatives. However, there is a lack of empirical research in understanding users\u201a\u00c4\u00f4 needs of counterfactual explanations in their usage of everyday intelligent applications. In this paper, we investigate whether and when to provide counterfactual explanations to support people\u201a\u00c4\u00f4s decision-making with everyday recommendations through a question-driven approach. We conducted a preliminary survey study and an interview study to understand how existing explanations might be insufficient to support users and elicit the triggers that prompt them to ask why not questions and seek additional explanations. The findings reveal that the utility of decision is a primary factor that may affect their counterfactual information needs. We then conducted an online scenario-based survey to quantify the correlation between utility and explanation needs and found significant correlations between the measured variables.", "keywords": "'Explainable recommender system; Counterfactual explanations; User studies'", "ccs_concepts": "'Information systems _ Recommender systems'", "author_names": "'Ruoxi Shang', 'K. J. Kevin Feng', 'Chirag Shah'", "author_affiliations": "'University of Washington', 'University of Washington', 'University of Washington'"}, {"link": "https://doi.org/10.1145/3531146.3533213", "title": "Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem", "abstract": "Algorithmic audits (or \u201a\u00c4\u00f2AI audits\u201a\u00c4\u00f4) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.", "keywords": "'AI audit', 'algorithm audit', 'audit', 'ethical AI', 'AI bias', 'AI harm', 'AI policy', 'algorithmic accountability'", "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Human-centered computing'", "author_names": "'Sasha Costanza-Chock', 'Inioluwa Deborah Raji', 'Joy Buolamwini'", "author_affiliations": "'Algorithmic Justice League', 'Algorithmic Justice League', 'Algorithmic Justice League'"}, {"link": "https://doi.org/10.1145/3531146.3533237", "title": "How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India", "abstract": "Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a \u201a\u00c4\u00f2high-risk\u201a\u00c4\u00f4 AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the \u201a\u00c4\u00f2boon\u201a\u00c4\u00f4 of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.", "keywords": "'algorithmic accountability', 'algorithmic fairness', 'human-ai interaction', 'instant loans', 'socio-technical systems'", "ccs_concepts": "'Computer systems organization _ Embedded systems', 'Computer systems organization~Robotics', 'Networks~Network reliability'", "author_names": "'Divya Ramesh', 'Vaishnav Kameswaran', 'Ding Wang', 'Nithya Sambasivan'", "author_affiliations": "'Computer Science and Engineering, University of Michigan, Ann Arbor', 'School of Information, University of Michigan, Ann Arbor', 'Google Research', 'Unaffiliated'"}, {"link": "https://doi.org/10.1145/3531146.3533072", "title": "#FuckTheAlgorithm: algorithmic imaginaries and political resistance", "abstract": "This paper applies and extends the concept of algorithmic imaginaries in the context of political resistance to sociotechnical injustice. Focusing on the 2020 UK OfQual protests, the role of the \u201a\u00c4\u00f9fuck the algorithm\u201a\u00c4\u00f9 chant is examined as an imaginary of resistance to confront power in sociotechnical systems. The protest is analysed as a shift in algorithmic imaginaries amidst evolving uses of #FuckTheAlgorithm on social media as part of everyday practices of resistance.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Garfield Benjamin'", "author_affiliations": "'Sociology, Solent University'"}, {"link": "https://doi.org/10.1145/3531146.3533071", "title": "A Data-driven analysis of the interplay between Criminological theory and predictive policing algorithms", "abstract": "Previous studies have focused on the biases and feedback loops that occur in predictive policing algorithms. These studies show how systemically and institutionally biased data leads to these feedback loops when predictive policing algorithms are applied in real life. We take a step back, and show that the choice in algorithm can be embedded in a specific criminological theory, and that the choice of a model on its own even without biased data can create biased feedback loops. By synthesizing \u201a\u00c4\u00fahistorical\u201a\u00c4\u00f9 data, in which we control the relationships between crimes, location and time, we show that the current predictive policing algorithms create biased feedback loops even with completely random data. We then review the process of creation and deployment of these predictive systems, and highlight when good practices, such as fitting a model to data, \u201a\u00c4\u00fago bad\u201a\u00c4\u00f9 within the context of larger system development and deployment. Using best practices from previous work on assessing and mitigating the impact of new technologies, we highlight where the design of these algorithms has broken down. The study also found that multidisciplinary analysis of such systems is vital for uncovering these issues and shows that any study of equitable AI should involve a systematic and holistic analysis of their design rationalities.", "keywords": "'model design', 'impact of data on algorithms'", "ccs_concepts": "'Applied computing _ Law', 'social and behavioral sciences', 'Software and its engineering _ Designing software', 'Theory of computation _ Models of computation'", "author_names": "'Adriane Chapman', 'Philip Grylls', 'Pamela Ugwudike', 'David Gammack', 'Jacqui Ayling'", "author_affiliations": "'University of Southampton', 'University of Southampton', 'University of Southampton', 'University of Southampton', 'University of Southampton'"}, {"link": "https://doi.org/10.1145/3531146.3533211", "title": "ABCinML: Anticipatory Bias Correction in Machine Learning Applications", "abstract": "The idealization of a static machine-learned model, trained once and deployed forever, is not practical. As input distributions change over time, the model will not only lose accuracy, any constraints to reduce bias against a protected class may fail to work as intended. Thus, researchers have begun to explore ways to maintain algorithmic fairness over time. One line of work focuses on dynamic learning: retraining after each batch, and the other on robust learning which tries to make algorithms robust against all possible future changes. Dynamic learning seeks to reduce biases soon after they have occurred and robust learning often yields (overly) conservative models. We propose an anticipatory dynamic learning approach for correcting the algorithm to mitigate bias before it occurs. Specifically, we make use of anticipations regarding the relative distributions of population subgroups (e.g., relative ratios of male and female applicants) in the next cycle to identify the right parameters for an importance weighing fairness approach. Results from experiments over multiple real-world datasets suggest that this approach has promise for anticipatory bias correction.", "keywords": "'classification', 'fairness', 'algorithmic bias'", "ccs_concepts": "'Computing methodologies _ Batch learning'", "author_names": "'Abdulaziz A. Almuzaini', 'Chidansh A. Bhatt', 'David M. Pennock', 'Vivek K. Singh'", "author_affiliations": "'Rutgers University', 'IBM', 'Rutgers University', 'Rutgers University'"}, {"link": "https://doi.org/10.1145/3531146.3533136", "title": "Achieving Fairness via Post-Processing in Web-Scale Recommender Systems_", "abstract": "Building fair recommender systems is a challenging and crucial area of study due to its immense impact on society. We extended the definitions of two commonly accepted notions of fairness to recommender systems, namely equality of opportunity and equalized odds. These fairness measures ensure that equally \u201a\u00c4\u00faqualified\u201a\u00c4\u00f9 (or \u201a\u00c4\u00faunqualified\u201a\u00c4\u00f9) candidates are treated equally regardless of their protected attribute status (such as gender or race). We propose scalable methods for achieving equality of opportunity and equalized odds in rankings in the presence of position bias, which commonly plagues data generated from recommender systems. Our algorithms are model agnostic in the sense that they depend only on the final scores provided by a model, making them easily applicable to virtually all web-scale recommender systems. We conduct extensive simulations as well as real-world experiments to show the efficacy of our approach.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Preetam Nandy', 'Cyrus DiCiccio', 'Divya Venugopalan', 'Heloise Logan', 'Kinjal Basu', 'Noureddine El Karoui'", "author_affiliations": "'LinkedIn Corporation', 'While at LinkedIn Corporation', 'LinkedIn Corporation', 'LinkedIn Corporation', 'LinkedIn Corporation', 'While at LinkedIn Corporation'"}, {"link": "https://doi.org/10.1145/3531146.3533228", "title": "Adversarial Scrutiny of Evidentiary Statistical Software", "abstract": "The U.S. criminal legal system increasingly relies on software output to convict and incarcerate people. In a large number of cases each year, the government makes these consequential decisions based on evidence from statistical software\u201a\u00c4\u00eesuch as probabilistic genotyping, environmental audio detection and toolmark analysis tools\u201a\u00c4\u00eethat the defense counsel cannot fully cross-examine or scrutinize. This undermines the commitments of the adversarial criminal legal system, which relies on the defense\u201a\u00c4\u00f4s ability to probe and test the prosecution\u201a\u00c4\u00f4s case to safeguard individual rights.  Responding to this need to adversarially scrutinize output from such software, we propose robust adversarial testing as a framework to examine the validity of evidentiary statistical software. We define and operationalize this notion of robust adversarial testing for defense use by drawing on a large body of recent work in robust machine learning and algorithmic fairness. We demonstrate how this framework both standardizes the process for scrutinizing such tools and empowers defense lawyers to examine their validity for instances most relevant to the case at hand. We further discuss existing structural and institutional challenges within the U.S. criminal legal system which may create barriers for implementing this framework and close with a discussion on policy changes that could help address these concerns.", "keywords": "'evidentiary software', 'statistical software', 'adversarial scrutiny', 'black-box software', 'robust machine learning'", "ccs_concepts": "'Applied computing _ Law', 'Computing methodologies _ Machine learning', 'Software and its engineering'", "author_names": "'Rediet Abebe', 'Moritz Hardt', 'Angela Jin', 'John Miller', 'Ludwig Schmidt', 'Rebecca Wexler'", "author_affiliations": "'Department of Electrical Engineering and Computer Sciences, University of California, Berkeley', 'Max Planck Institute for Intelligent Systems', 'Department of Electrical Engineering and Computer Sciences, University of California, Berkeley', 'Department of Electrical Engineering and Computer Sciences, University of California, Berkeley', 'Paul G. Allen School of Computer Science, University of Washington', 'School of Law, University of California, Berkeley'"}, {"link": "https://doi.org/10.1145/3531146.3533115", "title": "Affirmative Algorithms: Relational Equality as Algorithmic Fairness", "abstract": "Many statistical fairness notions have been proposed for algorithmic decision-making systems, and especially public safety pretrial risk assessment (PSPRA) algorithms such as COMPAS. Most fairness notions equalize something between groups, whether it is false positive rates or accuracy. In fact, I demonstrate that most prominent notions have their basis in equalizing some form of accuracy. However, statistical fairness metrics often do not capture the substantive point of equality. I argue that equal accuracy is not only difficult to measure but also unsatisfactory for ensuring equal justice. In response, I introduce philosopher Elizabeth Anderson\u201a\u00c4\u00f4s theory of relational equality as a fruitful alternative framework: to relate as equals, people need access to certain basic capabilities. I show that relational equality requires Affirmative PSPRA algorithms that lower risk scores for Black defendants. This is because fairness based on relational equality means considering the impact of PSPRA algorithms\u201a\u00c4\u00f4 decisions on access to basic capabilities. This impact is racially asymmetric in an unjust society. I make three main contributions: (1) I illustrate the shortcomings of statistical fairness notions in their reliance on equalizing some form of accuracy; (2) I present the first comprehensive ethical defense of Affirmative PSPRA algorithms, based on fairness in terms of relational equality instead; and (3) I show that equalizing accuracy is neither sufficient nor necessary for fairness based on relational equality. Overall, this work serves narrowly as a reason to re-evaluate algorithmic fairness for PSPRA algorithms, and serves broadly as an example of how discussions of algorithmic fairness can benefit from egalitarian philosophical frameworks.", "keywords": "'fairness', 'algorithmic fairness', 'philosophy', 'relational equality', 'affirmative algorithms', 'criminal justice', 'pretrial risk assessments'", "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Artificial intelligence', 'Applied computing _ Law', 'social and behavioral sciences'", "author_names": "'Marilyn Zhang'", "author_affiliations": "'Stanford University'"}, {"link": "https://doi.org/10.1145/3531146.3533204", "title": "Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models", "abstract": "This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity\u201a\u00c4\u00eeappropriately accounting for relevant differences across individuals\u201a\u00c4\u00eewhich is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods\u201a\u00c4\u00eeas opposed to simpler models\u201a\u00c4\u00eeshapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Emily Black', 'Hadi Elzayn', 'Alexandra Chouldechova', 'Jacob Goldin', 'Daniel Ho'", "author_affiliations": "'Computer Science Dept., Carngie Mellon University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University', 'Stanford University'"}, {"link": "https://doi.org/10.1145/3531146.3533172", "title": "An Algorithmic Framework for Bias Bounties", "abstract": "We propose and analyze an algorithmic framework for \u201a\u00c4\u00fabias bounties\u201a\u00c4\u00f9 \u201a\u00c4\u00ee events in which external participants are invited to propose improvements to a trained model, akin to bug bounty events in software and security. Our framework allows participants to submit arbitrary subgroup improvements, which are then algorithmically incorporated into an updated model. Our algorithm has the property that there is no tension between overall and subgroup accuracies, nor between different subgroup accuracies, and it enjoys provable convergence to either the Bayes optimal model or a state in which no further improvements can be found by the participants. We provide formal analyses of our framework, experimental evaluation, and findings from a preliminary bias bounty event.1", "keywords": "'bias bounty', 'subgroup fairness', 'multigroup fairness'", "ccs_concepts": "'Theory of computation _ Machine learning theory'", "author_names": "'Ira Globus-Harris', 'Michael Kearns', 'Aaron Roth'", "author_affiliations": "'University of Pennsylvania/Amazon', 'University of Pennsylvania/Amazon', 'University of Pennsylvania/Amazon'"}, {"link": "https://doi.org/10.1145/3531146.3533102", "title": "An Outcome Test of Discrimination for Ranked Lists", "abstract": "This paper extends Becker [3]\u201a\u00c4\u00f4s outcome test of discrimination to settings where a (human or algorithmic) decision-maker produces a ranked list of candidates. Ranked lists are particularly relevant in the context of online platforms that produce search results or feeds, and also arise when human decisionmakers express ordinal preferences over a list of candidates. We show that non-discrimination implies a system of moment inequalities, which intuitively impose that one cannot permute the position of a lower-ranked candidate from one group with a higher-ranked candidate from a second group and systematically improve the objective. Moreover, we show that that these moment inequalities are the only testable implications of non-discrimination when the auditor observes only outcomes and group membership by rank. We show how to statistically test the implied inequalities, and validate our approach in an application using data from LinkedIn.1", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Jonathan Roth', 'Guillaume Saint-Jacques', 'YinYin Yu'", "author_affiliations": "'Brown University', 'Apple', 'LinkedIn'"}, {"link": "https://doi.org/10.1145/3531146.3533216", "title": "Assessing Annotator Identity Sensitivity via Item Response Theory: A Case Study in a Hate Speech Corpus", "abstract": "Content Warning: This paper contains content considered profane, hateful, and offensive.  Annotators, by labeling data samples, play an essential role in the production of machine learning datasets. Their role is increasingly prevalent for more complex tasks such as hate speech or disinformation classification, where labels may be particularly subjective, as evidenced by low inter-annotator agreement statistics. Annotators may exhibit observable differences in their labeling patterns when grouped by their self-reported demographic identities, such as race, gender, etc. We frame these patterns as annotator identity sensitivities, referring to an annotator\u201a\u00c4\u00f4s increased likelihood of assigning a particular label on a data sample, conditional on a self-reported identity group. We purposefully refrain from using the term annotator bias, which we argue is problematic terminology in such subjective scenarios. Since annotator identity sensitivities can play a role in the patterns learned by machine learning algorithms, quantifying and characterizing them is of paramount importance for fairness and accountability in machine learning. In this work, we utilize item response theory (IRT), a methodological approach developed for measurement theory, to quantify annotator identity sensitivity. IRT models can be constructed to incorporate diverse factors that influence a label on a specific data sample, such as the data sample itself, the annotator, and the labeling instrument\u201a\u00c4\u00f4s wording and response options. An IRT model captures the contributions of these facets to the label via a latent-variable probabilistic model, thereby allowing the direct quantification of annotator sensitivity. As a case study, we examine a hate speech corpus containing over 50,000 social media comments from Reddit, YouTube, and Twitter, rated by 10,000 annotators on 10 components of hate speech (e.g., sentiment, respect, violence, dehumanization, etc.). We leverage three different IRT techniques which are complementary in that they quantify sensitivity from different perspectives: separated measurements, annotator-level interactions, and group-level interactions. We use these techniques to assess whether an annotator\u201a\u00c4\u00f4s racial identity is associated with their ratings on comments that target different racial identities. We find that, after controlling for the estimated hatefulness of social media comments, annotators tended to be more sensitive when rating comments targeting a group they identify with. Specifically, annotators were more likely to rate comments targeting their own racial identity as possessing elements of hate speech. Our results identify a correspondence between annotator identity and the target identity of hate speech comments, and provide a set of tools that can assess annotator identity sensitivity in machine learning datasets at large.", "keywords": "'annotation', 'hate speech', 'annotator sensitivity', 'item response theory', 'differential rater functioning'", "ccs_concepts": "'Human-centered computing _ Empirical studies in collaborative and social computing', 'Human-centered computing _ Social media'", "author_names": "'Pratik S. Sachdeva', 'Renata Barreto', 'Claudia von Vacano', 'Chris J. Kennedy'", "author_affiliations": "'D-Lab, University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'Harvard Medical School'"}, {"link": "https://doi.org/10.1145/3531146.3533121", "title": "Best vs. All: Equity and Accuracy of Standardized Test Score Reporting", "abstract": "We study a game theoretic model of standardized testing for college admissions. Students are of two types; High and Low. There is a college that would like to admit the High type students. Students take a potentially costly standardized exam which provides a noisy signal of their type.  The students come from two populations, which are identical in talent (i.e. the type distribution is the same), but differ in their access to resources: the higher resourced population can at their option take the exam multiple times, whereas the lower resourced population can only take the exam once. We study two models of score reporting, which capture existing policies used by colleges. The first policy (sometimes known as \u201a\u00c4\u00fasuper-scoring\u201a\u00c4\u00f9) allows students to report the max of the scores they achieve. The other policy requires that all scores be reported.  We find in our model that requiring that all scores be reported results in superior outcomes in equilibrium, both from the perspective of the college (the admissions rule is more accurate), and from the perspective of equity across populations: a student\u201a\u00c4\u00f4s probability of admission is independent of their population, conditional on their type. In particular, the false positive rates and false negative rates are identical in this setting, across the highly and poorly resourced student populations. This is the case despite the fact that the more highly resourced students can\u201a\u00c4\u00eeat their option\u201a\u00c4\u00eeeither report a more accurate signal of their type, or pool with the lower resourced population under this policy. This represents an unusual situation in the algorithmic fairness literature where the goals of accuracy and equity are in alignment, and do not need to be traded off against one another.", "keywords": "'algorithm fairness', 'screen accuracy', 'score reporting policies'", "ccs_concepts": "'Social and professional topics _ User characteristics', 'Theory of computation _ Theory and algorithms for application domains'", "author_names": "'Mingzi Niu', 'Sampath Kannan', 'Aaron Roth', 'Rakesh Vohra'", "author_affiliations": "'Rice University', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania'"}, {"link": "https://doi.org/10.1145/3531146.3533160", "title": "Beyond Fairness: Reparative Algorithms to Address Historical Injustices of Housing Discrimination in the US", "abstract": "Fairness in Machine Learning (ML) has mostly focused on interrogating the fairness of a particular decision point with assumptions made that the people represented in the data have been fairly treated throughout history. However, fairness cannot be ultimately achieved if such assumptions are not valid. This is the case for mortgage lending discrimination in the US, which should be critically understood as the result of historically accumulated injustices that were enacted through public policies and private practices including redlining, racial covenants, exclusionary zoning, and predatory inclusion, among others. With the erroneous assumptions of historical fairness in ML, Black borrowers with low income and low wealth are considered as a given condition in a lending algorithm, thus rejecting loans to them would be considered a \u201a\u00c4\u00fafair\u201a\u00c4\u00f9 decision even though Black borrowers were historically excluded from homeownership and wealth creation. To emphasize such issues, we introduce case studies using contemporary mortgage lending data as well as historical census data in the US. First, we show that historical housing discrimination has differentiated each racial group\u201a\u00c4\u00f4s baseline wealth which is a critical input for algorithmically determining mortgage loans. The second case study estimates the cost of housing reparations in the algorithmic lending context to redress historical harms because of such discriminatory housing policies. Through these case studies, we envision what reparative algorithms would look like in the context of housing discrimination in the US. This work connects to emerging scholarship on how algorithmic systems can contribute to redressing past harms through engaging with reparations policies and programs.", "keywords": "'fairness', 'mortgage lending', 'housing', 'racial wealth gap', 'reparations'", "ccs_concepts": "'Theory of computation _ Design and analysis of algorithms', 'Applied computing _ Sociology'", "author_names": "'Wonyoung So', 'Pranay Lohia', 'Rakesh Pimplikar', 'A.E. Hosoi', \"Catherine D'Ignazio\"", "author_affiliations": "'Department of Urban Studies and Planning, Massachusetts Institute of Technology', 'Microsoft', 'IBM Research AI', 'Institute for Data, Systems, and Society, Massachusetts Institute of Technology', 'Department of Urban Studies and Planning, Massachusetts Institute of Technology'"}, {"link": "https://doi.org/10.1145/3531146.3533089", "title": "Bias in Automated Speaker Recognition", "abstract": "Automated speaker recognition uses data processing to identify speakers by their voice. Today, automated speaker recognition is deployed on billions of smart devices and in services such as call centres. Despite their wide-scale deployment and known sources of bias in related domains like face recognition and natural language processing, bias in automated speaker recognition has not been studied systematically. We present an in-depth empirical and analytical study of bias in the machine learning development workflow of speaker verification, a voice biometric and core task in automated speaker recognition. Drawing on an established framework for understanding sources of harm in machine learning, we show that bias exists at every development stage in the well-known VoxCeleb Speaker Recognition Challenge, including data generation, model building, and implementation. Most affected are female speakers and non-US nationalities, who experience significant performance degradation. Leveraging the insights from our findings, we make practical recommendations for mitigating bias in automated speaker recognition, and outline future research directions.", "keywords": "'speaker recognition', 'speaker verification', 'bias', 'fairness', 'audit', 'evaluation'", "ccs_concepts": "'General and reference _ Evaluation', 'Security and privacy _ Biometrics', 'Computing methodologies _ Speech recognition', 'Computing methodologies~Machine learning'", "author_names": "'Wiebke Toussaint Hutiri', 'Aaron Yi Ding'", "author_affiliations": "'Technology, Policy & Management / Engineering Systems & Services / Cyber Physical Intelligence Lab, Delft University of Technology', 'Technology, Policy & Management/Engineering Systems & Services / Cyber Physical Intelligence Lab, Delft University of Technology'"}, {"link": "https://doi.org/10.1145/3531146.3533241", "title": "CounterFAccTual: How FAccT Undermines Its Organizing Principles", "abstract": "This essay joins recent scholarship in arguing that FAccT's fundamental framing of the potential to achieve the normative conditions for justice through bettering the design of algorithmic systems is counterproductive to achieving said justice in practice. Insofar as the FAccT community's research tends to prioritize design-stage interventions, it ignores the fact that the majority of the contextual factors that practically determine FAccT outcomes happen in the implementation and impact stages of AI/ML lifecycles. We analyze an emergent and widely-cited movement within the FAccT community for attempting to honor the centrality of contextual factors in shaping social outcomes, a set of strategies we term \u201a\u00c4\u00f2metadata maximalism\u201a\u00c4\u00f4. Symptomatic of design-centered approaches, metadata maximalism abstracts away its reliance on institutions and structures of justice that are, by every observable metric, already struggling (where not failing) to provide accessible, enforceable rights. These justice infrastructures, moreover, are currently wildly under-equipped to manage the disputes arising from digital transformation and machine learning. The political economy of AI/ML implementation provides further obstructions to realizing rights. Data and software supply chains, in tandem with intellectual property protections, introduce structural sources of opacity. Where duties of care to vulnerable persons should reign, profit incentives are given legal and regulatory primacy. Errors are inevitable and inextricable from the development of machine learning systems. In the face of these realities, FAccT programs, including metadata maximalism, tend to project their efforts in a fundamentally counter-factual universe: one in which functioning institutions and processes for due diligence in implementation and for redress of harms are working and ready to interoperate with. Unfortunately, in our world, these institutions and processes have been captured by the interests they are meant to hold accountable, intentionally hollowed-out, and/or were never designed to function in today's sociotechnical landscape. Continuing to produce (fair! accountable! transparent!) data-enabled systems that operate in high-impact areas, irrespective of this landscape's radically insufficient paths to justice, given the unavoidability of errors and/or intentional misuse in implementation, and the exhaustively-demonstrated disproportionate distribution of resulting harms onto already-marginalized communities, is a choice - a choice to be CounterFAccTual.", "keywords": "'metadata maximalism', 'algorithmic realism', 'failures of FAccT', 'accountability infrastructures'", "ccs_concepts": "'Applied computing _ Law', 'Applied computing _ Sociology', 'General and reference _ Evaluation', 'Software and its engineering _ Documentation', 'Software and its engineering _ System administration'", "author_names": "'Ben Gansky', 'Sean McDonald'", "author_affiliations": "'Human and Social Dimensions in Science and Technology, Arizona State University', 'Digital Public'"}, {"link": "https://doi.org/10.1145/3531146.3534644", "title": "Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems", "abstract": "Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.", "keywords": "'computer vision', 'fairness', 'algorithmic bias', 'AI ethics', 'violence detection', 'law enforcement technology'", "ccs_concepts": "'Computing methodologies', 'Machine learning', 'Social and professional topics_Computing / technology policy', 'Surveillance_Governmental surveillance'", "author_names": "'Ioannis Pastaltzidis', 'Nikolaos Dimitriou', 'Katherine Quezada-Tavarez', 'Stergios Aidinlis', 'Thomas Marquenie', 'Agata Gurzawska', 'Dimitrios Tzovaras'", "author_affiliations": "'Information Technologies Institute, Centre for Research and Technology Hellas', 'Information Technologies Institute, Centre for Research and Technology Hellas', 'Centre for IT and IP Law, KU Leuven', 'Trilateral Research', 'Centre for IT and IP Law, KU Leuven', 'Trilateral Research', 'Information Technologies Institute, Centre for Research and Technology Hellas'"}, {"link": "https://doi.org/10.1145/3531146.3533105", "title": "De-biasing \u00d4\u00f8\u03a9bias\u00d4\u00f8\u03a9 measurement", "abstract": "When a model\u201a\u00c4\u00f4s performance differs across socially or culturally relevant groups\u201a\u00c4\u00eclike race, gender, or the intersections of many such groups\u201a\u00c4\u00ecit is often called \u201a\u00c4\u00f9biased.\u201a\u00c4\u00f9 While much of the work in algorithmic fairness over the last several years has focused on developing various definitions of model fairness (the absence of group-wise model performance disparities) and eliminating such \u201a\u00c4\u00fabias,\u201a\u00c4\u00f9 much less work has gone into rigorously measuring it. In practice, it important to have high quality, human digestible measures of model performance disparities and associated uncertainty quantification about them that can serve as inputs into multi-faceted decision-making processes. In this paper, we show both mathematically and through simulation that many of the metrics used to measure group-wise model performance disparities are themselves statistically biased estimators of the underlying quantities they purport to represent. We argue that this can cause misleading conclusions about the relative group-wise model performance disparities along different dimensions, especially in cases where some sensitive variables consist of categories with few members. We propose the \u201a\u00c4\u00fadouble-corrected\u201a\u00c4\u00f9 variance estimator, which provides unbiased estimates and uncertainty quantification of the variance of model performance across groups. It is conceptually simple and easily implementable without statistical software package or numerical optimization. We demonstrate the utility of this approach through simulation and show on a real dataset that while statistically biased estimators of model group-wise model performance disparities indicate statistically significant between-group model performance disparities, when accounting for statistical bias in the estimator, the estimated group-wise disparities in model performance are no longer statistically significant.", "keywords": NaN, "ccs_concepts": "'Computing methodologies _ Machine learning', 'Computing methodologies _ Artificial intelligence'", "author_names": "'Kristian Lum', 'Yunfeng Zhang', 'Amanda Bower'", "author_affiliations": "'Twitter', 'Twitter', 'Twitter'"}, {"link": "https://doi.org/10.1145/3531146.3533226", "title": "Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness", "abstract": "Most proposed algorithmic fairness techniques require access to demographic data in order to make performance comparisons and standardizations across groups, however this data is largely unavailable in practice, hindering the widespread adoption of algorithmic fairness. Through this paper, we consider calls to collect more data on demographics to enable algorithmic fairness and challenge the notion that discrimination can be overcome with smart enough technical methods and sufficient data. We show how these techniques largely ignore broader questions of data governance and systemic oppression when categorizing individuals for the purpose of fairer algorithmic processing. In this work, we explore under what conditions demographic data should be collected and used to enable algorithmic fairness methods by characterizing a range of social risks to individuals and communities. For the risks to individuals we consider the unique privacy risks of sensitive attributes, the possible harms of miscategorization and misrepresentation, and the use of sensitive data beyond data subjects\u201a\u00c4\u00f4 expectations. Looking more broadly, the risks to entire groups and communities include the expansion of surveillance infrastructure in the name of fairness, misrepresenting and mischaracterizing what it means to be part of a demographic group, and ceding the ability to define what constitutes biased or unfair treatment. We argue that, by confronting these questions before and during the collection of demographic data, algorithmic fairness methods are more likely to actually mitigate harmful treatment disparities without reinforcing systems of oppression. Towards this end, we assess privacy-focused methods of data collection and use and participatory data governance structures as proposals for more responsibly collecting demographic data.", "keywords": "'demographic data', 'sensitive data', 'categorization', 'fairness', 'discrimination', 'identity', 'race', 'gender', 'sexuality', 'measurement'", "ccs_concepts": "'Security and privacy _ Social aspects of security and privacy', 'Privacy protections', 'Economics of security and privacy', 'Social and professional topics _ User characteristics', 'Gender', 'Sexual orientation'", "author_names": "'McKane Andrus', 'Sarah Villeneuve'", "author_affiliations": "'Partnership on AI', 'Partnership on AI'"}, {"link": "https://doi.org/10.1145/3531146.3533129", "title": "Don\u00d4\u00f8\u03a9t let Ricci v. DeStefano Hold You Back: A Bias-Aware Legal Solution to the Hiring Paradox", "abstract": "Companies that try to address inequality in employment face a hiring paradox. Failing to address workforce imbalance can result in legal sanctions and scrutiny, but proactive measures to address these issues might result in the same legal conflict. Recent run-ins of Microsoft and Wells Fargo with the Labor Department\u201a\u00c4\u00f4s Office of Federal Contract Compliance Programs (OFCCP) are not isolated and are likely to persist. To add to the confusion, existing scholarship on Ricci v. DeStefano often deems solutions to this paradox impossible. Circumventive practices such as the 4/5ths rule further illustrate tensions between too little action and too much action.  In this work, we give a powerful way to solve this hiring paradox that tracks both legal and algorithmic challenges. We unpack the nuances of Ricci v. DeStefano and extend the legal literature arguing that certain algorithmic approaches to employment are allowed by introducing the legal practice of banding to evaluate candidates. We thus show that a bias-aware technique can be used to diagnose and mitigate \u201a\u00c4\u00fabuilt-in\u201a\u00c4\u00f9 headwinds in the employment pipeline. We use the machinery of partially ordered sets to handle the presence of uncertainty in evaluations data. This approach allows us to move away from treating \u201a\u00c4\u00fapeople as numbers\u201a\u00c4\u00f9 to treating people as individuals\u201a\u00c4\u00eea property that is sought after by Title VII in the context of employment.", "keywords": "'anti-discrimination laws', 'hiring', 'resume screening', 'bias', 'uncertainty'", "ccs_concepts": "'Applied computing _ Law', 'Mathematics of computing _ Discrete mathematics'", "author_names": "'Jad Salem', 'Deven Desai', 'Swati Gupta'", "author_affiliations": "'Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology'"}, {"link": "https://doi.org/10.1145/3531146.3533199", "title": "Don\u00d4\u00f8\u03a9t Throw it Away! The Utility of Unlabeled Data in Fair Decision Making", "abstract": "Decision making algorithms, in practice, are often trained on data that exhibits a variety of biases. Decision-makers often aim to take decisions based on some ground-truth target that is assumed or expected to be unbiased, i.e., equally distributed across socially salient groups. In many practical settings, the ground-truth cannot be directly observed, and instead, we have to rely on a biased proxy measure of the ground-truth, i.e., biased labels, in the data. In addition, data is often selectively labeled, i.e., even the biased labels are only observed for a small fraction of the data that received a positive decision. To overcome label and selection biases, recent work proposes to learn stochastic, exploring decision policies via i) online training of new policies at each time-step and ii) enforcing fairness as a constraint on performance. However, the existing approach uses only labeled data, disregarding a large amount of unlabeled data, and thereby suffers from high instability and variance in the learned decision policies at different times. In this paper, we propose a novel method based on a variational autoencoder for practical fair decision-making. Our method learns an unbiased data representation leveraging both labeled and unlabeled data and uses the representations to learn a policy in an online process. Using synthetic data, we empirically validate that our method converges to the optimal (fair) policy according to the ground-truth with low variance. In real-world experiments, we further show that our training approach not only offers a more stable learning process but also yields policies with higher fairness as well as utility than previous approaches.", "keywords": "'fairness', 'decision making', 'label bias', 'selection bias', 'variational autoencoder', 'fair representation'", "ccs_concepts": "'Computing methodologies _ Machine learning algorithms', 'Computing methodologies _ Online learning settings', 'Social and professional topics'", "author_names": "'Miriam Rateike', 'Ayan Majumdar', 'Olga Mineeva', 'Krishna P. Gummadi', 'Isabel Valera'", "author_affiliations": "'Max Planck Institute of Intelligent Systems, T\u221a\u00babingen, Germany and Saarland University', 'Max Planck Institute for Software Systems, Germany and Saarland University', 'ETH Z\u221a\u00barich, Switzerland and Max Planck Institute for Intelligent Systems, T\u221a\u00babingen', 'Max Planck Institute for Software Systems', 'Saarland University, Germany and Max Planck Institute for Software Systems'"}, {"link": "https://doi.org/10.1145/3531146.3534645", "title": "Enforcing Group Fairness in Algorithmic Decision Making: Utility Maximization Under Sufficiency", "abstract": "Binary decision making classifiers are not fair by default. Fairness requirements are an additional element to the decision making rationale, which is typically driven by maximizing some utility function. In that sense, algorithmic fairness can be formulated as a constrained optimization problem. This paper contributes to the discussion on how to implement fairness, focusing on the fairness concepts of positive predictive value (PPV) parity, false omission rate (FOR) parity, and sufficiency (which combines the former two).  We show that group-specific threshold rules are optimal for PPV parity and FOR parity, similar to well-known results for other group fairness criteria. However, depending on the underlying population distributions and the utility function, we find that sometimes an upper-bound threshold rule for one group is optimal: utility maximization under PPV parity (or FOR parity) might thus lead to selecting the individuals with the smallest utility for one group, instead of selecting the most promising individuals. This result is counter-intuitive and in contrast to the analogous solutions for statistical parity and equality of opportunity.  We also provide a solution for the optimal decision rules satisfying the fairness constraint sufficiency. We show that more complex decision rules are required and that this leads to within-group unfairness for all but one of the groups. We illustrate our findings based on simulated and real data.", "keywords": "'algorithmic fairness', 'prediction-based decision making', 'constrained utility optimization', 'sufficiency', 'machine learning', 'group fairness metrics', 'fairness trade-offs'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Applied computing _ Decision analysis'", "author_names": "'Joachim Baumann', 'Anik\u221a\u2265 Hann\u221a\u00b0k', 'Christoph Heitz'", "author_affiliations": "'Universitiy of Zurich, Switzerland and Zurich University of Applied Sciences', 'Universitiy of Zurich', 'Zurich University of Applied Sciences'"}, {"link": "https://doi.org/10.1145/3531146.3533185", "title": "Evidence for Hypodescent in Visual Semantic AI", "abstract": "We examine the state-of-the-art multimodal \u201a\u00c4\u00f9visual semantic\u201a\u00c4\u00f9 model CLIP (\u201a\u00c4\u00f9Contrastive Language Image Pretraining\u201a\u00c4\u00f9) for the rule of hypodescent, or one-drop rule, whereby multiracial people are more likely to be assigned a racial or ethnic label corresponding to a minority or disadvantaged racial or ethnic group than to the equivalent majority or advantaged group. A face morphing experiment grounded in psychological research demonstrating hypodescent indicates that, at the midway point of 1,000 series of morphed images, CLIP associates 69.7% of Black-White female images with a Black text label over a White text label, and similarly prefers Latina (75.8%) and Asian (89.1%) text labels at the midway point for Latina-White female and Asian-White female morphs, reflecting hypodescent. Additionally, assessment of the underlying cosine similarities in the model reveals that association with White is correlated with association with \u201a\u00c4\u00f9person,\u201a\u00c4\u00f9 with Pearson\u201a\u00c4\u00f4s \u0153\u00c5 as high as 0.82, p < 10\u201a\u00e0\u00ed 90 over a 21,000-image morph series, indicating that a White person corresponds to the default representation of a person in CLIP. Finally, we show that the stereotype-congruent pleasantness association of an image correlates with association with the Black text label in CLIP, with Pearson\u201a\u00c4\u00f4s \u0153\u00c5 = 0.48, p < 10\u201a\u00e0\u00ed 90 for 21,000 Black-White multiracial male images, and \u0153\u00c5 = 0.41, p < 10\u201a\u00e0\u00ed 90 for Black-White multiracial female images. CLIP is trained on English-language text gathered using data collected from an American website (Wikipedia), and our findings demonstrate that CLIP embeds the values of American racial hierarchy, reflecting the implicit and explicit beliefs that are present in human minds. We contextualize these findings within the history of and psychology of hypodescent. Overall, the data suggests that AI supervised using natural language will, unless checked, learn biases that reflect racial hierarchies.", "keywords": "'multimodal', 'bias in AI', 'visual semantics', 'language-image models', 'racial bias', 'hypodescent'", "ccs_concepts": "'Computing methodologies _ Artificial intelligence'", "author_names": "'Robert Wolfe', 'Mahzarin R. Banaji', 'Aylin Caliskan'", "author_affiliations": "'University of Washington', 'Harvard University', 'University of Washington'"}, {"link": "https://doi.org/10.1145/3531146.3533113", "title": "Exploring How Machine Learning Practitioners (Try To) Use Fairness Toolkits", "abstract": "Recent years have seen the development of many open-source ML fairness toolkits aimed at helping ML practitioners assess and address unfairness in their systems. However, there has been little research investigating how ML practitioners actually use these toolkits in practice. In this paper, we conducted the first in-depth empirical exploration of how industry practitioners (try to) work with existing fairness toolkits. In particular, we conducted think-aloud interviews to understand how participants learn about and use fairness toolkits, and explored the generality of our findings through an anonymous online survey. We identified several opportunities for fairness toolkits to better address practitioner needs and scaffold them in using toolkits effectively and responsibly. Based on these findings, we highlight implications for the design of future open-source fairness toolkits that can support practitioners in better contextualizing, communicating, and collaborating around ML fairness efforts.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Wesley Hanwen Deng', 'Manish Nagireddy', 'Michelle Seng Ah Lee', 'Jatinder Singh', 'Zhiwei Steven Wu', 'Kenneth Holstein', 'Haiyi Zhu'", "author_affiliations": "'Human-Computer Interaction Institute, Carnegie Mellon University', 'Carnegie Mellon University', 'University of Cambridge', 'University of Cambridge', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University'"}, {"link": "https://doi.org/10.1145/3531146.3533144", "title": "Exploring the Role of Grammar and Word Choice in Bias Toward African American English (AAE) in Hate Speech Classification", "abstract": "Language usage on social media varies widely even within the context of American English. Despite this, the majority of natural language processing systems are trained only on \u201a\u00c4\u00faStandard American English,\u201a\u00c4\u00f9 or SAE, the construction of English most prominent among white Americans. For hate speech classification, prior work has shown that African American English (AAE) is more likely to be misclassified as hate speech. This has harmful implications for Black social media users as it reinforces and exacerbates existing notions of anti-Black racism. While past work has highlighted the relationship between AAE and hate speech classification, no work has explored the linguistic characteristics of AAE that lead to misclassification. Our work uses Twitter datasets for AAE dialect and hate speech classifiers to explore the fine-grained relationship between specific characteristics of AAE such as word choice and grammatical features and hate speech predictions. We further investigate these biases by removing profanity and examining the influence of four aspects of AAE grammar that are distinct from SAE. Results show that removing profanity accounts for a roughly 20 to 30% reduction in the percentage of samples classified as \u201a\u00c4\u00f4hate\u201a\u00c4\u00f4 \u201a\u00c4\u00f4abusive\u201a\u00c4\u00f4 or \u201a\u00c4\u00f4offensive,\u201a\u00c4\u00f4 and that similar classification patterns are observed regardless of grammar categories.", "keywords": "'Natural Language Processing', 'Linguistics', 'Fairness', 'African American English', 'Hate Speech', 'Social Media'", "ccs_concepts": "'Computing methodologies _ Natural language processing', 'Computing methodologies _ Information extraction', 'Computing methodologies _ Machine learning', 'Social and professional topics _ Race and ethnicity'", "author_names": "'Camille Harris', 'Matan Halevy', 'Ayanna Howard', 'Amy Bruckman', 'Diyi Yang'", "author_affiliations": "'Georgia Institute of Technology', 'Georgia Institute of Technology', 'The Ohio State University', 'Georgia Institute of Technology', 'Georgia Institute of Technology'"}, {"link": "https://doi.org/10.1145/3531146.3533167", "title": "FADE: FAir Double Ensemble Learning for Observable and Counterfactual Outcomes", "abstract": "Methods for building fair predictors often involve tradeoffs between fairness and accuracy and between different fairness criteria. Recent work seeks to characterize these tradeoffs in specific problem settings, but these methods often do not accommodate users who wish to improve the fairness of an existing benchmark model without sacrificing accuracy, or vice versa. These results are also typically restricted to observable accuracy and fairness criteria. We develop a flexible framework for fair ensemble learning that allows users to efficiently explore the fairness-accuracy space or to improve the fairness or accuracy of a benchmark model. Our framework can simultaneously target multiple observable or counterfactual fairness criteria, and it enables users to combine a large number of previously trained and newly trained predictors. We provide theoretical guarantees that our estimators converge at fast rates. We apply our method on both simulated and real data, with respect to both observable and counterfactual accuracy and fairness criteria. We show that, surprisingly, multiple unfairness measures can sometimes be minimized simultaneously with little impact on accuracy, relative to unconstrained predictors or existing benchmark models.", "keywords": "'fairness', 'counterfactual', 'ensemble learning', 'semiparametric'", "ccs_concepts": "'Computing methodologies _ Ensemble methods'", "author_names": "'Alan Mishler', 'Edward H. Kennedy'", "author_affiliations": "'JP Morgan AI Research', 'Department of Statistics & Data Science, Carnegie Mellon University'"}, {"link": "https://doi.org/10.1145/3531146.3533126", "title": "Fairness for AUC via Feature Augmentation", "abstract": "We study fairness in the context of classification where the performance is measured by the area under the curve (AUC) of the receiver operating characteristic. AUC is commonly used when both Type\u00ac\u2020I (false positive) and Type II (false negative) errors are important. However, the same classifier can have significantly varying AUCs for different protected groups and, in real-world applications, it is often desirable to reduce such cross-group differences. We address the problem of how to select additional features to most greatly improve AUC for the disadvantaged group. Our results establish that the unconditional variance of features does not inform us about AUC fairness but class-conditional variance does. Using this connection, we develop a novel approach, fairAUC, based on feature augmentation (adding features) to mitigate bias between identifiable groups. We evaluate fairAUC on synthetic and real-world (COMPAS) datasets and find that it significantly improves AUC for the disadvantaged group relative to benchmarks maximizing overall AUC and minimizing bias between groups.", "keywords": "'Classification', 'Area under the ROC curve (AUC)', 'feature augmentation', 'data collection and curation'", "ccs_concepts": "'Computing methodologies _ Supervised learning by classification'", "author_names": "'Hortense Fong', 'Vineet Kumar', 'Anay Mehrotra', 'Nisheeth K. Vishnoi'", "author_affiliations": "'Yale University', 'Yale University', 'Yale University', 'Yale University'"}, {"link": "https://doi.org/10.1145/3531146.3533074", "title": "Fairness Indicators for Systematic Assessments of Visual Feature Extractors", "abstract": "Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds.  Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world. We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models.  To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to \u201a\u00c4\u00faoff-the-shelf\u201a\u00c4\u00f9 models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size. The results of our fairness indicators on these systems suggest that blatant disparities still exist, which highlight the importance on the relationship between the context of the task and contents of a datasets. The code will be released to encourage the use of indicators.", "keywords": "'Fairness', 'Computer Vision', 'benchmarks', 'metrics'", "ccs_concepts": "'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Computer vision'", "author_names": "'Priya Goyal', 'Adriana Romero Soriano', 'Caner Hazirbas', 'Levent Sagun', 'Nicolas Usunier'", "author_affiliations": "'Meta', 'Meta', 'Meta', 'Meta', 'Meta'"}, {"link": "https://doi.org/10.1145/3531146.3533225", "title": "Fairness-aware Model-agnostic Positive and Unlabeled Learning", "abstract": "With the increasing application of machine learning in high-stake decision-making problems, potential algorithmic bias towards people from certain social groups poses negative impacts on individuals and our society at large. In the real-world scenario, many such problems involve positive and unlabeled data such as medical diagnosis, criminal risk assessment and recommender systems. For instance, in medical diagnosis, only the diagnosed diseases will be recorded (positive) while others will not (unlabeled). Despite the large amount of existing work on fairness-aware machine learning in the (semi-)supervised and unsupervised settings, the fairness issue is largely under-explored in the aforementioned Positive and Unlabeled Learning (PUL) context, where it is usually more severe. In this paper, to alleviate this tension, we propose a fairness-aware PUL method named FairPUL. In particular, for binary classification over individuals from two populations, we aim to achieve similar true positive rates and false positive rates in both populations as our fairness metric. Based on the analysis of the optimal fair classifier for PUL, we design a model-agnostic post-processing framework, leveraging both the positive examples and unlabeled ones. Our framework is proven to be statistically consistent in terms of both the classification error and the fairness metric. Experiments on the synthetic and real-world data sets demonstrate that our framework outperforms state-of-the-art in both PUL and fair classification.", "keywords": "'Fairness', 'Machine Learning', 'Positive and Unlabeled Learning'", "ccs_concepts": NaN, "author_names": "'Ziwei Wu', 'Jingrui He'", "author_affiliations": "'School of Information Sciences, University of Illinois at Urbana-Champaign', 'School of Information Sciences, University of Illinois at Urbana-Champaign'"}, {"link": "https://doi.org/10.1145/3531146.3533159", "title": "Female, white, 27? Bias Evaluation on Data and Algorithms for Affect Recognition in Faces", "abstract": "Nowadays, Artificial Intelligence (AI) algorithms show a strong performance for many use cases, making them desirable for real-world scenarios where the algorithms provide high-impact decisions. However, one major drawback of AI algorithms is their susceptibility to bias and resulting unfairness. This has a huge influence for their application, as they have a higher failure rate for certain subgroups. In this paper, we focus on the field of affective computing and particularly on the detection of bias for facial expressions. Depending on the deployment scenario, bias in facial expression models can have a disadvantageous impact and it is therefore essential to evaluate the bias and limitations of the model. In order to analyze the metadata distribution in affective computing datasets, we annotate several benchmark training datasets, containing both Action Units and categorical emotions, with age, gender, ethnicity, glasses, and beards. We show that there is a significantly skewed distribution, particularly for ethnicity and age. Based on this metadata annotation, we evaluate two trained state-of-the-art affective computing algorithms. Our evaluation shows that the strongest bias is in age, with the best performance for persons under 34 and a sharp decrease for older persons. Furthermore, we see an ethnicity bias with varying direction depending on the algorithm, a slight gender bias and worse performance for facial parts occluded by glasses.", "keywords": "'affective computing', 'action units', 'categorical emotions', 'metadata post-annotation', 'bias', 'fairness', 'data evaluation', 'algorithm evaluation'", "ccs_concepts": "'Computing methodologies _ Computer vision', 'Computing methodologies _ Machine learning', 'Social and professional topics _ Age', 'Social and professional topics _ Race and ethnicity', 'Social and professional topics~Gender'", "author_names": "'Jaspar Pahl', 'Ines Rieger', 'Anna M\u221a\u2202ller', 'Thomas Wittenberg', 'Ute Schmid'", "author_affiliations": "'Multimodal Human Sensing, Fraunhofer-Institute for Integrated Circuits IIS, Germany and Cognitive Systems Group, University of Bamberg', 'Fraunhofer-Institute for Integrated Circuits IIS, Germany and Cognitive Systems Group, University of Bamberg', 'Fraunhofer-Institute for Integrated Circuits IIS, Germany and Chair for Visual Computing, Friedrich-Alexander-Universit\u221a\u00a7t Erlangen-N\u221a\u00barnberg', 'Fraunhofer-Institute for Integrated Circuits IIS, Germany and Chair for Visual Computing, Friedrich-Alexander-Universit\u221a\u00a7t Erlangen-N\u221a\u00barnberg', 'Cognitive Systems Group, University of Bamberg, Germany and Project Group Comprehensible AI, Fraunhofer-Institute for Integrated Circuits IIS'"}, {"link": "https://doi.org/10.1145/3531146.3533104", "title": "Flipping the Script on Criminal Justice Risk Assessment: An actuarial model for assessing the risk the federal sentencing system poses to defendants", "abstract": "In the criminal justice system, algorithmic risk assessment instruments are used to predict the risk a defendant poses to society; examples include the risk of recidivating or the risk of failing to appear at future court dates. However, defendants are also at risk of harm from the criminal justice system. To date, there exists no risk assessment instrument that considers the risk the system poses to the individual. We develop a risk assessment instrument that \u201a\u00c4\u00faflips the script.\u201a\u00c4\u00f9 Using data about U.S. federal sentencing decisions, we build a risk assessment instrument that predicts the likelihood an individual will receive an especially lengthy sentence given factors that should be legally irrelevant to the sentencing decision. To do this, we develop a two-stage modeling approach. Our first-stage model is used to determine which sentences were \u201a\u00c4\u00faespecially lengthy.\u201a\u00c4\u00f9 We then use a second-stage model to predict the defendant\u201a\u00c4\u00f4s risk of receiving a sentence that is flagged as especially lengthy given factors that should be legally irrelevant. The factors that should be legally irrelevant include, for example, race, court location, and other socio-demographic information about the defendant. Our instrument achieves comparable predictive accuracy to risk assessment instruments used in pretrial and parole contexts. We discuss the limitations of our modeling approach and use the opportunity to highlight how traditional risk assessment instruments in various criminal justice settings also suffer from many of the same limitations and embedded value systems of their creators.", "keywords": "'risk assessment', 'criminal justice', 'heteroscedastic Bayesian additive regression trees', 'LASSO', 'two-stage model', 'perspective reversal'", "ccs_concepts": "'Applied computing _ Law'", "author_names": "'Mikaela Meyer', 'Aaron Horowitz', 'Erica Marshall', 'Kristian Lum'", "author_affiliations": "'Department of Statistics & Data Science and Heinz College, Carnegie Mellon University', 'American Civil Liberties Union', 'Idaho Justice Project', 'Department of Computer and Information Science, University of Pennsylvania'"}, {"link": "https://doi.org/10.1145/3531146.3533184", "title": "Gender and Racial Bias in Visual Question Answering Datasets", "abstract": "Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models. A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: e.g., questions about the color of a banana are answered with yellow, even if the banana in the image is green. If societal bias (e.g., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes. For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets. Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset collection process.", "keywords": "'visual question answering', 'gender stereotype', 'racial stereotype', 'datasets'", "ccs_concepts": "'Social and professional topics _ Gender', 'Social and professional topics _ Race and ethnicity', 'Computing methodologies _ Computer vision', 'Computing methodologies _ Natural language processing'", "author_names": "'Yusuke Hirota', 'Yuta Nakashima', 'Noa Garcia'", "author_affiliations": "'Osaka University', 'Osaka University', 'Osaka University'"}, {"link": "https://doi.org/10.1145/3531146.3533094", "title": "GetFair: Generalized Fairness Tuning of Classification Models", "abstract": "We present GetFair, a novel framework for tuning fairness of classification models. The fair classification problem deals with training models for a given classification task where data points have sensitive attributes. The goal of fair classification models is to not only generate accurate classification results but also to prevent discrimination against subpopulations (i.e., individuals with a specific value for the sensitive attribute). Existing methods for enhancing fairness of classification models, however, are often specifically designed for a particular fairness metric or a classifier model. They may also not be suitable for scenarios with incomplete training data or where optimizing for multiple fairness metrics is important. GetFair represents a general solution to this problem.  The GetFair approach works in the following way: First, a given classifier is trained on training data without any fairness objective. This is followed by a reinforcement learning inspired tuning procedure which updates the parameters of the learned model on a given fairness objective. This disentangles classifier training from fairness tuning, making our framework more general and allowing for the adoption of any parameterized classifier model. Because fairness metrics are designed as reward functions during tuning, GetFair generalizes across any fairness metric.  We demonstrate the generalizability of GetFair via evaluation over a benchmark suite of datasets, classification models, and fairness metrics. In addition, GetFair can also be deployed in settings where the training data is incomplete or the classifier needs to be tuned on multiple fairness metrics. GetFair not only contributes a flexible method to the repertoire of tools available to improve the fairness of classification models, it also seamlessly adapts to settings where existing fair classification methods may not be suitable or applicable.", "keywords": "'Fair classification', 'fairness metrics', 'sensitive attribute', 'classifier models'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Social and professional topics _ User characteristics'", "author_names": "'Sandipan Sikdar', 'Florian Lemmerich', 'Markus Strohmaier'", "author_affiliations": "'RWTH Aachen University', 'University of Passau', 'University of Mannheim, Germany and GESIS - Leibniz Institute for the Social Sciences'"}, {"link": "https://doi.org/10.1145/3531146.3533097", "title": "How Different Groups Prioritize Ethical Values for Responsible AI", "abstract": "Private companies, public sector organizations, and academic groups have outlined ethical values they consider important for responsible artificial intelligence technologies. While their recommendations converge on a set of central values, little is known about the values a more representative public would find important for the AI technologies they interact with and might be affected by. We conducted a survey examining how individuals perceive and prioritize responsible AI values across three groups: a representative sample of the US population (N=743), a sample of crowdworkers (N=755), and a sample of AI practitioners (N=175). Our results empirically confirm a common concern: AI practitioners\u201a\u00c4\u00f4 value priorities differ from those of the general public. Compared to the US-representative sample, AI practitioners appear to consider responsible AI values as less important and emphasize a different set of values. In contrast, self-identified women and black respondents found responsible AI values more important than other groups. Surprisingly, more liberal-leaning participants, rather than participants reporting experiences with discrimination, were more likely to prioritize fairness than other groups. Our findings highlight the importance of paying attention to who gets to define \u201a\u00c4\u00faresponsible AI.\u201a\u00c4\u00f9", "keywords": "'Responsible AI', 'value-sensitive design', 'empirical ethics'", "ccs_concepts": "'Human-centered computing _ Empirical studies in HCI', 'Social and professional topics _ Cultural characteristics', 'Social and professional topics _ Codes of ethics'", "author_names": "'Maurice Jakesch', 'Zana Bu\u221a\u00dfinca', 'Saleema Amershi', 'Alexandra Olteanu'", "author_affiliations": "'Cornell University', 'Harvard University', 'Microsoft Research', 'Microsoft Research'"}, {"link": "https://doi.org/10.1145/3531146.3533140", "title": "Imperfect Inferences: A Practical Assessment", "abstract": "Measuring racial disparities is challenging, especially when demographic labels are unavailable. Recently, some researchers and advocates have argued that companies should infer race and other demographic factors to help them understand and address discrimination. Others have been more skeptical, emphasizing the inaccuracy of racial inferences, critiquing the conceptualization of demographic categories themselves, and arguing that the use of demographic data might encourage algorithmic tweaks where more radical interventions are needed.  We conduct a novel empirical analysis that informs this debate, using a dataset of self-reported demographic information provided by users of the ride-hailing service Uber who consented to share this information for research purposes. As a threshold matter, we show how this data reflects the enduring power of racism in society. We find differences by race across a range of outcomes. For example, among self-reported African-American riders, we see racial differences on factors from iOS use to local pollution levels.  We then turn to a practical assessment of racial inference methodologies and offer two key findings. First, every inference method we tested has significant errors, miscategorizing people relative to their self-reports (even as the self-reports themselves suffer from selection bias). Second, and most importantly, we found that the inference methods worked: they reliably confirmed directional racial disparities that we knew were reflected in our dataset.  Our analysis also suggests that the choice of inference methods should be informed by the measurement task. For example, disparities that are geographic in nature might be best captured by inferences that rely on geography; discrimination based on a person\u201a\u00c4\u00f4s name might be best detected by inferences that rely on names.  In conclusion, our analysis shows that common racial inference methods have real and practical utility in shedding light on aggregate, directional disparities, despite their imperfections. While the recent literature has identified notable challenges regarding the collection and use of this data, these challenges should not be seen as dispositive.", "keywords": "'inference', 'demographics', 'race', 'discrimination', 'civil rights', 'fairness'", "ccs_concepts": "'Social and professional topics _ Race and ethnicity'", "author_names": "'Aaron Rieke', 'Vincent Southerland', 'Dan Svirsky', 'Mingwei Hsu'", "author_affiliations": "'Upturn', 'New York University School of Law', 'Uber Technologies, Inc', 'Upturn'"}, {"link": "https://doi.org/10.1145/3531146.3533245", "title": "Is calibration a fairness requirement?: An argument from the point of view of moral philosophy and decision theory", "abstract": "In this paper, we provide a moral analysis of two criteria of statistical fairness debated in the machine learning literature: 1) calibration between groups and 2) equality of false positive and false negative rates between groups. In our paper, we focus on moral arguments in support of either measure. The conflict between group calibration vs. false positive and false negative rate equality is one of the core issues in the debate about group fairness definitions among practitioners. For any thorough moral analysis, the meaning of the term \u201a\u00c4\u00fafairness\u201a\u00c4\u00f9 has to be made explicit and defined properly. For our paper, we equate fairness with (non-)discrimination, which is a legitimate understanding in the discussion about group fairness. More specifically, we equate it with \u201a\u00c4\u00faprima facie wrongful discrimination\u201a\u00c4\u00f9 in the sense this is used in Prof. Lippert-Rasmussen's treatment of this definition. In this paper, we argue that a violation of group calibration may be unfair in some cases, but not unfair in others. Our argument analyzes in great detail two specific hypothetical examples of usage of predictions in decision making. The most important practical implication is that between-group calibration is defensible as a bias standard in some cases but not others; we show this by referring to examples in which the violation of between-group calibration is discriminatory, and others in which it is not. This is in line with claims already advanced in the literature, that algorithmic fairness should be defined in a way that is sensitive to context. The most important practical implication is that arguments based on examples in which fairness requires between-group calibration, or equality in the false-positive/false-negative rates, do no generalize. For it may be that group calibration is a fairness requirement in one case, but not in another.", "keywords": "'calibration', 'fairness', 'equalized odds', 'opportunity', 'prediction'", "ccs_concepts": "'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence'", "author_names": "'Michele Loi', 'Christoph Heitz'", "author_affiliations": "'Department of Mathematics, Politecnico di Milano', 'Institute of Data Analysis and Process, Zurich University of Applied Sciences'"}, {"link": "https://doi.org/10.1145/3531146.3533205", "title": "Justice in Misinformation Detection Systems: An Analysis of Algorithms, Stakeholders, and Potential Harms", "abstract": "Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.", "keywords": "'algorithmic fairness', 'justice', 'misinformation detection', 'machine learning', 'informational justice'", "ccs_concepts": NaN, "author_names": "'Terrence Neumann', 'Maria De-Arteaga', 'Sina Fazelpour'", "author_affiliations": "'University of Texas at Austin', 'University of Texas at Austin', 'Northeastern University'"}, {"link": "https://doi.org/10.1145/3531146.3533117", "title": "Language variation and algorithmic bias: understanding algorithmic bias in British English automatic speech recognition", "abstract": "All language is characterised by variation which language users employ to construct complex social identities and express social meaning. Like other machine learning technologies, speech and language technologies (re)produce structural oppression when they perform worse for marginalised language communities. Using knowledge and theories from sociolinguistics, I explore why commercial automatic speech recognition systems and other language technologies perform significantly worse for already marginalised populations, such as second-language speakers and speakers of stigmatised varieties of English in the British Isles. Situating language technologies within the broader scholarship around algorithmic bias, consider the allocative and representational harms they can cause even (and perhaps especially) in systems which do not exhibit predictive bias, narrowly defined as differential performance between groups. This raises the question whether addressing or \u201a\u00c4\u00fafixing\u201a\u00c4\u00f9 this \u201a\u00c4\u00fabias\u201a\u00c4\u00f9 is actually always equivalent to mitigating the harms algorithmic systems can cause, in particular to marginalised communities.", "keywords": "'algorithmic bias', 'speech and language technologies', 'language variation', 'speech recognition'", "ccs_concepts": "'Computing methodologies _ Natural language processing', 'Computing methodologies _ Speech recognition'", "author_names": "'Nina Markl'", "author_affiliations": "'University of Edinburgh'"}, {"link": "https://doi.org/10.1145/3531146.3534646", "title": "Locality of Technical Objects and the Role of Structural Interventions for Systemic Change", "abstract": "Technical objects, like algorithms, exhibit causal capacities both in terms of their internal makeup and the position they occupy in relation to other objects and processes within a system. At the same time, systems encompassing technical objects interact with other systems themselves, producing a multi-scale structural composition. In the framework of fair artificial intelligence, typical causal inference interventions focus on the internal workings of technical objects (fairness constraints), and often forsake structural properties of the system. However, these interventions are often not sufficient to capture forms of discrimination and harm at a systemic level. To complement this approach we introduce the notion of locality and define structural interventions. We compare the effect of structural interventions on a system compared to local, structure-preserving interventions on technical objects. We focus on comparing interventions on generating mechanisms (representing social dynamics giving rise to discrimination) with constraining algorithms to satisfy some measure of fairness. This framework allows us to identify bias outside the algorithmic stage and propose joint interventions on social dynamics and algorithm design. We show how, for a model of financial lending, structural interventions can drive the system towards equality even when algorithmic interventions are unable to do so. This suggests that the responsibility of decision makers extends beyond ensuring that local fairness metrics are satisfied to an ecosystem that fosters equity for all.", "keywords": "'sociotechnical systems', 'ethics', 'fairness', 'system dynamics', 'interventions', 'lending'", "ccs_concepts": "'Social and professional topics _ User characteristics', 'Computing methodologies _ Causal reasoning and diagnostics', 'Human-centered computing', 'Applied computing _ Sociology'", "author_names": "'Efr\u221a\u00a9n Cruz Cort\u221a\u00a9s', 'Sarah Rajtmajer', 'Debashis Ghosh'", "author_affiliations": "'Michigan Institute of Data Science, Center for the Study of Complex Systems, University of Michigan', 'College of Information Sciences and Technology, Pennsylvania State University', 'Department of Biostatistics and Informatics, University of Colorado School of Public Health'"}, {"link": "https://doi.org/10.1145/3531146.3533183", "title": "Markedness in Visual Semantic AI", "abstract": "We evaluate the state-of-the-art multimodal \u201a\u00c4\u00f9visual semantic\u201a\u00c4\u00f9 model CLIP (\u201a\u00c4\u00f9Contrastive Language Image Pretraining\u201a\u00c4\u00f9) for biases related to the marking of age, gender, and race or ethnicity. Given the option to label an image as \u201a\u00c4\u00f9a photo of a person\u201a\u00c4\u00f9 or to select a label denoting race or ethnicity, CLIP chooses the \u201a\u00c4\u00f9person\u201a\u00c4\u00f9 label 47.9% of the time for White individuals, compared with 5.0% or less for individuals who are Black, East Asian, Southeast Asian, Indian, or Latino or Hispanic. The model is also more likely to rank the unmarked \u201a\u00c4\u00f9person\u201a\u00c4\u00f9 label higher than labels denoting gender for Male individuals (26.7% of the time) vs. Female individuals (15.2% of the time). Age also affects whether an individual is marked by the model: Female individuals under the age of 20 are more likely than Male individuals to be marked with a gender label, but less likely to be marked with an age label, while Female individuals over the age of 40 are more likely to be marked based on age than Male individuals. We trace our results back to the CLIP embedding space by examining the self-similarity (mean pairwise cosine similarity) for each social group, where higher self-similarity denotes greater attention directed by CLIP to the shared characteristics (i.e., age, race, or gender) of the social group. The results indicate that, as age increases, the self-similarity of representations of Female individuals increases at a higher rate than for Male individuals, with the disparity most pronounced at the \u201a\u00c4\u00f9more than 70\u201a\u00c4\u00f9 age range. Six of the ten least self-similar social groups are individuals who are White and Male, while all ten of the most self-similar social groups are individuals under the age of 10 or over the age of 70, and six of the ten are Female individuals. Our results yield evidence that bias in CLIP is intersectional: existing biases of self-similarity and markedness between Male and Female gender groups are further exacerbated when the groups compared are individuals who are White and Male and individuals who are Black and Female. CLIP is an English-language model trained on internet content gathered based on a query list generated from an American website (Wikipedia), and results indicate that CLIP reflects the biases of the language and society which produced this training data.", "keywords": "'multimodal', 'bias in AI', 'visual semantics', 'language-and-vision AI', 'markedness', 'age bias'", "ccs_concepts": "'Computing methodologies _ Artificial intelligence'", "author_names": "'Robert Wolfe', 'Aylin Caliskan'", "author_affiliations": "'University of Washington', 'University of Washington'"}, {"link": "https://doi.org/10.1145/3531146.3533236", "title": "Marrying Fairness and Explainability in Supervised Learning", "abstract": "Machine learning algorithms that aid human decision-making may inadvertently discriminate against certain protected groups. Therefore, we formalize direct discrimination as a direct causal effect of the protected attributes on the decisions, while induced discrimination as a change in the causal influence of non-protected features associated with the protected attributes. The measurements of marginal direct effect (MDE) and SHapley Additive exPlanations (SHAP) reveal that state-of-the-art fair learning methods can induce discrimination via association or reverse discrimination in synthetic and real-world datasets. To inhibit discrimination in algorithmic systems, we propose to nullify the influence of the protected attribute on the output of the system, while preserving the influence of remaining features. We introduce and study post-processing methods achieving such objectives, finding that they yield relatively high model accuracy, prevent direct discrimination, and diminishes various disparity measures, e.g., demographic disparity.", "keywords": "'machine learning', 'explainability', 'algorithmic fairness', 'discrimination', 'supervised learning'", "ccs_concepts": "'Computing methodologies _ Machine learning algorithms', 'Applied computing~Law', 'social and behavioral sciences', 'Computing methodologies _ Supervised learning'", "author_names": "'Przemyslaw A. Grabowicz', 'Nicholas Perello', 'Aarshee Mishra'", "author_affiliations": "'College of Information and Computer Sciences, University of Massachusetts Amherst', 'University of Massachusetts Amherst', 'University of Massachusetts Amherst'"}, {"link": "https://doi.org/10.1145/3531146.3534641", "title": "Measuring Fairness of Rankings under Noisy Sensitive Information", "abstract": "Metrics commonly used to assess group fairness in ranking require the knowledge of group membership labels (e.g., whether a job applicant is male or female). Obtaining accurate group membership labels, however, may be costly, operationally difficult, or even infeasible. Where it is not possible to obtain these labels, one common solution is to use proxy labels in their place, which are typically predicted by machine learning models. Proxy labels are susceptible to systematic biases, and using them for fairness estimation can thus lead to unreliable assessments. We investigate the problem of measuring group fairness in ranking for a suite of divergence-based metrics in the presence of proxy labels. We show that under certain assumptions, fairness of a ranking can reliably be measured from the proxy labels. We formalize two assumptions and provide a theoretical analysis for each showing how the true metric values can be derived from the estimates based on proxy labels. We prove that without such assumptions fairness assessment based on proxy labels is impossible. Through extensive experiments on both synthetic and real datasets, we demonstrate the effectiveness of our proposed methods for recovering reliable fairness assessments in rankings.", "keywords": NaN, "ccs_concepts": "'Information systems _ Information retrieval', 'Computing methodologies _ Machine learning'", "author_names": "'Azin Ghazimatin', 'Matthaus Kleindessner', 'Chris Russell', 'Ziawasch Abedjan', 'Jacek Golebiowski'", "author_affiliations": "'Spotify', 'Amazon Web Services', 'Amazon Web Services', 'Leibniz Universitat Hannover, Germany and Amazon Search', 'Amazon Search'"}, {"link": "https://doi.org/10.1145/3531146.3533081", "title": "Minimax Demographic Group Fairness in Federated Learning", "abstract": "Federated learning is an increasingly popular paradigm that enables a large number of entities to collaboratively learn better models. In this work, we study minimax group fairness in federated learning scenarios where different participating entities may only have access to a subset of the population groups during the training phase. We formally analyze how our proposed group fairness objective differs from existing federated learning fairness criteria that impose similar performance across participants instead of demographic groups. We provide an optimization algorithm \u201a\u00c4\u00ec FedMinMax \u201a\u00c4\u00ec for solving the proposed problem that provably enjoys the performance guarantees of centralized learning algorithms. We experimentally compare the proposed approach against other state-of-the-art methods in terms of group fairness in various federated learning setups, showing that our approach exhibits competitive or superior performance.", "keywords": "'Federated Learning', 'Algorithmic Fairness', 'Minimax Group Fairness'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Computing methodologies _ Cooperation and coordination'", "author_names": "'Afroditi Papadaki', 'Natalia Martinez', 'Martin Bertran', 'Guillermo Sapiro', 'Miguel Rodrigues'", "author_affiliations": "'University College London', 'Duke University', 'Duke University', 'Duke University, USA and Apple Inc.', 'University College London'"}, {"link": "https://doi.org/10.1145/3531146.3533149", "title": "Model Multiplicity: Opportunities, Concerns, and Solutions", "abstract": "Recent scholarship has brought attention to the fact that there often exist multiple models for a given prediction task with equal accuracy that differ in their individual-level predictions or aggregate properties. This phenomenon\u201a\u00c4\u00eewhich we call model multiplicity\u201a\u00c4\u00eecan introduce a good deal of flexibility into the model selection process, creating a range of exciting opportunities. By demonstrating that there are many different ways of making equally accurate predictions, multiplicity gives model developers the freedom to prioritize other values in their model selection process without having to abandon their commitment to maximizing accuracy. However, multiplicity also brings to light a concerning truth: model selection on the basis of accuracy alone\u201a\u00c4\u00eethe default procedure in many deployment scenarios\u201a\u00c4\u00eefails to consider what might be meaningful differences between equally accurate models with respect to other criteria such as fairness, robustness, and interpretability. Unless these criteria are taken into account explicitly, developers might end up making unnecessary trade-offs or could even mask intentional discrimination. Furthermore, the prospect that there might exist another model of equal accuracy that flips a prediction for a particular individual may lead to a crisis in justifiability: why should an individual be subject to an adverse model outcome if there exists an equally accurate model that treats them more favorably? In this work, we investigate how to take advantage of the flexibility afforded by model multiplicity while addressing the concerns with justifiability that it might raise?", "keywords": "'Model multiplicity', 'predictive multiplicity', 'procedural multiplicity', 'fairness', 'discrimination', 'recourse', 'arbitrariness'", "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Machine learning', 'Theory of computation _ Machine learning theory', 'General and reference _ Evaluation', 'General and reference _ Performance'", "author_names": "'Emily Black', 'Manish Raghavan', 'Solon Barocas'", "author_affiliations": "'Carngie Mellon University', 'Harvard SEAS', 'Microsoft Research'"}, {"link": "https://doi.org/10.1145/3531146.3533178", "title": "Multi Stage Screening: Enforcing Fairness and Maximizing Efficiency in a Pre-Existing Pipeline", "abstract": "Consider an actor making selection decisions (e.g., hiring) using a series of classifiers, which we term a sequential screening process. The early stages (e.g. resume screen, coding screen, phone interview) filter out some of the applicants, and in the final stage an expensive but accurate test (e.g. a full interview) is applied to those individuals that make it to the final stage. Since the final stage is expensive, if there are multiple groups with different fractions of positives in them at the penultimate stage (even if a slight gap), then the firm may naturally only choose to apply the final (interview) stage solely to the highest precision group which would be clearly unfair to the other groups. Even if the firm is required to interview all those who pass to the final round, the tests themselves could have the property that qualified individuals from some groups pass more easily than qualified individuals from others.  Accordingly, we consider requiring Equality of Opportunity (qualified members of each group have the same chance of reaching the final stage and being interviewed). We then examine the goal of maximizing quantities of interest to the decision maker subject to this constraint, via modification of the probabilities of promotion through the screening process at each stage based on performance at the previous stage.  We exhibit algorithms for satisfying Equal Opportunity over the selection process and maximizing precision (the fraction of interviews that yield qualified candidates) as well as linear combinations of precision and recall (recall determines the number of applicants needed per hire) at the end of the final stage. We also present examples showing that the solution space is non-convex, which motivate our combinatorial exact and (FPTAS) approximation algorithms for maximizing the linear combination of precision and recall. Finally, we discuss the \u201a\u00c4\u00f2price of\u201a\u00c4\u00f4 adding additional restrictions, such as not allowing the decision-maker to use group membership in its decision process.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Avrim Blum', 'Kevin Stangl', 'Ali Vakilian'", "author_affiliations": "'TTIC', 'TTIC', 'TTIC'"}, {"link": "https://doi.org/10.1145/3531146.3533154", "title": "Multi-disciplinary fairness considerations in machine learning for clinical trials", "abstract": "While interest in the application of machine learning to improve healthcare has grown tremendously in recent years, a number of barriers prevent deployment in medical practice. A notable concern is the potential to exacerbate entrenched biases and existing health disparities in society. The area of fairness in machine learning seeks to address these issues of equity; however, appropriate approaches are context-dependent, necessitating domain-specific consideration. We focus on clinical trials, i.e., research studies conducted on humans to evaluate medical treatments. Clinical trials are a relatively under-explored application in machine learning for healthcare, in part due to complex ethical, legal, and regulatory requirements and high costs. Our aim is to provide a multi-disciplinary assessment of how fairness for machine learning fits into the context of clinical trials research and practice. We start by reviewing the current ethical considerations and guidelines for clinical trials and examine their relationship with common definitions of fairness in machine learning. We examine potential sources of unfairness in clinical trials, providing concrete examples, and discuss the role machine learning might play in either mitigating potential biases or exacerbating them when applied without care. Particular focus is given to adaptive clinical trials, which may employ machine learning. Finally, we highlight concepts that require further investigation and development, and emphasize new approaches to fairness that may be relevant to the design of clinical trials.", "keywords": "'clinical trials', 'adaptive clinical trials', 'health informatics', 'machine learning for healthcare'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Applied computing _ Health informatics'", "author_names": "'Isabel Chien', 'Nina Deliu', 'Richard Turner', 'Adrian Weller', 'Sofia Villar', 'Niki Kilbertus'", "author_affiliations": "'Machine Learning Group, University of Cambridge', 'MRC Biostatistics Unit, University of Cambridge, United Kingdom and MEMOTEF Department, Sapienza University of Rome', 'Machine Learning Group, University of Cambridge', 'Machine Learning Group, University of Cambridge, United Kingdom and The Alan Turing Institute', 'MRC Biostatistics Unit, University of Cambridge', 'Technical University of Munich, Germany and Helmholtz Munich'"}, {"link": "https://doi.org/10.1145/3531146.3533166", "title": "Net benefit, calibration, threshold selection, and training objectives for algorithmic fairness in healthcare", "abstract": "A growing body of work uses the paradigm of algorithmic fairness to frame the development of techniques to anticipate and proactively mitigate the introduction or exacerbation of health inequities that may follow from the use of model-guided decision-making. We evaluate the interplay between measures of model performance, fairness, and the expected utility of decision-making to offer practical recommendations for the operationalization of algorithmic fairness principles for the development and evaluation of predictive models in healthcare. We conduct an empirical case-study via development of models to estimate the ten-year risk of atherosclerotic cardiovascular disease to inform statin initiation in accordance with clinical practice guidelines. We demonstrate that approaches that incorporate fairness considerations into the model training objective typically do not improve model performance or confer greater net benefit for any of the studied patient populations compared to the use of standard learning paradigms followed by threshold selection concordant with patient preferences, evidence of intervention effectiveness, and model calibration. These results hold when the measured outcomes are not subject to differential measurement error across patient populations and threshold selection is unconstrained, regardless of whether differences in model performance metrics, such as in true and false positive error rates, are present. In closing, we argue for focusing model development efforts on developing calibrated models that predict outcomes well for all patient populations while emphasizing that such efforts are complementary to transparent reporting, participatory design, and reasoning about the impact of model-informed interventions in context.", "keywords": "'healthcare', 'fairness', 'cardiovascular disease'", "ccs_concepts": "'Applied computing _ Health informatics'", "author_names": "'Stephen Pfohl', 'Yizhe Xu', 'Agata Foryciarz', 'Nikolaos Ignatiadis', 'Julian Genkins', 'Nigam Shah'", "author_affiliations": "'Stanford University, USA and Google', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University'"}, {"link": "https://doi.org/10.1145/3531146.3533152", "title": "On the Fairness of Machine-Assisted Human Decisions", "abstract": "When machine-learning algorithms are deployed in high-stakes decisions, we want to ensure that their deployment leads to fair and equitable outcomes. This concern has motivated a fast-growing literature that focuses on diagnosing and addressing disparities in machine predictions. However, many machine predictions are deployed to assist in decisions where a human decision-maker retains the ultimate decision authority. In this article, we therefore consider how properties of machine predictions affect the resulting human decisions. We show in a formal model that the inclusion of a biased human decision-maker can revert common relationships between the structure of the algorithm and the qualities of resulting decisions. Specifically, we document that excluding information about protected groups from the prediction may fail to reduce, and may even increase, ultimate disparities. While our concrete results rely on specific assumptions about the data, algorithm, and decision-maker, they show more broadly that any study of critical properties of complex decision systems, such as the fairness of machine-assisted human decisions, should go beyond focusing on the underlying algorithmic predictions in isolation.", "keywords": "'Protected Classes', 'Human Computer Interaction', 'Decision Support Systems', 'Fairness', 'Machine Learning'", "ccs_concepts": "'Human-centered computing _ HCI theory', 'concepts and models', 'Applied computing~Law', 'Applied computing~Economics'", "author_names": "'Bryce McLaughlin', 'Jann Spiess', 'Talia Gillis'", "author_affiliations": "'Stanford Graduate School of Business', 'Stanford Graduate School of Business', 'Columbia Law School'"}, {"link": "https://doi.org/10.1145/3531146.3533209", "title": "On the Power of Randomization in Fair Classification and Representation", "abstract": "Fair classification and fair representation learning are two important problems in supervised and unsupervised fair machine learning, respectively. Fair classification asks for a classifier that maximizes accuracy on a given data distribution subject to fairness constraints. Fair representation maps a given data distribution over the original feature space to a distribution over a new representation space such that all classifiers over the representation satisfy fairness. In this paper, we examine the power of randomization in both these problems to minimize the loss of accuracy that results when we impose fairness constraints. Previous work on fair classification has characterized the optimal fair classifiers on a given data distribution that maximize accuracy subject to fairness constraints, e.g., Demographic Parity (DP), Equal Opportunity (EO), and Predictive Equality (PE). We refine these characterizations to demonstrate when the optimal randomized fair classifiers can surpass their deterministic counterparts in accuracy. We also show how the optimal randomized fair classifier that we characterize can be obtained as a solution to a convex optimization problem. Recent work has provided techniques to construct fair representations for a given data distribution such that any classifier over this representation satisfies DP. However, the classifiers on these fair representations either come with no or weak accuracy guarantees when compared to the optimal fair classifier on the original data distribution. Extending our ideas for randomized fair classification, we improve on these works, and construct DP-fair, EO-fair, and PE-fair representations that have provably optimal accuracy and suffer no accuracy loss compared to the optimal DP-fair, EO-fair, and PE-fair classifiers respectively on the original data distribution.", "keywords": "'fairness; demographic parity; equal opportunity; randomization; machine learning; classification; representation'", "ccs_concepts": "'Theory of computation _ Unsupervised learning and clustering', 'Computing methodologies _ Supervised learning by classification', 'Learning latent representations'", "author_names": "'Sushant Agarwal', 'Amit Deshpande'", "author_affiliations": "'University of Waterloo', 'Microsoft Research'"}, {"link": "https://doi.org/10.1145/3531146.3533099", "title": "Measuring Representational Harms in Image Captioning", "abstract": "Previous work has largely considered the fairness of image captioning systems through the underspecified lens of \u201a\u00c4\u00fabias.\u201a\u00c4\u00f9 In contrast, we present a set of techniques for measuring five types of representational harms, as well as the resulting measurements obtained for two of the most popular image captioning datasets using a state-of-the-art image captioning system. Our goal was not to audit this image captioning system, but rather to develop normatively grounded measurement techniques, in turn providing an opportunity to reflect on the many challenges involved. We propose multiple measurement techniques for each type of harm. We argue that by doing so, we are better able to capture the multi-faceted nature of each type of harm, in turn improving the (collective) validity of the resulting measurements. Throughout, we discuss the assumptions underlying our measurement approach and point out when they do not hold.", "keywords": "'fairness measurement', 'image captioning', 'harm propagation'", "ccs_concepts": "'Social and professional topics _ User characteristics', 'Computing methodologies _ Computer vision problems', 'Computing methodologies _ Natural language processing'", "author_names": "'Angelina Wang', 'Solon Barocas', 'Kristen Laird', 'Hanna Wallach'", "author_affiliations": "'Princeton University', 'Microsoft Research', 'Microsoft', 'Microsoft Research'"}, {"link": "https://doi.org/10.1145/3531146.3534643", "title": "People are not coins: Morally distinct types of predictions necessitate different fairness constraints", "abstract": "In a recent paper [1], Brian Hedden has argued that most of the group fairness constraints discussed in the machine learning literature are not necessary conditions for the fairness of predictions, and hence that there are no genuine fairness metrics. This is proven by discussing a special case of a fair prediction. In our paper, we show that Hedden's argument does not hold for the most common kind of predictions used in data science, which are about people and based on data from similar people; we call these \u201a\u00c4\u00fahuman-group-based practices.\u201a\u00c4\u00f9 We argue that there is a morally salient distinction between human-group-based practices and those that are based on data of only one person, which we call \u201a\u00c4\u00fahuman-individual-based practices.\u201a\u00c4\u00f9 Thus, what may be a necessary condition for the fairness of human-group-based practices may not be a necessary condition for the fairness of human-individual-based practices, on which Hedden's argument is based. Accordingly, the group fairness metrics discussed in the machine learning literature may still be relevant for most applications of prediction-based decision making.", "keywords": "'fairness metrics', 'algorithmic decision making', 'fair prediction', 'moral principles'", "ccs_concepts": "'Computing methodologies _ Artificial intelligence _ Philosophical/theoretical foundations of artificial intelligence'", "author_names": "'Eleonora Vigan\u221a\u2264', 'Corinna Hertweck', 'Christoph Heitz', 'Michele Loi'", "author_affiliations": "'Institute of Biomedical Ethics and History of Medicine, University of Zurich', 'University of Zurich, Zurich University of Applied Sciences', 'Zurich University of Applied Sciences', 'Politecnico di Milano'"}, {"link": "https://doi.org/10.1145/3531146.3533155", "title": "Prediction as Extraction of Discretion", "abstract": "I argue that data-driven predictions work primarily as instruments for systematic extraction of discretionary power \u201a\u00c4\u00ec the practical capacity to make everyday decisions and define one's situation. This extractive relation reprises a long historical pattern, in which new methods of producing knowledge generate a redistribution of epistemic power: who declares what kind of truth about me, to count for what kinds of decisions? I argue that prediction as extraction of discretion is normal and fundamental to the technology, rather than isolated cases of bias or error. Synthesising critical observations across anthropology, history of technology and critical data studies, the paper demonstrates this dynamic in two contemporary domains: (1) crime and policing demonstrates how predictive systems are extractive by design. Rather than neutral models led astray by garbage data, pre-existing interests thoroughly shape how prediction conceives of its object, its measures, and most importantly, what it does not measure and in doing so devalues. (2) I then examine the prediction of productivity in the long tradition of extracting discretion as a means to extract labour power. Making human behaviour more predictable for the client of prediction (the manager, the corporation, the police officer) often means making life and work more unpredictable for the target of prediction (the employee, the applicant, the citizen).", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Sun-ha Hong'", "author_affiliations": "'Simon Fraser University'"}, {"link": "https://doi.org/10.1145/3531146.3533151", "title": "Promoting Ethical Awareness in Communication Analysis: Investigating Potentials and Limits of Visual Analytics for Intelligence Applications", "abstract": "Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies.  Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science & Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts\u201a\u00c4\u00f4 understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations.", "keywords": "'Communication Analysis', 'Visual Analytics', 'Intelligence Analysis', 'Ethic Awareness', 'Science & Technology Studies', 'Critical Algorithm Studies', 'Critical Data Studies', 'Machine Learning', 'Interdisciplinary Research'", "ccs_concepts": "'Computing methodologies _ Visual analytics', 'Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Natural language processing'", "author_names": "'Maximilian T. Fischer', 'Simon David Hirsbrunner', 'Wolfgang Jentner', 'Matthias Miller', 'Daniel A. Keim', 'Paula Helm'", "author_affiliations": "'University of Konstanz', 'International Center for Ethics in the Sciences (IZEW), University of T\u221a\u00babingen', 'University of Konstanz', 'University of Konstanz', 'University of Konstanz', 'International Center for Ethics in the Sciences (IZEW), University of T\u221a\u00babingen'"}, {"link": "https://doi.org/10.1145/3531146.3533079", "title": "Providing Item-side Individual Fairness for Deep Recommender Systems", "abstract": "Recent advent of deep learning techniques have reinforced the development of new recommender systems. Although these systems have been demonstrated as efficient and effective, the issue of item popularity bias in these recommender systems has raised serious concerns. While most of the existing works focus on group fairness at item side, individual fairness at item side is left largely unexplored. To address this issue, in this paper, first, we define a new notion of individual fairness from the perspective of items, namely (\u0152\u00b1, \u0152\u2264)-fairness, to deal with item popularity bias in recommendations. In particular, (\u0152\u00b1, \u0152\u2264)-fairness requires that similar items should receive similar coverage in the recommendations, where \u0152\u00b1 and \u0152\u2264 control item similarity and coverage similarity respectively, and both item and coverage similarity metrics are defined as task specific for deep recommender systems. Next, we design two bias mitigation methods, namely embedding-based re-ranking (ER) and greedy substitution (GS), for deep recommender systems. ER is an in-processing mitigation method that equips (\u0152\u00b1, \u0152\u2264)-fairness as a constraint to the objective function of the recommendation algorithm, while GS is a post-processing approach that accepts the biased recommendations as the input, and substitutes high-coverage items with low-coverage ones in the recommendations to satisfy (\u0152\u00b1, \u0152\u2264)-fairness. We evaluate the performance of both mitigation algorithms on two real-world datasets and a set of state-of-the-art deep recommender systems. Our results demonstrate that both ER and GS outperform the existing minimum-coverage (MC) mitigation solutions [Koutsopoulos and Halkidi 2018; Patro et\u00ac\u2020al. 2020] in terms of both fairness and accuracy of recommendations. Furthermore, ER delivers the best trade-off between fairness and recommendation accuracy among a set of alternative mitigation methods, including GS, the hybrid of ER and GS, and the existing MC solutions.", "keywords": "'Individual fairness', 'deep recommender systems', 'algorithmic fairness in machine learning'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Information systems _ Data analytics'", "author_names": "'Xiuling Wang', 'Wendy Hui Wang'", "author_affiliations": "'Stevens institute of technology', 'Stevens institute of technology'"}, {"link": "https://doi.org/10.1145/3531146.3533138", "title": "Robots Enact Malignant Stereotypes", "abstract": "Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV)\u00ac\u2020[18, 80], Natural Language Processing (NLP)\u00ac\u2020[6], or both, in the case of large image and caption models such as OpenAI CLIP\u00ac\u2020[14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called \u201a\u00c4\u00fafoundation models\u201a\u00c4\u00f9, e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Andrew Hundt', 'William Agnew', 'Vicky Zeng', 'Severin Kacianka', 'Matthew Gombolay'", "author_affiliations": "'School of Interactive Computing, Georgia Institute of Technology', 'University of Washington', 'Johns Hopkins University', 'Technical University of Munich', 'School of Interactive Computing, Georgia Institute of Technology'"}, {"link": "https://doi.org/10.1145/3531146.3533124", "title": "Selection in the Presence of Implicit Bias: The Advantage of Intersectional Constraints", "abstract": "In selection processes such as hiring, promotion, and college admissions, implicit bias toward socially-salient attributes such as race, gender, or sexual orientation of candidates is known to produce persistent inequality and reduce aggregate utility for the decision maker. Interventions such as the Rooney Rule and its generalizations, which require the decision maker to select at least a specified number of individuals from each affected group, have been proposed to mitigate the adverse effects of implicit bias in selection. Recent works have established that such lower-bound constraints can be very effective in improving aggregate utility in the case when each individual belongs to at most one affected group. However, in several settings, individuals may belong to multiple affected groups and, consequently, face more extreme implicit bias due to this intersectionality. We consider independently drawn utilities and show that, in the intersectional case, the aforementioned non-intersectional constraints can only recover part of the total utility achievable in the absence of implicit bias. On the other hand, we show that if one includes appropriate lower-bound constraints on the intersections, almost all the utility achievable in the absence of implicit bias can be recovered. Thus, intersectional constraints can offer a significant advantage over a reductionist dimension-by-dimension non-intersectional approach to reducing inequality.", "keywords": "'Implicit bias', 'selection', 'Intersectionality', 'Intersectional biases', 'Rooney Rule'", "ccs_concepts": NaN, "author_names": "'Anay Mehrotra', 'Bary S. R. Pradelski', 'Nisheeth K. Vishnoi'", "author_affiliations": "'Yale University', 'National Centre for Scientific Research (CNRS)', 'Yale University'"}, {"link": "https://doi.org/10.1145/3531146.3533175", "title": "Smallset Timelines: A Visual Representation of Data Preprocessing Decisions", "abstract": "Data preprocessing is a crucial stage in the data analysis pipeline, with both technical and social aspects to consider. Yet, the attention it receives is often lacking in research practice and dissemination. We present the Smallset Timeline, a visualisation to help reflect on and communicate data preprocessing decisions. A \u201a\u00c4\u00faSmallset\u201a\u00c4\u00f9 is a small selection of rows from the original dataset containing instances of dataset alterations. The Timeline is comprised of Smallset snapshots representing different points in the preprocessing stage and captions to describe the alterations visualised at each point. Edits, additions, and deletions to the dataset are highlighted with colour. We develop the R software package, smallsets, that can create Smallset Timelines from R and Python data preprocessing scripts. Constructing the figure asks practitioners to reflect on and revise decisions as necessary, while sharing it aims to make the process accessible to a diverse range of audiences. We present two case studies to illustrate use of the Smallset Timeline for visualising preprocessing decisions. Case studies include software defect data and income survey benchmark data, in which preprocessing affects levels of data loss and group fairness in prediction tasks, respectively. We envision Smallset Timelines as a go-to data provenance tool, enabling better documentation and communication of preprocessing tasks at large.", "keywords": "'data preprocessing', 'visualization', 'communication', 'open-source software', 'reflexivity'", "ccs_concepts": "'Human-centered computing _ Visualization toolkits'", "author_names": "'Lydia R. Lucchesi', 'Petra M. Kuhnert', 'Jenny L. Davis', 'Lexing Xie'", "author_affiliations": "\"Australian National University, Australia and CSIRO's Data61\", \"CSIRO's Data61, Australia and Australian National University\", 'Australian National University', \"Australian National University, Australia and CSIRO's Data61\""}, {"link": "https://doi.org/10.1145/3531146.3533095", "title": "Social Inclusion in Curated Contexts: Insights from Museum Practices", "abstract": "Artificial intelligence literature suggests that minority and fragile communities in society can be negatively impacted by machine learning algorithms due to inherent biases in the design process, which lead to socially exclusive decisions and policies. Faced with similar challenges in dealing with an increasingly diversified audience, the museum sector has seen changes in theory and practice, particularly in the areas of representation and meaning-making. While rarity and grandeur used to be at the centre stage of the early museum practices, folk life and museums\u201a\u00c4\u00f4 relationships with the diverse communities they serve become a widely integrated part of the contemporary practices. These changes address issues of diversity and accessibility in order to offer more socially inclusive services. Drawing on these changes and reflecting back on the AI world, we argue that the museum experience provides useful lessons for building AI with socially inclusive approaches, especially in situations in which both a collection and access to it will need to be curated or filtered, as frequently happens in search engines, recommender systems and digital libraries. We highlight three principles: (1) Instead of upholding the value of neutrality, practitioners are aware of the influences of their own backgrounds and those of others on their work. By not claiming to be neutral but practising cultural humility, the chances of addressing potential biases can be increased. (2) There should be room for situational interpretation beyond the stages of data collection and machine learning. Before applying models and predictions, the contexts in which relevant parties exist should be taken into account. (3) Community participation serves the needs of communities and has the added benefit of bringing practitioners and communities together.", "keywords": "'Curation', 'museums', 'libraries', 'social inclusion', 'diversity'", "ccs_concepts": "'General and reference _ Design', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Applied computing _ Arts and humanities'", "author_names": "'Han-Yin Huang', 'Cynthia C. S. Liem'", "author_affiliations": "'Multimedia Computing Group, Delft University of Technology', 'Multimedia Computing Group, Delft University of Technology'"}, {"link": "https://doi.org/10.1145/3531146.3533128", "title": "Subverting Fair Image Search with Generative Adversarial Perturbations", "abstract": "In this work we explore the intersection fairness and robustness in the context of ranking: when a ranking model has been calibrated to achieve some definition of fairness, is it possible for an external adversary to make the ranking model behave unfairly without having access to the model or training data? To investigate this question, we present a case study in which we develop and then attack a state-of-the-art, fairness-aware image search engine using images that have been maliciously modified using a Generative Adversarial Perturbation (GAP) model\u00ac\u2020[75]. These perturbations attempt to cause the fair re-ranking algorithm to unfairly boost the rank of images containing people from an adversary-selected subpopulation.  We present results from extensive experiments demonstrating that our attacks can successfully confer significant unfair advantage to people from the majority class relative to fairly-ranked baseline search results. We demonstrate that our attacks are robust across a number of variables, that they have close to zero impact on the relevance of search results, and that they succeed under a strict threat model. Our findings highlight the danger of deploying fair machine learning algorithms in-the-wild when (1) the data necessary to achieve fairness may be adversarially manipulated, and (2) the models themselves are not robust against attacks.", "keywords": "'Information Retrieval', 'Fair Ranking', 'Adversarial Machine Learning', 'Demographic Inference'", "ccs_concepts": "'Information systems _ Retrieval models and ranking', 'Security and privacy'", "author_names": "'Avijit Ghosh', 'Matthew Jagielski', 'Christo Wilson'", "author_affiliations": "'Khoury College of Computer Sciences, Northeastern University', 'Google Brain', 'Khoury College of Computer Sciences, Northeastern University'"}, {"link": "https://doi.org/10.1145/3531146.3533169", "title": "Tackling Algorithmic Disability Discrimination in the Hiring Process: An Ethical, Legal and Technical Analysis", "abstract": "Tackling algorithmic discrimination against persons with disabilities (PWDs) demands a distinctive approach that is fundamentally different to that applied to other protected characteristics, due to particular ethical, legal, and technical challenges. We address these challenges specifically in the context of artificial intelligence (AI) systems used in hiring processes (or automated hiring systems, AHSs), in which automated assessment procedures are subject to unique ethical and legal considerations and have an undeniable adverse impact on PWDs. In this paper, we discuss concerns and opportunities raised by AI-driven hiring in relation to disability discrimination. Ultimately, we aim to encourage further research into this topic. Hence, we establish some starting points and design a roadmap for ethicists, lawmakers, advocates as well as AI practitioners alike.", "keywords": "'ethics of discrimination', 'persons with disabilities', 'reasonable accommodation', 'Artificial Intelligence Act', 'equality law', 'data protection law', 'automated hiring systems', 'algorithmic discrimination', 'social justice'", "ccs_concepts": "'Social and professional topics _ People with disabilities', 'Governmental regulations', 'Computing methodologies _ Artificial intelligence'", "author_names": "'Maarten Buyl', 'Christina Cociancig', 'Cristina Frattone', 'Nele Roekens'", "author_affiliations": "'Ghent University', 'University of Bremen', 'Roma Tre University', 'Unia'"}, {"link": "https://doi.org/10.1145/3531146.3533186", "title": "The Algorithmic Imprint", "abstract": "When algorithmic harms emerge, a reasonable response is to stop using the algorithm to resolve concerns related to fairness, accountability, transparency, and ethics (FATE). However, just because an algorithm is removed does not imply its FATE-related issues cease to exist. In this paper, we introduce the notion of the \u201a\u00c4\u00faalgorithmic imprint\u201a\u00c4\u00f9 to illustrate how merely removing an algorithm does not necessarily undo or mitigate its consequences. We operationalize this concept and its implications through the 2020 events surrounding the algorithmic grading of the General Certificate of Education (GCE) Advanced (A) Level exams, an internationally recognized UK-based high school diploma exam administered in over 160 countries. While the algorithmic standardization was ultimately removed due to global protests, we show how the removal failed to undo the algorithmic imprint on the sociotechnical infrastructures that shape students\u201a\u00c4\u00f4, teachers\u201a\u00c4\u00f4, and parents\u201a\u00c4\u00f4 lives. These events provide a rare chance to analyze the state of the world both with and without algorithmic mediation. We situate our case study in Bangladesh to illustrate how algorithms made in the Global North disproportionately impact stakeholders in the Global South. Chronicling more than a year-long community engagement consisting of 47 interviews, we present the first coherent timeline of \u201a\u00c4\u00fawhat\u201a\u00c4\u00f9 happened in Bangladesh, contextualizing \u201a\u00c4\u00fawhy\u201a\u00c4\u00f9 and \u201a\u00c4\u00fahow\u201a\u00c4\u00f9 they happened through the lenses of the algorithmic imprint and situated algorithmic fairness. Analyzing these events, we highlight how the contours of the algorithmic imprints can be inferred at the infrastructural, social, and individual levels. We share conceptual and practical implications around how imprint-awareness can (a) broaden the boundaries of how we think about algorithmic impact, (b) inform how we design algorithms, and (c) guide us in AI governance. The imprint-aware design mindset can make the algorithmic development process more human-centered and sociotechnically-informed.", "keywords": "'Algorithmic Imprint', 'Algorithmic Impact Assessment', 'Situated Fairness', 'Infrastructure', 'Global South', 'Folk Theories of Algorithms', 'User Perceptions'", "ccs_concepts": "'Human-centered computing _ Collaborative and social computing'", "author_names": "'Upol Ehsan', 'Ranjit Singh', 'Jacob Metcalf', 'Mark Riedl'", "author_affiliations": "'Georgia Institute of Technology', 'AI on the Ground Initiative, Data & Society Research Institute', 'AI on the Ground Initiative, Data & Society Research Institute', 'Georgia Institute of Technology'"}, {"link": "https://doi.org/10.1145/3531146.3533134", "title": "The Death of the Legal Subject: How Predictive Algorithms Are (Re)constructing Legal Subjectivity", "abstract": "This paper explores the epistemological differences between the socio-political legal subject of Western liberalism, and the algorithmic subject of informational capitalism. It argues that the increasing use of predictive algorithms in judicial decision-making is reconstructing both the nature and experience of legal subjectivity in a manner that is incompatible with law's normative commitments to individualized justice. Whereas algorithmic subjectivity derives its epistemic authority from population-level insights, legal subjectivity has historically derived credibility from its close approximation of the underlying individual, through careful evaluation of their mental and physical autonomy, prior to any assignment of legal liability. With the introduction of predictive algorithms in judicial decision-making, knowledge about the legal subject is increasingly algorithmically produced, in a manner that discounts, and effectively displaces, qualitative knowledge about the legal subject's intentions, motivations, and moral capabilities. This results in the death of the legal subject, or the emergence of new, algorithmic practices of signification that no longer require the input of the underlying individual. As algorithms increasingly guide judicial decision-making, the shifting epistemology of legal subjectivity has long-term consequences for the legitimacy of legal institutions.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Katrina Geddes'", "author_affiliations": "'New York University'"}, {"link": "https://doi.org/10.1145/3531146.3534629", "title": "The Effects of Crowd Worker Biases in Fact-Checking Tasks", "abstract": "Due to the increasing amount of information shared online every day, the need for sound and reliable ways of distinguishing between trustworthy and non-trustworthy information is as present as ever. One technique for performing fact-checking at scale is to employ human intelligence in the form of crowd workers. Although earlier work has suggested that crowd workers can reliably identify misinformation, cognitive biases of crowd workers may reduce the quality of truthfulness judgments in this context. We performed a systematic exploratory analysis of publicly available crowdsourced data to identify a set of potential systematic biases that may occur when crowd workers perform fact-checking tasks. Following this exploratory study, we collected a novel data set of crowdsourced truthfulness judgments to validate our hypotheses. Our findings suggest that workers generally overestimate the truthfulness of statements and that different individual characteristics (i.e., their belief in science) and cognitive biases (i.e., the affect heuristic and overconfidence) can affect their annotations. Interestingly, we find that, depending on the general judgment tendencies of workers, their biases may sometimes lead to more accurate judgments.", "keywords": "'Truthfulness', 'Crowdsourcing', 'Misinformation', 'Explainability', 'Bias'", "ccs_concepts": "'Information systems _ Crowdsourcing', 'General and reference _ Estimation'", "author_names": "'Tim Draws', 'David La Barbera', 'Michael Soprano', 'Kevin Roitero', 'Davide Ceolin', 'Alessandro Checco', 'Stefano Mizzaro'", "author_affiliations": "'Delft University of Technology', 'University of Udine', 'University of Udine', 'University of Udine', 'CWI', 'University of Rome La Sapienza', 'University of Udine'"}, {"link": "https://doi.org/10.1145/3531146.3533157", "title": "The Forgotten Margins of AI Ethics", "abstract": "How has recent AI Ethics literature addressed topics such as fairness and justice in the context of continued social and structural power asymmetries? We trace both the historical roots and current landmark work that have been shaping the field and categorize these works under three broad umbrellas: (i) those grounded in Western canonical philosophy, (ii) mathematical and statistical methods, and (iii) those emerging from critical data/algorithm/information studies. We also survey the field and explore emerging trends by examining the rapidly growing body of literature that falls under the broad umbrella of AI Ethics. To that end, we read and annotated peer-reviewed papers published over the past four years in two premier conferences: FAccT and AIES. We organize the literature based on an annotation scheme we developed according to three main dimensions: whether the paper deals with concrete applications, use-cases, and/or people\u201a\u00c4\u00f4s lived experience; to what extent it addresses harmed, threatened, or otherwise marginalized groups; and if so, whether it explicitly names such groups. We note that although the goals of the majority of FAccT and AIES papers were often commendable, their consideration of the negative impacts of AI on traditionally marginalized groups remained shallow. Taken together, our conceptual analysis and the data from annotated papers indicate that the field would benefit from an increased focus on ethical analysis grounded in concrete use-cases, people\u201a\u00c4\u00f4s experiences, and applications as well as from approaches that are sensitive to structural and historical power asymmetries.", "keywords": "'AI Ethics', 'Trends', 'Justice', 'FAccT', 'AIES'", "ccs_concepts": "'Social and professional topics', 'Computing methodologies _ Artificial intelligence'", "author_names": "'Abeba Birhane', 'Elayne Ruane', 'Thomas Laurent', 'Matthew S. Brown', 'Johnathan Flowers', 'Anthony Ventresque', 'Christopher L. Dancy'", "author_affiliations": "'Mozilla Foundation, USA and School of Computer Science, University College Dublin', 'School of Computer Science, University College Dublin', 'School of Computer Science, University College Dublin', 'School of Computer Science, Bucknell University', 'Worcester State University', 'School of Computer Science, University College Dublin', 'Dept of Industrial and Manufacturing Engineering & Dept of Computer Science and Engineering, Pennsylvania State University'"}, {"link": "https://doi.org/10.1145/3531146.3533179", "title": "The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations", "abstract": "Machine learning models in safety-critical settings like healthcare are often \u201a\u00c4\u00fablackboxes\u201a\u00c4\u00f9: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.", "keywords": "'explainability', 'machine learning', 'fairness'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Human-centred computing _ explanations'", "author_names": "'Aparna Balagopalan', 'Haoran Zhang', 'Kimia Hamidieh', 'Thomas Hartvigsen', 'Frank Rudzicz', 'Marzyeh Ghassemi'", "author_affiliations": "'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute, Canada and Unity Health Toronto', 'Massachusetts Institute of Technology, USA and Vector Institute'"}, {"link": "https://doi.org/10.1145/3531146.3534627", "title": "Theories of \u00d4\u00f8\u03a9Gender\u00d4\u00f8\u03a9 in NLP Bias Research", "abstract": "The rise of concern around Natural Language Processing (NLP) technologies containing and perpetuating social biases has led to a rich and rapidly growing area of research. Gender bias is one of the central biases being analyzed, but to date there is no comprehensive analysis of how \u201a\u00c4\u00fagender\u201a\u00c4\u00f9 is theorized in the field. We survey nearly 200 articles concerning gender bias in NLP to discover how the field conceptualizes gender both explicitly (e.g. through definitions of terms) and implicitly (e.g. through how gender is operationalized in practice). In order to get a better idea of emerging trajectories of thought, we split these articles into two sections by time.  We find that the majority of the articles do not make their theorization of gender explicit, even if they clearly define \u201a\u00c4\u00fabias.\u201a\u00c4\u00f9 Almost none use a model of gender that is intersectional or inclusive of nonbinary genders; and many conflate sex characteristics, social gender, and linguistic gender in ways that disregard the existence and experience of trans, nonbinary, and intersex people. There is an increase between the two time-sections in statements acknowledging that gender is a complicated reality, however, very few articles manage to put this acknowledgment into practice. In addition to analyzing these findings, we provide specific recommendations to facilitate interdisciplinary work, and to incorporate theory and methodology from Gender Studies. Our hope is that this will produce more inclusive gender bias research in NLP.", "keywords": "'natural language processing', 'gender bias', 'gender studies'", "ccs_concepts": NaN, "author_names": "'Hannah Devinney', 'Jenny Bj\u221a\u2202rklund', 'Henrik Bj\u221a\u2202rklund'", "author_affiliations": "'Ume\u221a\u2022 University', 'Uppsala University', 'Ume\u221a\u2022 University'"}, {"link": "https://doi.org/10.1145/3531146.3533118", "title": "Towards a multi-stakeholder value-based assessment framework for algorithmic systems", "abstract": "In an effort to regulate Machine Learning-driven (ML) systems, current auditing processes mostly focus on detecting harmful algorithmic biases. While these strategies have proven to be impactful, some values outlined in documents dealing with ethics in ML-driven systems are still underrepresented in auditing processes. Such unaddressed values mainly deal with contextual factors that cannot be easily quantified. In this paper, we develop a value-based assessment framework that is not limited to bias auditing and that covers prominent ethical principles for algorithmic systems. Our framework presents a circular arrangement of values with two bipolar dimensions that make common motivations and potential tensions explicit. In order to operationalize these high-level principles, values are then broken down into specific criteria and their manifestations. However, some of these value-specific criteria are mutually exclusive and require negotiation. As opposed to some other auditing frameworks that merely rely on ML researchers\u201a\u00c4\u00f4 and practitioners\u201a\u00c4\u00f4 input, we argue that it is necessary to include stakeholders that present diverse standpoints to systematically negotiate and consolidate value and criteria tensions. To that end, we map stakeholders with different insight needs, and assign tailored means for communicating value manifestations to them. We, therefore, contribute to current ML auditing practices with an assessment framework that visualizes closeness and tensions between values and we give guidelines on how to operationalize them, while opening up the evaluation and deliberation process to a wide range of stakeholders.", "keywords": "'values', 'ML development and deployment pipeline', 'algorithm assessment', 'multi-stakeholder'", "ccs_concepts": "'General and reference _ Evaluation', 'Human-centered computing _ Human computer interaction (HCI)', 'Social and professional topics _ User characteristics'", "author_names": "'Mireia Yurrita', 'Dave Murray-Rust', 'Agathe Balayn', 'Alessandro Bozzon'", "author_affiliations": "'Human Information Communication Design, TU Delft', 'Human Information Communication Design, TU Delft', 'Web Information Systems, TU Delft', 'Knowledge and Intelligence Design, TU Delft'"}, {"link": "https://doi.org/10.1145/3531146.3533197", "title": "Towards Fair Unsupervised Learning", "abstract": "Bias-mitigating techniques are now well established in the supervised learning literature and have shown their ability to tackle fairness-accuracy, as well as fairness-fairness trade-offs. These are usually predicated on different conceptions of fairness, such as demographic parity or equal odds that depend on the available labels in the dataset. However, it is often the case in practice that unsupervised learning is used as part of a machine learning pipeline (for instance, to perform dimensionality reduction or representation learning via SVD) or as a standalone model (for example, to derive a customer segmentation via k-means). It is thus crucial to develop approaches towards fair unsupervised learning. This work investigates fair unsupervised learning within the broad framework of generalised low-rank models (GLRM). Importantly, we introduce the concept of fairness functional that encompasses both traditional unsupervised learning techniques and min-max algorithms (whereby one minimises the maximum group loss). To do so, we design straightforward alternate convex search or biconvex gradient descent algorithms that also provide partial debiasing techniques. Finally, we show on benchmark datasets that our fair generalised low-rank models (\u201a\u00c4\u00fafGLRM\u201a\u00c4\u00f9) perform well and help reduce disparity amongst groups while only incurring small runtime overheads.", "keywords": "'Unsupervised Learning', 'PCA', 'Clustering', 'Fairness'", "ccs_concepts": "'Theory of computation _ Unsupervised learning and clustering', 'Social and professional topics _ User characteristics'", "author_names": "'Francois Buet-Golfouse', 'Islam Utyagulov'", "author_affiliations": "'University College London', 'Independent'"}, {"link": "https://doi.org/10.1145/3531146.3533132", "title": "Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Feminicide Counterdata Collection", "abstract": "Data ethics and fairness have emerged as important areas of research in recent years. However, much work in this area focuses on retroactively auditing and \u201a\u00c4\u00famitigating bias\u201a\u00c4\u00f9 in existing, potentially flawed systems, without interrogating the deeper structural inequalities underlying them. There are not yet examples of how to apply feminist and participatory methodologies from the start, to conceptualize and design machine learning-based tools that center and aim to challenge power inequalities. Our work targets this more prospective goal. Guided by the framework of data feminism, we co-design datasets and machine learning models to support the efforts of activists who collect and monitor data about feminicide\u00ac\u2020\u201a\u00c4\u00ee\u00ac\u2020gender-based killings of women and girls. We describe how intersectional feminist goals and participatory processes shaped each stage of our approach, from problem conceptualization to data collection to model evaluation. We highlight several methodological contributions, including 1) an iterative data collection and annotation process that targets model weaknesses and interrogates framing concepts (such as who is included/excluded in \u201a\u00c4\u00fafeminicide\u201a\u00c4\u00f9), 2) models that explicitly focus on intersectional identities rather than statistical majorities, and 3) a multi-step evaluation process\u00ac\u2020\u201a\u00c4\u00ee\u00ac\u2020with quantitative, qualitative and participatory steps\u00ac\u2020\u201a\u00c4\u00ee\u00ac\u2020focused on context-specific relevance. We also distill insights and tensions that arise from bridging intersectional feminist goals with ML. These include reflections on how ML may challenge power, embrace pluralism, rethink binaries and consider context, as well as the inherent limitations of any technology-based solution to address durable structural inequalities.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Harini Suresh', 'Rajiv Movva', 'Amelia Lee Dogan', 'Rahul Bhargava', 'Isadora Cruxen', 'Angeles Martinez Cuba', 'Guilia Taurino', 'Wonyoung So', \"Catherine D'Ignazio\"", "author_affiliations": "'Data + Feminism Lab, Massachusetts Institute of Technology, USA and CSAIL, Massachusetts Institute of Technology', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'School of Journalism, Northeastern University', 'School of Business and Management, Queen Mary University of London', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'Khoury College of Computer Sciences, Northeastern University', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'Data + Feminism Lab, Massachusetts Institute of Technology'"}, {"link": "https://doi.org/10.1145/3531146.3533087", "title": "Treatment Effect Risk: Bounds and Inference", "abstract": "Since the average treatment effect (ATE) measures the change in social welfare, even if positive, there is a risk of negative effect on, say, some 10% of the population. Assessing such risk is difficult, however, because any one individual treatment effect (ITE) is never observed so the 10% worst-affected cannot be identified, while distributional treatment effects only compare the first deciles within each treatment group, which does not correspond to any 10%-subpopulation. In this paper we consider how to nonetheless assess this important risk measure, formalized as the conditional value at risk (CVaR) of the ITE-distribution. We leverage the availability of pre-treatment covariates and characterize the tightest-possible upper and lower bounds on ITE-CVaR given by the covariate-conditional average treatment effect (CATE) function. We then proceed to study how to estimate these bounds efficiently from data and construct confidence intervals. This is challenging even in randomized experiments as it requires understanding the distribution of the unknown CATE function, which can be very complex if we use rich covariates so as to best control for heterogeneity. We develop a debiasing method that overcomes this and prove it enjoys favorable statistical properties even when CATE and other nuisances are estimated by black-box machine learning or even inconsistently. Studying a hypothetical change to French job-search counseling services, our bounds and inference demonstrate a small social benefit entails a negative impact on a substantial subpopulation.", "keywords": "'Program evaluation', 'Individual treatment effect', 'Conditional average treatment effect', 'Conditional value at risk', 'Partial identification', 'Debiased machine learning'", "ccs_concepts": NaN, "author_names": "'Nathan Kallus'", "author_affiliations": "'Cornell University, USA and Netflix'"}, {"link": "https://doi.org/10.1145/3531146.3533145", "title": "Trucks Don\u00d4\u00f8\u03a9t Mean Trump: Diagnosing Human Error in Image Analysis", "abstract": "Algorithms provide powerful tools for detecting and dissecting human bias and error. Here, we develop machine learning methods to to analyze how humans err in a particular high-stakes task: image interpretation. We leverage a unique dataset of 16,135,392 human predictions of whether a neighborhood voted for Donald Trump or Joe Biden in the 2020 US election, based on a Google Street View image. We show that by training a machine learning estimator of the Bayes optimal decision for each image, we can provide an actionable decomposition of human error into bias, variance, and noise terms, and further identify specific features (like pickup trucks) which lead humans astray. Our methods can be applied to ensure that human-in-the-loop decision-making is accurate and fair and are also applicable to black-box algorithmic systems.", "keywords": "'image analysis', 'human error', 'diagnosing bias'", "ccs_concepts": "'Human-centered computing _ HCI design and evaluation methods', 'Computing methodologies _ Machine learning approaches'", "author_names": "'J.D. Zamfirescu-Pereira', 'Jerry Chen', 'Emily Wen', 'Allison Koenecke', 'Nikhil Garg', 'Emma Pierson'", "author_affiliations": "'UC Berkeley', 'Stanford University', 'Stanford University', 'Cornell University', 'Cornell Tech', 'Cornell University'"}, {"link": "https://doi.org/10.1145/3531146.3533243", "title": "Uncertainty and the Social Planner\u00d4\u00f8\u03a9s Problem: Why Sample Complexity Matters", "abstract": "Welfare measures overall utility across a population, whereas malfare measures overall disutility, and the social planner\u201a\u00c4\u00f4s problem can be cast either as maximizing the former or minimizing the latter. We show novel bounds on the expectations and tail probabilities of estimators of welfare, malfare, and regret of per-group (dis)utility values, where estimates are made from a finite sample drawn from each group. In particular, we consider estimating these quantities for individual functions (e.g., allocations or classifiers) with standard probabilistic bounds, and optimizing and bounding generalization error over hypothesis classes (i.e., we quantify overfitting) using Rademacher averages. We then study algorithmic fairness through the lens of sample complexity, finding that because marginalized or minority groups are often understudied, and fewer data are therefore available, the social planner is more likely to overfit to these groups, thus even models that seem fair in training can be systematically biased against such groups. We argue that this effect can be mitigated by ensuring sufficient sample sizes for each group, and our sample complexity analysis characterizes these sample sizes. Motivated by these conclusions, we present progressive sampling algorithms to efficiently optimize various fairness objectives.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Cyrus Cousins'", "author_affiliations": "'Department of Computer Science, Brown University'"}, {"link": "https://doi.org/10.1145/3531146.3533242", "title": "What is Proxy Discrimination?", "abstract": "The near universal condemnation of proxy discrimination hides a disagreement over what it is. This work surveys various notions of proxy and proxy discrimination found in prior work and represents them in a common framework. These notions variously turn on statistical dependencies, causal effects, and intentions. It discusses the limitations and uses of each notation and of the concept as a whole.", "keywords": "'proxy', 'discrimination', 'conceptual analysis'", "ccs_concepts": "'Social and professional topics _ Computing / technology policy'", "author_names": "'Michael Carl Tschantz'", "author_affiliations": "'International Computer Science Institute'"}, {"link": "https://doi.org/10.1145/3531146.3533080", "title": "What People Think AI Should Infer From Faces", "abstract": "Faces play an indispensable role in human social life. At present, computer vision artificial intelligence (AI) captures and interprets human faces for a variety of digital applications and services. The ambiguity of facial information has recently led to a debate among scholars in different fields about the types of inferences AI should make about people based on their facial looks. AI research often justifies facial AI inference-making by referring to how people form impressions in first-encounter scenarios. Critics raise concerns about bias and discrimination and warn that facial analysis AI resembles an automated version of physiognomy. What has been missing from this debate, however, is an understanding of how \u201a\u00c4\u00fanon-experts\u201a\u00c4\u00f9 in AI ethically evaluate facial AI inference-making. In a two-scenario vignette study with 24 treatment groups, we show that non-experts (N = 3745) reject facial AI inferences such as trustworthiness and likability from portrait images in a low-stake advertising and a high-stake hiring context. In contrast, non-experts agree with facial AI inferences such as skin color or gender in the advertising but not the hiring decision context. For each AI inference, we ask non-experts to justify their evaluation in a written response. Analyzing 29,760 written justifications, we find that non-experts are either \u201a\u00c4\u00faevidentialists\u201a\u00c4\u00f9 or \u201a\u00c4\u00fapragmatists\u201a\u00c4\u00f9: they assess the ethical status of a facial AI inference based on whether they think faces warrant sufficient or insufficient evidence for an inference (evidentialist justification) or whether making the inference results in beneficial or detrimental outcomes (pragmatist justification). Non-experts\u201a\u00c4\u00f4 justifications underscore the normative complexity behind facial AI inference-making. AI inferences with insufficient evidence can be rationalized by considerations of relevance while irrelevant inferences can be justified by reference to sufficient evidence. We argue that participatory approaches contribute valuable insights for the development of ethical AI in an increasingly visual data culture.", "keywords": "'artificial intelligence', 'computer vision', 'human faces', 'participatory AI ethics'", "ccs_concepts": "'Computing methodologies _ Computer vision tasks', 'Social and professional topics _ User characteristics', 'Security and privacy _ Social aspects of security and privacy'", "author_names": "'Severin Engelmann', 'Chiara Ullstein', 'Orestis Papakyriakopoulos', 'Jens Grossklags'", "author_affiliations": "'Chair of Cyber Trust, Department of Informatics, Technical University Munich', 'Chair of Cyber Trust, Department of Informatics, Technical University Munich', 'Princeton University', 'Chair of Cyber Trust, Department of Informatics, Technical University Munich'"}, {"link": "https://doi.org/10.1145/3531146.3533078", "title": "When learning becomes impossible", "abstract": "We formally analyze an epistemic bias we call interpretive blindness (IB), in which under certain conditions a learner will be incapable of learning. IB is now common in our society, but it is a natural consequence of Bayesian inference and what we argue are mild assumptions about the relation between belief and evidence. IB a special problem for learning from testimony, in which one acquires information only from text or conversation. We show that IB follows from a codependence between background beliefs and interpretation in a Bayesian setting and the nature of contemporary testimony. We argue that a particular characteristic of contemporary testimony, argumentative completeness, can preclude learning in hierarchical Bayesian settings, even in the presence of constraints that are designed to promote good epistemic practices.", "keywords": "'learning', 'bias', 'agent modeling', 'echo chambers', 'Bayesian learning', 'philosophical foundations'", "ccs_concepts": NaN, "author_names": "'Nicholas Asher', 'Julie Hunter'", "author_affiliations": "'Institut de Recherche en Informatique de Toulouse, Centre Nationale de Recherche Scientifique', 'LINAGORA Labs'"}, {"link": "https://doi.org/10.1145/3531146.3533213", "title": "Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem", "abstract": "Algorithmic audits (or \u201a\u00c4\u00f2AI audits\u201a\u00c4\u00f4) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.", "keywords": "'AI audit', 'algorithm audit', 'audit', 'ethical AI', 'AI bias', 'AI harm', 'AI policy', 'algorithmic accountability'", "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Human-centered computing'", "author_names": "'Sasha Costanza-Chock', 'Inioluwa Deborah Raji', 'Joy Buolamwini'", "author_affiliations": "'Algorithmic Justice League', 'Algorithmic Justice League', 'Algorithmic Justice League'"}, {"link": "https://doi.org/10.1145/3531146.3533176", "title": "Brain Computer Interfaces and Human Rights: Brave new rights for a brave new world", "abstract": "Digital health applications include a wide range of wearable, implantable, injectable and ingestible digital medical devices. Many of these devices use machine learning algorithms to assist medical prognosis and decision-making. One of the most compelling digital medical device developments is brain-computer interfaces (BCIs) which entails the connecting of a person's brain to a computer, or to another device outside the human body. BCIs allow bidirectional communication and control between the human brain and the outside world by exporting brain data or altering brain activity. Although being marveled at for its clinical promises, this technological advancement also raises novel ethical, legal, social and technical implications (ELSTI). Debates in this regard centers around patient autonomy, equity, trustworthiness in healthcare, data protection and security, risks of dehumanization, the limitations of machine learning-based decision-making, and the influence that BCIs have on what it means to be human and human rights. Since the adoption of the Universal Declaration of Human Rights (UDHR) after World War II, the landscape that give rise to these human rights has evolved enormously. Human life and humans\u201a\u00c4\u00f4 role in society are being transformed and threatened by technologies that were never imagined at the time the UDHR was adopted. BCIs, in particular, harbor the greatest possibility of social and individual disruption through its capability to record, interpret, manipulate, or alter brain activity that may potentially alter what it means to be human and how we control humans in future. Cutting edge technological innovations that increasingly blur the lines between human and computer beg the rethinking and extension of existing human rights to remain relevant in a digitized world. In this paper sui generis human rights such as mental privacy, the right to identity or self, agency or free will and fair access to cognitive augmentation will be discussed and how a regulatory framework must be adapted to act as technology enablers, whilst ensuring fairness, accountability, and transparency in sociotechnical systems.", "keywords": "'brain computer interfaces', 'human rights', 'neurological privacy', 'autonomy', 'identity'", "ccs_concepts": NaN, "author_names": "'Marietjie Wilhelmina Maria Botes'", "author_affiliations": "'Computer Sciences, SnT Interdisciplinary Centre for Security Reliability and Trust'"}, {"link": "https://doi.org/10.1145/3531146.3534637", "title": "Data Governance in the Age of Large-Scale Data-Driven Language Technology", "abstract": "The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.", "keywords": "'datasets', 'technology governance', 'data rights', 'language data'", "ccs_concepts": "'Social and professional topics', 'Social and professional topics _ Information system economics', 'Social and professional topics _ Digital rights management'", "author_names": "'Yacine Jernite', 'Huu Nguyen', 'Stella Biderman', 'Anna Rogers', 'Maraim Masoud', 'Valentin Danchev', 'Samson Tan', 'Alexandra Sasha Luccioni', 'Nishant Subramani', 'Isaac Johnson', 'Gerard Dupont', 'Jesse Dodge', 'Kyle Lo', 'Zeerak Talat', 'Dragomir Radev', 'Aaron Gokaslan', 'Somaieh Nikpoor', 'Peter Henderson', 'Rishi Bommasani', 'Margaret Mitchell'", "author_affiliations": "'Hugging Face', 'Ontocord', 'EleutherAI', 'University of Copenhagen', 'Independent researcher', 'University of Essex', 'AWS AI Research & Education', 'Hugging Face', 'Allen Institute for Artificial Intelligence', 'Wikimedia', 'Independent', 'Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence', 'Simon Fraser University', 'Yale University', 'Cornell University', 'CAIDP', 'Stanford University', 'Stanford University', 'Hugging Face'"}, {"link": "https://doi.org/10.1145/3531146.3533226", "title": "Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness", "abstract": "Most proposed algorithmic fairness techniques require access to demographic data in order to make performance comparisons and standardizations across groups, however this data is largely unavailable in practice, hindering the widespread adoption of algorithmic fairness. Through this paper, we consider calls to collect more data on demographics to enable algorithmic fairness and challenge the notion that discrimination can be overcome with smart enough technical methods and sufficient data. We show how these techniques largely ignore broader questions of data governance and systemic oppression when categorizing individuals for the purpose of fairer algorithmic processing. In this work, we explore under what conditions demographic data should be collected and used to enable algorithmic fairness methods by characterizing a range of social risks to individuals and communities. For the risks to individuals we consider the unique privacy risks of sensitive attributes, the possible harms of miscategorization and misrepresentation, and the use of sensitive data beyond data subjects\u201a\u00c4\u00f4 expectations. Looking more broadly, the risks to entire groups and communities include the expansion of surveillance infrastructure in the name of fairness, misrepresenting and mischaracterizing what it means to be part of a demographic group, and ceding the ability to define what constitutes biased or unfair treatment. We argue that, by confronting these questions before and during the collection of demographic data, algorithmic fairness methods are more likely to actually mitigate harmful treatment disparities without reinforcing systems of oppression. Towards this end, we assess privacy-focused methods of data collection and use and participatory data governance structures as proposals for more responsibly collecting demographic data.", "keywords": "'demographic data', 'sensitive data', 'categorization', 'fairness', 'discrimination', 'identity', 'race', 'gender', 'sexuality', 'measurement'", "ccs_concepts": "'Security and privacy _ Social aspects of security and privacy', 'Privacy protections', 'Economics of security and privacy', 'Social and professional topics _ User characteristics', 'Gender', 'Sexual orientation'", "author_names": "'McKane Andrus', 'Sarah Villeneuve'", "author_affiliations": "'Partnership on AI', 'Partnership on AI'"}, {"link": "https://doi.org/10.1145/3531146.3533133", "title": "Disclosure by Design: Designing information disclosures to support meaningful transparency and accountability", "abstract": "There is a strong push for organisations to become more transparent and accountable for their undertakings. Towards this, various transparency regimes oblige organisations to disclose certain information to relevant stakeholders (individuals, regulators, etc). This information intends to empower and support the monitoring, oversight, scrutiny and challenge of organisational practices. Importantly, however, these disclosures are of limited benefit if they are not meaningful for their recipients. Yet, in practice, the disclosures of tech/data-driven organisations are often highly technical, fragmented, and therefore of limited utility to all but experts. This undermines a disclosure\u201a\u00c4\u00f4s effectiveness, works to disempower, and ultimately hinders broader transparency aims.  This paper argues for a paradigm shift towards reconceptualising disclosures as \u201a\u00c4\u00f2interfaces\u201a\u00c4\u00f4 \u201a\u00c4\u00ec designed for the needs, expectations and requirements of the recipients they serve to inform. In making this case, and to provide a practical way forward, we demonstrate Document Engineering as one potential methodology for specifying, designing, and deploying more effective information disclosures. Focusing on data protection disclosures, we illustrate and explore how designing disclosures as interfaces can better support greater oversight of organisational data and practices, and thus better align with broader transparency and accountability aims.", "keywords": "'transparency', 'accountability', 'GDPR', 'document engineering', 'interfaces', 'data rights', 'usability'", "ccs_concepts": "'Security and privacy _ Human and societal aspects of security and privacy', 'Human-centered computing', 'Human-centered computing _ Interaction design'", "author_names": "'Chris Norval', 'Kristin Cornelius', 'Jennifer Cobbe', 'Jatinder Singh'", "author_affiliations": "'Compliant & Accountable Systems Group, University of Cambridge', 'Informatics Department, UCLA, Thousand Oaks, California, United States', 'Compliant & Accountable Systems Group, University of Cambridge', 'Compliant & Accountable Systems Group, University of Cambridge'"}, {"link": "https://doi.org/10.1145/3531146.3533148", "title": "Learning to Limit Data Collection via Scaling Laws: A Computational Interpretation for the Legal Principle of Data Minimization", "abstract": "Modern machine learning systems are increasingly characterized by extensive personal data collection, despite the diminishing returns and increasing societal costs of such practices. Yet, data minimisation is one of the core data protection principles enshrined in the European Union\u201a\u00c4\u00f4s General Data Protection Regulation (\u201a\u00c4\u00f4GDPR\u201a\u00c4\u00f4) and requires that only personal data that is adequate, relevant and limited to what is necessary is processed. However, the principle has seen limited adoption due to the lack of technical interpretation.  In this work, we build on literature in machine learning and law to propose FIDO, a Framework for Inhibiting Data Overcollection. FIDO learns to limit data collection based on an interpretation of data minimization tied to system performance. Concretely, FIDO provides a data collection stopping criterion by iteratively updating an estimate of the performance curve, or the relationship between dataset size and performance, as data is acquired. FIDO estimates the performance curve via a piecewise power law technique that models distinct phases of an algorithm\u201a\u00c4\u00f4s performance throughout data collection separately. Empirical experiments show that the framework produces accurate performance curves and data collection stopping criteria across datasets and feature acquisition algorithms. We further demonstrate that many other families of curves systematically overestimate the return on additional data. Results and analysis from our investigation offer deeper insights into the relevant considerations when designing a data minimization framework, including the impacts of active feature acquisition on individual users and the feasability of user-specific data minimization. We conclude with practical recommendations for the implementation of data minimization.", "keywords": NaN, "ccs_concepts": "'Social and professional topics _ Governmental regulations', 'Applied computing _ Law', 'Computing methodologies _ Feature selection', 'Computing methodologies~Online learning settings'", "author_names": "'Divya Shanmugam', 'Fernando Diaz', 'Samira Shabanian', 'Michele Finck', 'Asia Biega'", "author_affiliations": "'MIT', 'Google', 'Microsoft Research, Canada', 'University of Tuebigen', 'Max Planck Institute for Security and Privacy'"}, {"link": "https://doi.org/10.1145/3531146.3533235", "title": "Model Explanations with Differential Privacy", "abstract": "Using machine learning models in critical decision-making processes has given rise to a call for algorithmic transparency. Model explanations, however, might leak information about the sensitive data used to train and explain the model, undermining data privacy. We focus on black-box feature-based model explanations, which locally approximate the model around the point of interest, using potentially sensitive data. We design differentially private local approximation mechanisms, and evaluate their effect on explanation quality. To protect training data, we use existing differentially private learning algorithms. However, to protect the privacy of data which is used during the local approximation, we design an adaptive differentially private algorithm, which finds the minimal privacy budget required to produce accurate explanations. Both empirically and analytically, we evaluate the impact of the randomness needed in differential privacy algorithms on the fidelity of model explanations.", "keywords": "'Differential Privacy', 'Model Explainations'", "ccs_concepts": NaN, "author_names": "'Neel Patel', 'Reza Shokri', 'Yair Zick'", "author_affiliations": "'Viterbi School of engineering, University of Southern California', 'National University of Singapore', 'University of Massachusetts, Amherst'"}, {"link": "https://doi.org/10.1145/3531146.3534642", "title": "What Does it Mean for a Language Model to Preserve Privacy?", "abstract": "Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.", "keywords": "'Natural Language Processing', 'Privacy', 'Differential Privacy', 'Data Sanitization'", "ccs_concepts": "'Social and professional topics _ Privacy policies', 'Computing methodologies _ Natural language processing'", "author_names": "'Hannah Brown', 'Katherine Lee', 'Fatemehsadat Mireshghallah', 'Reza Shokri', 'Florian Tram\u221a\u00aer'", "author_affiliations": "'National University of Singapore', 'Cornell University', 'University of California, San Diego', 'National University of Singapore', 'Google Research'"}, {"link": "https://doi.org/10.1145/3531146.3533237", "title": "How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India", "abstract": "Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a \u201a\u00c4\u00f2high-risk\u201a\u00c4\u00f4 AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the \u201a\u00c4\u00f2boon\u201a\u00c4\u00f4 of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.", "keywords": "'algorithmic accountability', 'algorithmic fairness', 'human-ai interaction', 'instant loans', 'socio-technical systems'", "ccs_concepts": "'Computer systems organization _ Embedded systems', 'Computer systems organization~Robotics', 'Networks~Network reliability'", "author_names": "'Divya Ramesh', 'Vaishnav Kameswaran', 'Ding Wang', 'Nithya Sambasivan'", "author_affiliations": "'Computer Science and Engineering, University of Michigan, Ann Arbor', 'School of Information, University of Michigan, Ann Arbor', 'Google Research', 'Unaffiliated'"}, {"link": "https://doi.org/10.1145/3531146.3533115", "title": "Affirmative Algorithms: Relational Equality as Algorithmic Fairness", "abstract": "Many statistical fairness notions have been proposed for algorithmic decision-making systems, and especially public safety pretrial risk assessment (PSPRA) algorithms such as COMPAS. Most fairness notions equalize something between groups, whether it is false positive rates or accuracy. In fact, I demonstrate that most prominent notions have their basis in equalizing some form of accuracy. However, statistical fairness metrics often do not capture the substantive point of equality. I argue that equal accuracy is not only difficult to measure but also unsatisfactory for ensuring equal justice. In response, I introduce philosopher Elizabeth Anderson\u201a\u00c4\u00f4s theory of relational equality as a fruitful alternative framework: to relate as equals, people need access to certain basic capabilities. I show that relational equality requires Affirmative PSPRA algorithms that lower risk scores for Black defendants. This is because fairness based on relational equality means considering the impact of PSPRA algorithms\u201a\u00c4\u00f4 decisions on access to basic capabilities. This impact is racially asymmetric in an unjust society. I make three main contributions: (1) I illustrate the shortcomings of statistical fairness notions in their reliance on equalizing some form of accuracy; (2) I present the first comprehensive ethical defense of Affirmative PSPRA algorithms, based on fairness in terms of relational equality instead; and (3) I show that equalizing accuracy is neither sufficient nor necessary for fairness based on relational equality. Overall, this work serves narrowly as a reason to re-evaluate algorithmic fairness for PSPRA algorithms, and serves broadly as an example of how discussions of algorithmic fairness can benefit from egalitarian philosophical frameworks.", "keywords": "'fairness', 'algorithmic fairness', 'philosophy', 'relational equality', 'affirmative algorithms', 'criminal justice', 'pretrial risk assessments'", "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Artificial intelligence', 'Applied computing _ Law', 'social and behavioral sciences'", "author_names": "'Marilyn Zhang'", "author_affiliations": "'Stanford University'"}, {"link": "https://doi.org/10.1145/3531146.3533084", "title": "AI Opacity and Explainability in Tort Litigation", "abstract": "A spate of recent accidents and a lawsuit involving Tesla's \u201a\u00c4\u00f2self-driving\u201a\u00c4\u00f4 cars highlights the growing need for meaningful accountability when harms are caused by AI systems. Tort (or civil liability) lawsuits are one important way for victims to redress such harms. The prospect of tort liability may also prompt AI developers to take better precautions against safety risks. Tort claims of all kinds will be hindered by AI opacity: the difficulty of determining how and why complex AI systems make decisions. We address this problem by formulating and evaluating several options for mitigating AI opacity that combine expert evidence, legal argumentation, civil procedure, and Explainable AI approaches. We emphasise the need for explanations of AI systems in tort litigation to be attuned to the elements of legal \u201a\u00c4\u00f2causes of action\u201a\u00c4\u00f4 \u201a\u00c4\u00ec the specific facts that must be proven to succeed in a lawsuit. We take a recent Australian case involving explainable AI evidence as a starting point from which to map contemporary Explainable AI approaches to elements of tortious causes of action, focusing on misleading conduct, negligence, and product liability for safety defects. Our work synthesizes law, legal procedure, and computer science to provide greater clarity on the opportunities and challenges for Explainable AI in civil litigation, and may prove helpful to potential litigants, to courts, and to illuminate key targets for regulatory intervention.", "keywords": "'Explainable AI', 'Law', 'Evidence', 'Expert Evidence', 'AI Opacity', 'Civil Procedure', 'Negligence', 'Product Liability', 'Autonomous Vehicle', 'Accidents', 'Causation', 'Damages'", "ccs_concepts": "'Applied computing _ Law', 'Computing methodologies _ Machine learning', 'Computing methodologies _ Artificial intelligence', 'Social and professional topics _ Computing / technology policy'", "author_names": "'Henry Fraser', 'Rhyle Simcock', 'Aaron J. Snoswell'", "author_affiliations": "'Queensland University of Technology, Centre for Automated Decision-Making and Society, Australia and Queensland University of Technology, Digital Media Research Centre', 'Queensland University of Technology, Centre for Automated Decision-Making and Society, Australia and Queensland University of Technology, Digital Media Research Centre', 'Queensland University of Technology, Centre for Automated Decision-Making and Society, Australia and Queensland University of Technology, Digital Media Research Centre'"}, {"link": "https://doi.org/10.1145/3531146.3533204", "title": "Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models", "abstract": "This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity\u201a\u00c4\u00eeappropriately accounting for relevant differences across individuals\u201a\u00c4\u00eewhich is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods\u201a\u00c4\u00eeas opposed to simpler models\u201a\u00c4\u00eeshapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.", "keywords": NaN, "ccs_concepts": NaN, "author_names": "'Emily Black', 'Hadi Elzayn', 'Alexandra Chouldechova', 'Jacob Goldin', 'Daniel Ho'", "author_affiliations": "'Computer Science Dept., Carngie Mellon University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University', 'Stanford University'"}, {"link": "https://doi.org/10.1145/3531146.3533172", "title": "An Algorithmic Framework for Bias Bounties", "abstract": "We propose and analyze an algorithmic framework for \u201a\u00c4\u00fabias bounties\u201a\u00c4\u00f9 \u201a\u00c4\u00ee events in which external participants are invited to propose improvements to a trained model, akin to bug bounty events in software and security. Our framework allows participants to submit arbitrary subgroup improvements, which are then algorithmically incorporated into an updated model. Our algorithm has the property that there is no tension between overall and subgroup accuracies, nor between different subgroup accuracies, and it enjoys provable convergence to either the Bayes optimal model or a state in which no further improvements can be found by the participants. We provide formal analyses of our framework, experimental evaluation, and findings from a preliminary bias bounty event.1", "keywords": "'bias bounty', 'subgroup fairness', 'multigroup fairness'", "ccs_concepts": "'Theory of computation _ Machine learning theory'", "author_names": "'Ira Globus-Harris', 'Michael Kearns', 'Aaron Roth'", "author_affiliations": "'University of Pennsylvania/Amazon', 'University of Pennsylvania/Amazon', 'University of Pennsylvania/Amazon'"}, {"link": "https://doi.org/10.1145/3531146.3533176", "title": "Brain Computer Interfaces and Human Rights: Brave new rights for a brave new world", "abstract": "Digital health applications include a wide range of wearable, implantable, injectable and ingestible digital medical devices. Many of these devices use machine learning algorithms to assist medical prognosis and decision-making. One of the most compelling digital medical device developments is brain-computer interfaces (BCIs) which entails the connecting of a person's brain to a computer, or to another device outside the human body. BCIs allow bidirectional communication and control between the human brain and the outside world by exporting brain data or altering brain activity. Although being marveled at for its clinical promises, this technological advancement also raises novel ethical, legal, social and technical implications (ELSTI). Debates in this regard centers around patient autonomy, equity, trustworthiness in healthcare, data protection and security, risks of dehumanization, the limitations of machine learning-based decision-making, and the influence that BCIs have on what it means to be human and human rights. Since the adoption of the Universal Declaration of Human Rights (UDHR) after World War II, the landscape that give rise to these human rights has evolved enormously. Human life and humans\u201a\u00c4\u00f4 role in society are being transformed and threatened by technologies that were never imagined at the time the UDHR was adopted. BCIs, in particular, harbor the greatest possibility of social and individual disruption through its capability to record, interpret, manipulate, or alter brain activity that may potentially alter what it means to be human and how we control humans in future. Cutting edge technological innovations that increasingly blur the lines between human and computer beg the rethinking and extension of existing human rights to remain relevant in a digitized world. In this paper sui generis human rights such as mental privacy, the right to identity or self, agency or free will and fair access to cognitive augmentation will be discussed and how a regulatory framework must be adapted to act as technology enablers, whilst ensuring fairness, accountability, and transparency in sociotechnical systems.", "keywords": "'brain computer interfaces', 'human rights', 'neurological privacy', 'autonomy', 'identity'", "ccs_concepts": NaN, "author_names": "'Marietjie Wilhelmina Maria Botes'", "author_affiliations": "'Computer Sciences, SnT Interdisciplinary Centre for Security Reliability and Trust'"}, {"link": "https://doi.org/10.1145/3531146.3533188", "title": "DualCF: Efficient Model Extraction Attack from Counterfactual Explanations", "abstract": "Cloud service providers have launched Machine-Learning-as-a-Service (MLaaS) platforms to allow users to access large-scale cloud-based models via APIs. In addition to prediction outputs, these APIs can also provide other information in a more human-understandable way, such as counterfactual explanations (CF). However, such extra information inevitably causes the cloud models to be more vulnerable to extraction attacks which aim to steal the internal functionality of models in the cloud. Due to the black-box nature of cloud models, however, a vast number of queries are inevitably required by existing attack strategies before the substitute model achieves high fidelity. In this paper, we propose a novel simple yet efficient querying strategy to greatly enhance the querying efficiency to steal a classification model. This is motivated by our observation that current querying strategies suffer from decision boundary shift issue induced by taking far-distant queries and close-to-boundary CFs into substitute model training. We then propose DualCF strategy to circumvent the above issues, which is achieved by taking not only CF but also counterfactual explanation of CF (CCF) as pairs of training samples for the substitute model. Extensive and comprehensive experimental evaluations are conducted on both synthetic and real-world datasets. The experimental results favorably illustrate that DualCF can produce a high-fidelity model with fewer queries efficiently and effectively.", "keywords": "'Counterfactual Explanations', 'Model Extraction Attack', 'Decision Boundary Shift', 'Model Security and Privacy'", "ccs_concepts": "'Security and privacy _ Software and application security', 'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Neural networks', 'Computing methodologies _ Machine learning algorithms', 'Computing methodologies _ Reasoning about belief and knowledge'", "author_names": "'Yongjie Wang', 'Hangwei Qian', 'Chunyan Miao'", "author_affiliations": "'School of Computer Science and Engineering, Nanyang Technological University', 'School of Computer Science and Engineering, Nanyang Technological University', 'School of Computer Science and Engineering, Nanyang Technological University'"}, {"link": "https://doi.org/10.1145/3531146.3533074", "title": "Fairness Indicators for Systematic Assessments of Visual Feature Extractors", "abstract": "Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds.  Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world. We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models.  To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to \u201a\u00c4\u00faoff-the-shelf\u201a\u00c4\u00f9 models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size. The results of our fairness indicators on these systems suggest that blatant disparities still exist, which highlight the importance on the relationship between the context of the task and contents of a datasets. The code will be released to encourage the use of indicators.", "keywords": "'Fairness', 'Computer Vision', 'benchmarks', 'metrics'", "ccs_concepts": "'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Computer vision'", "author_names": "'Priya Goyal', 'Adriana Romero Soriano', 'Caner Hazirbas', 'Levent Sagun', 'Nicolas Usunier'", "author_affiliations": "'Meta', 'Meta', 'Meta', 'Meta', 'Meta'"}, {"link": "https://doi.org/10.1145/3531146.3533108", "title": "Interactive Model Cards: A Human-Centered Approach to Model Documentation", "abstract": "Deep learning models for natural language processing (NLP) are increasingly adopted and deployed by analysts without formal training in NLP or machine learning (ML). However, the documentation intended to convey the model\u201a\u00c4\u00f4s details and appropriate use is tailored primarily to individuals with ML or NLP expertise. To address this gap, we conduct a design inquiry into interactive model cards, which augment traditionally static model cards with affordances for exploring model documentation and interacting with the models themselves. Our investigation consists of an initial conceptual study with experts in ML, NLP, and AI Ethics, followed by a separate evaluative study with non-expert analysts who use ML models in their work. Using a semi-structured interview format coupled with a think-aloud protocol, we collected feedback from a total of 30 participants who engaged with different versions of standard and interactive model cards. Through a thematic analysis of the collected data, we identified several conceptual dimensions that summarize the strengths and limitations of standard and interactive model cards, including: stakeholders; design; guidance; understandability & interpretability; sensemaking & skepticism; and trust & safety. Our findings demonstrate the importance of carefully considered design and interactivity for orienting and supporting non-expert analysts using deep learning models, along with a need for consideration of broader sociotechnical contexts and organizational dynamics. We have also identified design elements, such as language, visual cues, and warnings, among others, that support interactivity and make non-interactive content accessible. We summarize our findings as design guidelines and discuss their implications for a human-centered approach towards AI/ML documentation.", "keywords": "'model cards', 'human centered design', 'interactive data visualization'", "ccs_concepts": "'Computing methodologies _ Natural language processing', 'Human-centered computing _ Visualization', 'Human-centered computing _ Human computer interaction (HCI)', 'Interaction design process and methods'", "author_names": "'Anamaria Crisan', 'Margaret Drouhard', 'Jesse Vig', 'Nazneen Rajani'", "author_affiliations": "'Tableau Research', 'Tableau Software', 'Salesforce Research', 'Salesforce Research'"}, {"link": "https://doi.org/10.1145/3531146.3533069", "title": "Interdisciplinarity, Gender Diversity, and Network Structure Predict the Centrality of AI Organizations", "abstract": "Artificial intelligence (AI) research plays an increasingly important role in society, impacting key aspects of human life. From face recognition algorithms aiding national security in airports, to software that advises judges in criminal cases, and medical staff in healthcare, AI research is shaping critical facets of our experience in the world. But who are the people and institutional bodies behind this influential research? What are the predictors of influence of AI researchers and research organizations? We study this question using social network analysis, in an exploration of the structural characteristics, i.e., network topology, of research organizations that shape modern AI. In a sample of 149 organizations with 9,987 affiliated authors of published papers in a major AI conference (NeurIPS) and two major conferences that specifically focus on societal impacts of AI (FAccT and AIES), we find that both industry and academic research organizations with influential authors are more interdisciplinary, have a greater fraction of women, are more hierarchical, and less clustered, even when controlling for the size of the organizations. The influence is operationalized as betweenness centrality in co-authorship networks, i.e., how often an author is on the shortest path connecting any pair of authors, acting as a bridge connecting otherwise distant (or even disconneted) members of the network, such as their own co-authors who are not each other\u201a\u00c4\u00f4s co-author themselves. Using this operationalization, we also find that women have less influence in the AI community, determined as lower betweenness centrality in co-authorship networks. These results suggest that while diverse AI institutions are more influential, the individuals contributing to the increased diversity are marginalized in the AI field. We discuss these results in the context of current events with important societal implications.", "keywords": "'organizational structure', 'artificial intelligence', 'gender diversity', 'interdisciplinarity'", "ccs_concepts": NaN, "author_names": "'Madalina Vlasceanu', 'Miroslav Dudik', 'Ida Momennejad'", "author_affiliations": "'New York University', 'Microsoft Research', 'Microsoft Research'"}, {"link": "https://doi.org/10.1145/3531146.3533205", "title": "Justice in Misinformation Detection Systems: An Analysis of Algorithms, Stakeholders, and Potential Harms", "abstract": "Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.", "keywords": "'algorithmic fairness', 'justice', 'misinformation detection', 'machine learning', 'informational justice'", "ccs_concepts": NaN, "author_names": "'Terrence Neumann', 'Maria De-Arteaga', 'Sina Fazelpour'", "author_affiliations": "'University of Texas at Austin', 'University of Texas at Austin', 'Northeastern University'"}, {"link": "https://doi.org/10.1145/3531146.3533073", "title": "Learning to Break Deep Perceptual Hashing: The Use Case NeuralHash", "abstract": "Apple recently revealed its deep perceptual hashing system NeuralHash to detect child sexual abuse material (CSAM) on user devices before files are uploaded to its iCloud service. Public criticism quickly arose regarding the protection of user privacy and the system\u201a\u00c4\u00f4s reliability. In this paper, we present the first comprehensive empirical analysis of deep perceptual hashing based on NeuralHash. Specifically, we show that current deep perceptual hashing may not be robust. An adversary can manipulate the hash values by applying slight changes in images, either induced by gradient-based approaches or simply by performing standard image transformations, forcing or preventing hash collisions. Such attacks permit malicious actors easily to exploit the detection system: from hiding abusive material to framing innocent users, everything is possible. Moreover, using the hash values, inferences can still be made about the data stored on user devices. In our view, based on our results, deep perceptual hashing in its current form is generally not ready for robust client-side scanning and should not be used from a privacy perspective.1", "keywords": "'perceptual hashing', 'client-side scanning', 'neuralhash', 'neural networks', 'deep learning', 'privacy'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Security and privacy _ Software and application security'", "author_names": "'Lukas Struppek', 'Dominik Hintersdorf', 'Daniel Neider', 'Kristian Kersting'", "author_affiliations": "'Department of Computer Science, Technical University of Darmstadt', 'Department of Computer Science, Technical University of Darmstadt', 'Max Planck Institute for Software Systems, Germany and Safety and Explainability of Learning Systems, Carl von Ossietzky University of Oldenburg', 'Department of Computer Science and Centre for Cognitive Science, Technical University of Darmstadt, Germany and Hessian Center for Artificial Intelligence'"}, {"link": "https://doi.org/10.1145/3531146.3533128", "title": "Subverting Fair Image Search with Generative Adversarial Perturbations", "abstract": "In this work we explore the intersection fairness and robustness in the context of ranking: when a ranking model has been calibrated to achieve some definition of fairness, is it possible for an external adversary to make the ranking model behave unfairly without having access to the model or training data? To investigate this question, we present a case study in which we develop and then attack a state-of-the-art, fairness-aware image search engine using images that have been maliciously modified using a Generative Adversarial Perturbation (GAP) model\u00ac\u2020[75]. These perturbations attempt to cause the fair re-ranking algorithm to unfairly boost the rank of images containing people from an adversary-selected subpopulation.  We present results from extensive experiments demonstrating that our attacks can successfully confer significant unfair advantage to people from the majority class relative to fairly-ranked baseline search results. We demonstrate that our attacks are robust across a number of variables, that they have close to zero impact on the relevance of search results, and that they succeed under a strict threat model. Our findings highlight the danger of deploying fair machine learning algorithms in-the-wild when (1) the data necessary to achieve fairness may be adversarially manipulated, and (2) the models themselves are not robust against attacks.", "keywords": "'Information Retrieval', 'Fair Ranking', 'Adversarial Machine Learning', 'Demographic Inference'", "ccs_concepts": "'Information systems _ Retrieval models and ranking', 'Security and privacy'", "author_names": "'Avijit Ghosh', 'Matthew Jagielski', 'Christo Wilson'", "author_affiliations": "'Khoury College of Computer Sciences, Northeastern University', 'Google Brain', 'Khoury College of Computer Sciences, Northeastern University'"}, {"link": "https://doi.org/10.1145/3531146.3533215", "title": "System Safety and Artificial Intelligence", "abstract": "This article formulates seven lessons for preventing harm in artificial intelligence (AI) systems based on insights from the field of system safety for software-based automation in safety-critical domains. New applications of AI across societal domains and public organizations and infrastructures come with new hazards, which lead to new forms of harm, both grave and pernicious. The text addresses the lack of consensus for diagnosing and eliminating new AI system hazards. For decades, the field of system safety has dealt with accidents and harm in safety-critical systems governed by varying degrees of software-based automation and decision-making. This field embraces the core assumption of systems and control that AI systems cannot be safeguarded by technical design choices on the model or algorithm alone, instead requiring an end-to-end hazard analysis and design frame that includes the context of use, impacted stakeholders and the formal and informal institutional environment in which the system operates. Safety and other values are then inherently socio-technical and emergent system properties that require design and control measures to instantiate these across the technical, social and institutional components of a system. This article honors system safety pioneer Nancy Leveson, by situating her core lessons for today\u201a\u00c4\u00f4s AI system safety challenges\u00ac\u2020[2]. For every lesson, concrete tools are offered for rethinking and reorganizing the safety management of AI systems, both in design and governance. This history tells us that effective AI safety management requires transdisciplinary approaches and a shared language that allows involvement of all levels of society.  The article is a non-archival contribution to FAccT 2022, and will be published as a chapter to The Oxford Handbook of AI Governance\u00ac\u2020[1]. The full article is available as a pre-print on ArXiv via\u00ac\u2020 https://arxiv.org/abs/2202.09292.", "keywords": "'artificial intelligence', 'harms', 'audits', 'culture', 'safety', 'system safety', 'governance', 'policy', 'automation', 'systems and control'", "ccs_concepts": "'Computer systems organization _ Embedded and cyber-physical systems', 'Computing methodologies _ Artificial intelligence', 'Social and professional topics _ Government technology policy'", "author_names": "'Roel Dobbe'", "author_affiliations": "'Technology, Policy and Management, Delft University of Technology'"}, {"link": "https://doi.org/10.1145/3531146.3533179", "title": "The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations", "abstract": "Machine learning models in safety-critical settings like healthcare are often \u201a\u00c4\u00fablackboxes\u201a\u00c4\u00f9: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.", "keywords": "'explainability', 'machine learning', 'fairness'", "ccs_concepts": "'Computing methodologies _ Machine learning', 'Human-centred computing _ explanations'", "author_names": "'Aparna Balagopalan', 'Haoran Zhang', 'Kimia Hamidieh', 'Thomas Hartvigsen', 'Frank Rudzicz', 'Marzyeh Ghassemi'", "author_affiliations": "'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute, Canada and Unity Health Toronto', 'Massachusetts Institute of Technology, USA and Vector Institute'"}, {"link": "https://doi.org/10.1145/3531146.3533213", "title": "Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem", "abstract": "Algorithmic audits (or \u201a\u00c4\u00f2AI audits\u201a\u00c4\u00f4) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.", "keywords": "'AI audit', 'algorithm audit', 'audit', 'ethical AI', 'AI bias', 'AI harm', 'AI policy', 'algorithmic accountability'", "ccs_concepts": "'Social and professional topics _ Computing / technology policy', 'Human-centered computing'", "author_names": "'Sasha Costanza-Chock', 'Inioluwa Deborah Raji', 'Joy Buolamwini'", "author_affiliations": "'Algorithmic Justice League', 'Algorithmic Justice League', 'Algorithmic Justice League'"}]